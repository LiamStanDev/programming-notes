# å¯¦æˆ°æ¡ˆä¾‹èˆ‡æœ€ä½³å¯¦è¸

## å¾®æœå‹™é–“ç•°æ­¥é€šä¿¡

### å ´æ™¯: é›»å•†è¨‚å–®è™•ç†ç³»çµ±

```mermaid
graph LR
    A[è¨‚å–®æœå‹™] -->|order.created| K[Kafka]
    K -->|è¨‚é–±| B[åº«å­˜æœå‹™]
    K -->|è¨‚é–±| C[æ”¯ä»˜æœå‹™]
    K -->|è¨‚é–±| D[é€šçŸ¥æœå‹™]
    
    B -->|inventory.reserved| K
    C -->|payment.completed| K
    D -->|notification.sent| K
    
    style K fill:#4caf50,color:#fff
```

### å¯¦ç¾

#### è¨‚å–®æœå‹™ (Producer)

```python
from kafka import KafkaProducer
import json
import uuid
from datetime import datetime
from enum import Enum

class OrderEventType(Enum):
    CREATED = "order.created"
    CONFIRMED = "order.confirmed"
    CANCELLED = "order.cancelled"
    SHIPPED = "order.shipped"
    DELIVERED = "order.delivered"

class OrderService:
    def __init__(self, bootstrap_servers):
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            acks='all',
            retries=5,
            max_in_flight_requests_per_connection=1,
            enable_idempotence=True,
            key_serializer=lambda k: k.encode('utf-8'),
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
    
    def create_order(self, user_id, items, total_amount):
        order_id = str(uuid.uuid4())
        
        order_event = {
            'event_id': str(uuid.uuid4()),
            'event_type': OrderEventType.CREATED.value,
            'timestamp': datetime.utcnow().isoformat(),
            'order_id': order_id,
            'user_id': user_id,
            'items': items,
            'total_amount': total_amount,
            'status': 'pending'
        }
        
        try:
            future = self.producer.send(
                topic='order-events',
                key=order_id,
                value=order_event,
                headers=[
                    ('event_type', OrderEventType.CREATED.value.encode('utf-8')),
                    ('correlation_id', order_id.encode('utf-8'))
                ]
            )
            
            record_metadata = future.get(timeout=10)
            
            print(f"âœ… è¨‚å–®å‰µå»ºæˆåŠŸ: {order_id}")
            print(f"   Topic: {record_metadata.topic}")
            print(f"   Partition: {record_metadata.partition}")
            print(f"   Offset: {record_metadata.offset}")
            
            return order_id
            
        except Exception as e:
            print(f"âŒ è¨‚å–®å‰µå»ºå¤±æ•—: {e}")
            raise
    
    def update_order_status(self, order_id, status, event_type):
        event = {
            'event_id': str(uuid.uuid4()),
            'event_type': event_type.value,
            'timestamp': datetime.utcnow().isoformat(),
            'order_id': order_id,
            'status': status
        }
        
        self.producer.send(
            topic='order-events',
            key=order_id,
            value=event,
            headers=[('event_type', event_type.value.encode('utf-8'))]
        )
        
        print(f"ğŸ“¤ è¨‚å–®ç‹€æ…‹æ›´æ–°: {order_id} -> {status}")
    
    def close(self):
        self.producer.flush()
        self.producer.close()

if __name__ == '__main__':
    order_service = OrderService(['localhost:9092'])
    
    order_id = order_service.create_order(
        user_id='user_12345',
        items=[
            {'product_id': 'prod_001', 'quantity': 2, 'price': 29.99},
            {'product_id': 'prod_002', 'quantity': 1, 'price': 49.99}
        ],
        total_amount=109.97
    )
    
    order_service.close()
```

#### åº«å­˜æœå‹™ (Consumer)

```python
from kafka import KafkaConsumer, KafkaProducer
import json
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class InventoryService:
    def __init__(self, bootstrap_servers):
        self.consumer = KafkaConsumer(
            'order-events',
            bootstrap_servers=bootstrap_servers,
            group_id='inventory-service',
            enable_auto_commit=False,
            auto_offset_reset='earliest',
            value_deserializer=lambda v: json.loads(v.decode('utf-8'))
        )
        
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            acks='all',
            key_serializer=lambda k: k.encode('utf-8'),
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        
        self.inventory = {
            'prod_001': 100,
            'prod_002': 50,
            'prod_003': 200
        }
    
    def reserve_inventory(self, order_id, items):
        for item in items:
            product_id = item['product_id']
            quantity = item['quantity']
            
            if product_id not in self.inventory:
                logger.error(f"âŒ ç”¢å“ä¸å­˜åœ¨: {product_id}")
                return False
            
            if self.inventory[product_id] < quantity:
                logger.error(f"âŒ åº«å­˜ä¸è¶³: {product_id} (éœ€è¦ {quantity}, ç¾æœ‰ {self.inventory[product_id]})")
                return False
            
            self.inventory[product_id] -= quantity
            logger.info(f"âœ… åº«å­˜é ç•™æˆåŠŸ: {product_id} x {quantity}")
        
        self.publish_event(order_id, 'inventory.reserved', {
            'order_id': order_id,
            'items': items,
            'status': 'reserved'
        })
        
        return True
    
    def publish_event(self, order_id, event_type, data):
        self.producer.send(
            topic='inventory-events',
            key=order_id,
            value={
                'event_type': event_type,
                'timestamp': datetime.utcnow().isoformat(),
                **data
            }
        )
        self.producer.flush()
    
    def process_order_event(self, event):
        event_type = event.get('event_type')
        
        if event_type == 'order.created':
            order_id = event['order_id']
            items = event['items']
            
            logger.info(f"ğŸ“¦ è™•ç†è¨‚å–®: {order_id}")
            
            if self.reserve_inventory(order_id, items):
                logger.info(f"âœ… è¨‚å–® {order_id} åº«å­˜é ç•™æˆåŠŸ")
            else:
                logger.error(f"âŒ è¨‚å–® {order_id} åº«å­˜é ç•™å¤±æ•—")
                self.publish_event(order_id, 'inventory.failed', {
                    'order_id': order_id,
                    'reason': 'Insufficient inventory'
                })
    
    def start(self):
        logger.info("åº«å­˜æœå‹™å•Ÿå‹•...")
        
        try:
            for message in self.consumer:
                event = message.value
                
                try:
                    self.process_order_event(event)
                    self.consumer.commit()
                except Exception as e:
                    logger.error(f"è™•ç†äº‹ä»¶å¤±æ•—: {e}")
                    
        except KeyboardInterrupt:
            logger.info("åœæ­¢åº«å­˜æœå‹™")
        finally:
            self.consumer.close()
            self.producer.close()

if __name__ == '__main__':
    from datetime import datetime
    
    inventory_service = InventoryService(['localhost:9092'])
    inventory_service.start()
```

#### Saga æ¨¡å¼å”èª¿

```python
from kafka import KafkaConsumer, KafkaProducer
import json
from enum import Enum
from datetime import datetime

class SagaStatus(Enum):
    PENDING = "pending"
    INVENTORY_RESERVED = "inventory_reserved"
    PAYMENT_COMPLETED = "payment_completed"
    COMPLETED = "completed"
    FAILED = "failed"
    COMPENSATING = "compensating"

class OrderSagaCoordinator:
    def __init__(self, bootstrap_servers):
        self.consumer = KafkaConsumer(
            'order-events',
            'inventory-events',
            'payment-events',
            bootstrap_servers=bootstrap_servers,
            group_id='saga-coordinator',
            enable_auto_commit=False,
            value_deserializer=lambda v: json.loads(v.decode('utf-8'))
        )
        
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            acks='all',
            key_serializer=lambda k: k.encode('utf-8'),
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        
        self.sagas = {}
    
    def handle_event(self, event):
        event_type = event.get('event_type')
        order_id = event.get('order_id')
        
        if order_id not in self.sagas:
            self.sagas[order_id] = {
                'status': SagaStatus.PENDING,
                'events': []
            }
        
        saga = self.sagas[order_id]
        saga['events'].append(event)
        
        if event_type == 'order.created':
            saga['status'] = SagaStatus.PENDING
            print(f"ğŸ”„ Saga å•Ÿå‹•: {order_id}")
        
        elif event_type == 'inventory.reserved':
            saga['status'] = SagaStatus.INVENTORY_RESERVED
            print(f"âœ… åº«å­˜é ç•™å®Œæˆ: {order_id}")
            self.trigger_payment(order_id, event)
        
        elif event_type == 'inventory.failed':
            saga['status'] = SagaStatus.FAILED
            print(f"âŒ åº«å­˜é ç•™å¤±æ•—ï¼ŒSaga å¤±æ•—: {order_id}")
            self.publish_order_cancelled(order_id)
        
        elif event_type == 'payment.completed':
            saga['status'] = SagaStatus.PAYMENT_COMPLETED
            print(f"âœ… æ”¯ä»˜å®Œæˆ: {order_id}")
            self.publish_order_confirmed(order_id)
        
        elif event_type == 'payment.failed':
            saga['status'] = SagaStatus.COMPENSATING
            print(f"âŒ æ”¯ä»˜å¤±æ•—ï¼Œé–‹å§‹è£œå„Ÿ: {order_id}")
            self.compensate_inventory(order_id)
    
    def trigger_payment(self, order_id, inventory_event):
        self.producer.send(
            topic='payment-commands',
            key=order_id,
            value={
                'command': 'process_payment',
                'order_id': order_id,
                'timestamp': datetime.utcnow().isoformat()
            }
        )
        self.producer.flush()
    
    def compensate_inventory(self, order_id):
        self.producer.send(
            topic='inventory-commands',
            key=order_id,
            value={
                'command': 'release_inventory',
                'order_id': order_id,
                'timestamp': datetime.utcnow().isoformat()
            }
        )
        self.producer.flush()
    
    def publish_order_confirmed(self, order_id):
        self.producer.send(
            topic='order-events',
            key=order_id,
            value={
                'event_type': 'order.confirmed',
                'order_id': order_id,
                'timestamp': datetime.utcnow().isoformat()
            }
        )
        self.producer.flush()
    
    def publish_order_cancelled(self, order_id):
        self.producer.send(
            topic='order-events',
            key=order_id,
            value={
                'event_type': 'order.cancelled',
                'order_id': order_id,
                'timestamp': datetime.utcnow().isoformat()
            }
        )
        self.producer.flush()
    
    def start(self):
        print("Saga å”èª¿å™¨å•Ÿå‹•...")
        
        try:
            for message in self.consumer:
                event = message.value
                self.handle_event(event)
                self.consumer.commit()
        except KeyboardInterrupt:
            print("åœæ­¢ Saga å”èª¿å™¨")
        finally:
            self.consumer.close()
            self.producer.close()
```

## äº‹ä»¶æº¯æº (Event Sourcing)

### å ´æ™¯: éŠ€è¡Œè³¬æˆ¶ç³»çµ±

```python
from kafka import KafkaProducer, KafkaConsumer
import json
import uuid
from datetime import datetime
from decimal import Decimal

class AccountEventType:
    CREATED = "account.created"
    DEPOSITED = "account.deposited"
    WITHDRAWN = "account.withdrawn"
    TRANSFERRED = "account.transferred"

class BankAccount:
    def __init__(self, account_id, bootstrap_servers):
        self.account_id = account_id
        self.balance = Decimal('0')
        self.version = 0
        
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            acks='all',
            enable_idempotence=True,
            key_serializer=lambda k: k.encode('utf-8'),
            value_serializer=lambda v: json.dumps(v, default=str).encode('utf-8')
        )
    
    def publish_event(self, event_type, data):
        event = {
            'event_id': str(uuid.uuid4()),
            'event_type': event_type,
            'account_id': self.account_id,
            'version': self.version + 1,
            'timestamp': datetime.utcnow().isoformat(),
            **data
        }
        
        future = self.producer.send(
            topic='account-events',
            key=self.account_id,
            value=event
        )
        
        future.get(timeout=10)
        self.version += 1
        
        print(f"ğŸ“ äº‹ä»¶ç™¼å¸ƒ: {event_type} (ç‰ˆæœ¬ {self.version})")
        return event
    
    def create_account(self, owner_name, initial_balance=0):
        event = self.publish_event(AccountEventType.CREATED, {
            'owner_name': owner_name,
            'initial_balance': float(initial_balance)
        })
        
        self.balance = Decimal(str(initial_balance))
        return event
    
    def deposit(self, amount):
        if amount <= 0:
            raise ValueError("å­˜æ¬¾é‡‘é¡å¿…é ˆå¤§æ–¼ 0")
        
        event = self.publish_event(AccountEventType.DEPOSITED, {
            'amount': float(amount),
            'balance_after': float(self.balance + Decimal(str(amount)))
        })
        
        self.balance += Decimal(str(amount))
        return event
    
    def withdraw(self, amount):
        if amount <= 0:
            raise ValueError("å–æ¬¾é‡‘é¡å¿…é ˆå¤§æ–¼ 0")
        
        if self.balance < Decimal(str(amount)):
            raise ValueError("é¤˜é¡ä¸è¶³")
        
        event = self.publish_event(AccountEventType.WITHDRAWN, {
            'amount': float(amount),
            'balance_after': float(self.balance - Decimal(str(amount)))
        })
        
        self.balance -= Decimal(str(amount))
        return event
    
    def close(self):
        self.producer.close()

class AccountProjection:
    def __init__(self, bootstrap_servers):
        self.consumer = KafkaConsumer(
            'account-events',
            bootstrap_servers=bootstrap_servers,
            group_id='account-projection',
            auto_offset_reset='earliest',
            enable_auto_commit=False,
            value_deserializer=lambda v: json.loads(v.decode('utf-8'))
        )
        
        self.accounts = {}
    
    def apply_event(self, event):
        event_type = event['event_type']
        account_id = event['account_id']
        
        if account_id not in self.accounts:
            self.accounts[account_id] = {
                'balance': Decimal('0'),
                'version': 0,
                'transactions': []
            }
        
        account = self.accounts[account_id]
        
        if event_type == AccountEventType.CREATED:
            account['owner_name'] = event['owner_name']
            account['balance'] = Decimal(str(event['initial_balance']))
            account['created_at'] = event['timestamp']
        
        elif event_type == AccountEventType.DEPOSITED:
            account['balance'] += Decimal(str(event['amount']))
        
        elif event_type == AccountEventType.WITHDRAWN:
            account['balance'] -= Decimal(str(event['amount']))
        
        account['version'] = event['version']
        account['transactions'].append({
            'event_type': event_type,
            'timestamp': event['timestamp'],
            'amount': event.get('amount')
        })
        
        print(f"âœ… äº‹ä»¶æ‡‰ç”¨: {event_type} -> è³¬æˆ¶ {account_id} (é¤˜é¡: {account['balance']})")
    
    def rebuild_projection(self):
        print("ğŸ”„ é‡å»ºæŠ•å½±...")
        
        for message in self.consumer:
            event = message.value
            self.apply_event(event)
            self.consumer.commit()
    
    def get_account_state(self, account_id):
        return self.accounts.get(account_id)
    
    def close(self):
        self.consumer.close()

if __name__ == '__main__':
    account = BankAccount('ACC001', ['localhost:9092'])
    
    account.create_account('Alice Smith', 1000.00)
    account.deposit(500.00)
    account.withdraw(200.00)
    account.deposit(1000.00)
    
    account.close()
    
    projection = AccountProjection(['localhost:9092'])
    
    import time
    time.sleep(2)
    
    state = projection.get_account_state('ACC001')
    if state:
        print(f"\n=== è³¬æˆ¶ç‹€æ…‹ ===")
        print(f"æ‰€æœ‰è€…: {state.get('owner_name')}")
        print(f"é¤˜é¡: ${state['balance']}")
        print(f"ç‰ˆæœ¬: {state['version']}")
        print(f"äº¤æ˜“è¨˜éŒ„: {len(state['transactions'])} ç­†")
```

## å¯¦æ™‚æ•¸æ“šæµè™•ç†

### å ´æ™¯: å¯¦æ™‚é»æ“Šæµåˆ†æ

```python
from kafka import KafkaConsumer, KafkaProducer
import json
from datetime import datetime, timedelta
from collections import defaultdict
import time

class ClickStreamAnalyzer:
    def __init__(self, bootstrap_servers):
        self.consumer = KafkaConsumer(
            'clickstream-events',
            bootstrap_servers=bootstrap_servers,
            group_id='analytics-processor',
            auto_offset_reset='latest',
            enable_auto_commit=False,
            value_deserializer=lambda v: json.loads(v.decode('utf-8'))
        )
        
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        
        self.window_size_seconds = 60
        self.windows = defaultdict(lambda: {
            'page_views': defaultdict(int),
            'unique_users': set(),
            'events': []
        })
    
    def get_window_key(self, timestamp):
        dt = datetime.fromisoformat(timestamp)
        window_start = dt.replace(second=0, microsecond=0)
        return window_start.isoformat()
    
    def process_event(self, event):
        timestamp = event.get('timestamp', datetime.utcnow().isoformat())
        window_key = self.get_window_key(timestamp)
        
        window = self.windows[window_key]
        
        window['page_views'][event.get('page_url')] += 1
        window['unique_users'].add(event.get('user_id'))
        window['events'].append(event)
        
        return window_key
    
    def compute_aggregates(self, window_key):
        window = self.windows[window_key]
        
        total_page_views = sum(window['page_views'].values())
        unique_users = len(window['unique_users'])
        
        top_pages = sorted(
            window['page_views'].items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        aggregate = {
            'window_start': window_key,
            'total_page_views': total_page_views,
            'unique_users': unique_users,
            'top_pages': [
                {'url': url, 'views': count}
                for url, count in top_pages
            ],
            'computed_at': datetime.utcnow().isoformat()
        }
        
        return aggregate
    
    def publish_aggregate(self, aggregate):
        self.producer.send(
            topic='analytics-results',
            value=aggregate
        )
        self.producer.flush()
        
        print(f"\nğŸ“Š èšåˆçµæœç™¼å¸ƒ: {aggregate['window_start']}")
        print(f"   ç¸½é é¢ç€è¦½: {aggregate['total_page_views']}")
        print(f"   ç¨ç«‹è¨ªå®¢: {aggregate['unique_users']}")
        print(f"   ç†±é–€é é¢:")
        for page in aggregate['top_pages']:
            print(f"     - {page['url']}: {page['views']} æ¬¡")
    
    def cleanup_old_windows(self):
        cutoff = datetime.utcnow() - timedelta(minutes=5)
        cutoff_key = cutoff.replace(second=0, microsecond=0).isoformat()
        
        old_windows = [k for k in self.windows.keys() if k < cutoff_key]
        
        for window_key in old_windows:
            del self.windows[window_key]
    
    def start(self):
        print("å¯¦æ™‚åˆ†æå¼•æ“å•Ÿå‹•...")
        
        last_aggregate_time = datetime.utcnow()
        
        try:
            for message in self.consumer:
                event = message.value
                
                window_key = self.process_event(event)
                
                now = datetime.utcnow()
                if (now - last_aggregate_time).seconds >= 60:
                    for wk in list(self.windows.keys()):
                        aggregate = self.compute_aggregates(wk)
                        self.publish_aggregate(aggregate)
                    
                    self.cleanup_old_windows()
                    last_aggregate_time = now
                
                self.consumer.commit()
                
        except KeyboardInterrupt:
            print("åœæ­¢åˆ†æå¼•æ“")
        finally:
            self.consumer.close()
            self.producer.close()

class ClickStreamGenerator:
    def __init__(self, bootstrap_servers):
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        
        self.pages = [
            '/home',
            '/products',
            '/about',
            '/contact',
            '/checkout'
        ]
        
        self.users = [f'user_{i}' for i in range(100)]
    
    def generate_event(self):
        import random
        
        event = {
            'user_id': random.choice(self.users),
            'page_url': random.choice(self.pages),
            'session_id': str(uuid.uuid4()),
            'timestamp': datetime.utcnow().isoformat(),
            'user_agent': 'Mozilla/5.0'
        }
        
        self.producer.send('clickstream-events', value=event)
        return event
    
    def start_generating(self, events_per_second=10, duration_seconds=300):
        print(f"é–‹å§‹ç”Ÿæˆé»æ“Šæµäº‹ä»¶ ({events_per_second} events/sec)...")
        
        import time
        
        end_time = time.time() + duration_seconds
        
        while time.time() < end_time:
            for _ in range(events_per_second):
                self.generate_event()
            
            time.sleep(1)
        
        self.producer.close()

if __name__ == '__main__':
    analyzer = ClickStreamAnalyzer(['localhost:9092'])
    analyzer.start()
```

## ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸

### 1. é…ç½®ç®¡ç†

```python
import os
from dataclasses import dataclass
from typing import List

@dataclass
class KafkaConfig:
    bootstrap_servers: List[str]
    security_protocol: str = 'PLAINTEXT'
    sasl_mechanism: str = None
    sasl_username: str = None
    sasl_password: str = None
    
    producer_acks: str = 'all'
    producer_retries: int = 5
    producer_compression: str = 'lz4'
    producer_batch_size: int = 32768
    producer_linger_ms: int = 10
    
    consumer_group_id: str = None
    consumer_auto_offset_reset: str = 'earliest'
    consumer_enable_auto_commit: bool = False
    consumer_max_poll_records: int = 500
    
    @classmethod
    def from_env(cls):
        return cls(
            bootstrap_servers=os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092').split(','),
            security_protocol=os.getenv('KAFKA_SECURITY_PROTOCOL', 'PLAINTEXT'),
            sasl_mechanism=os.getenv('KAFKA_SASL_MECHANISM'),
            sasl_username=os.getenv('KAFKA_SASL_USERNAME'),
            sasl_password=os.getenv('KAFKA_SASL_PASSWORD'),
            producer_acks=os.getenv('KAFKA_PRODUCER_ACKS', 'all'),
            consumer_group_id=os.getenv('KAFKA_CONSUMER_GROUP_ID')
        )
    
    def get_producer_config(self):
        config = {
            'bootstrap_servers': self.bootstrap_servers,
            'acks': self.producer_acks,
            'retries': self.producer_retries,
            'compression_type': self.producer_compression,
            'batch_size': self.producer_batch_size,
            'linger_ms': self.producer_linger_ms,
            'enable_idempotence': True,
            'max_in_flight_requests_per_connection': 5
        }
        
        if self.security_protocol != 'PLAINTEXT':
            config.update({
                'security_protocol': self.security_protocol,
                'sasl_mechanism': self.sasl_mechanism,
                'sasl_plain_username': self.sasl_username,
                'sasl_plain_password': self.sasl_password
            })
        
        return config
    
    def get_consumer_config(self):
        config = {
            'bootstrap_servers': self.bootstrap_servers,
            'group_id': self.consumer_group_id,
            'auto_offset_reset': self.consumer_auto_offset_reset,
            'enable_auto_commit': self.consumer_enable_auto_commit,
            'max_poll_records': self.consumer_max_poll_records
        }
        
        if self.security_protocol != 'PLAINTEXT':
            config.update({
                'security_protocol': self.security_protocol,
                'sasl_mechanism': self.sasl_mechanism,
                'sasl_plain_username': self.sasl_username,
                'sasl_plain_password': self.sasl_password
            })
        
        return config

config = KafkaConfig.from_env()
```

### 2. éŒ¯èª¤è™•ç†èˆ‡é‡è©¦

```python
from kafka import KafkaProducer
from kafka.errors import KafkaError, KafkaTimeoutError
import time
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ResilientProducer:
    def __init__(self, config):
        self.config = config
        self.producer = None
        self.connect()
    
    def connect(self):
        max_retries = 3
        retry_delay = 5
        
        for attempt in range(max_retries):
            try:
                self.producer = KafkaProducer(**self.config.get_producer_config())
                logger.info("âœ… Producer é€£æ¥æˆåŠŸ")
                return
            except Exception as e:
                logger.error(f"âŒ é€£æ¥å¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                else:
                    raise
    
    def send_with_retry(self, topic, value, key=None, max_retries=3):
        for attempt in range(max_retries):
            try:
                future = self.producer.send(topic, key=key, value=value)
                record_metadata = future.get(timeout=30)
                
                logger.info(f"âœ… æ¶ˆæ¯ç™¼é€æˆåŠŸ: {topic}:{record_metadata.partition}:{record_metadata.offset}")
                return record_metadata
                
            except KafkaTimeoutError:
                logger.warning(f"âš ï¸  ç™¼é€è¶…æ™‚ (å˜—è©¦ {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    logger.error("âŒ ç™¼é€æœ€çµ‚å¤±æ•—: è¶…æ™‚")
                    raise
            
            except KafkaError as e:
                logger.error(f"âŒ Kafka éŒ¯èª¤: {e}")
                raise
            
            except Exception as e:
                logger.error(f"âŒ æœªçŸ¥éŒ¯èª¤: {e}")
                raise
    
    def close(self):
        if self.producer:
            self.producer.flush()
            self.producer.close()
            logger.info("Producer å·²é—œé–‰")
```

### 3. å„ªé›…é—œé–‰

```python
from kafka import KafkaConsumer
import signal
import sys
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GracefulConsumer:
    def __init__(self, config, topics, message_handler):
        self.config = config
        self.topics = topics
        self.message_handler = message_handler
        self.running = True
        
        self.consumer = KafkaConsumer(
            *topics,
            **config.get_consumer_config()
        )
        
        signal.signal(signal.SIGINT, self.shutdown)
        signal.signal(signal.SIGTERM, self.shutdown)
    
    def shutdown(self, signum, frame):
        logger.info(f"æ”¶åˆ°ä¿¡è™Ÿ {signum}ï¼Œé–‹å§‹å„ªé›…é—œé–‰...")
        self.running = False
    
    def start(self):
        logger.info(f"é–‹å§‹æ¶ˆè²» topics: {self.topics}")
        
        try:
            while self.running:
                messages = self.consumer.poll(timeout_ms=1000)
                
                if not messages:
                    continue
                
                for topic_partition, records in messages.items():
                    for message in records:
                        try:
                            self.message_handler(message)
                        except Exception as e:
                            logger.error(f"æ¶ˆæ¯è™•ç†å¤±æ•—: {e}")
                            continue
                
                self.consumer.commit()
        
        finally:
            logger.info("æäº¤æœ€çµ‚ offset...")
            self.consumer.commit()
            
            logger.info("é—œé–‰ consumer...")
            self.consumer.close()
            
            logger.info("âœ… å„ªé›…é—œé–‰å®Œæˆ")

def handle_message(message):
    logger.info(f"è™•ç†æ¶ˆæ¯: {message.value}")

if __name__ == '__main__':
    config = KafkaConfig.from_env()
    consumer = GracefulConsumer(
        config=config,
        topics=['user-events'],
        message_handler=handle_message
    )
    consumer.start()
```

### 4. Dead Letter Queue (DLQ)

```python
from kafka import KafkaConsumer, KafkaProducer
import json
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DLQConsumer:
    def __init__(self, config, source_topic, dlq_topic):
        self.consumer = KafkaConsumer(
            source_topic,
            **config.get_consumer_config()
        )
        
        self.dlq_producer = KafkaProducer(
            **config.get_producer_config(),
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        
        self.dlq_topic = dlq_topic
        self.max_retries = 3
    
    def send_to_dlq(self, message, error, retry_count=0):
        dlq_message = {
            'original_topic': message.topic,
            'original_partition': message.partition,
            'original_offset': message.offset,
            'original_key': message.key.decode('utf-8') if message.key else None,
            'original_value': message.value,
            'error': str(error),
            'retry_count': retry_count,
            'timestamp': datetime.utcnow().isoformat()
        }
        
        self.dlq_producer.send(self.dlq_topic, value=dlq_message)
        self.dlq_producer.flush()
        
        logger.warning(f"âš ï¸  æ¶ˆæ¯ç™¼é€åˆ° DLQ: {self.dlq_topic}")
    
    def process_message(self, message):
        retry_count = 0
        
        while retry_count < self.max_retries:
            try:
                self.handle_message(message.value)
                return True
            
            except Exception as e:
                retry_count += 1
                logger.warning(f"è™•ç†å¤±æ•— (å˜—è©¦ {retry_count}/{self.max_retries}): {e}")
                
                if retry_count < self.max_retries:
                    time.sleep(2 ** retry_count)
        
        self.send_to_dlq(message, f"è™•ç†å¤±æ•— {self.max_retries} æ¬¡", retry_count)
        return False
    
    def handle_message(self, value):
        raise NotImplementedError("å­é¡éœ€å¯¦ç¾æ­¤æ–¹æ³•")
    
    def start(self):
        logger.info("DLQ Consumer å•Ÿå‹•...")
        
        try:
            for message in self.consumer:
                self.process_message(message)
                self.consumer.commit()
        
        except KeyboardInterrupt:
            logger.info("åœæ­¢ DLQ Consumer")
        
        finally:
            self.consumer.close()
            self.dlq_producer.close()
```

## ç¸½çµèˆ‡æª¢æŸ¥æ¸…å–®

### ç”Ÿç”¢ç’°å¢ƒæª¢æŸ¥æ¸…å–®

#### Kafka é›†ç¾¤

- [ ] è‡³å°‘ 3 å€‹ Broker
- [ ] å‰¯æœ¬å› å­ = 3
- [ ] `min.insync.replicas` = 2
- [ ] ç£ç¢Ÿä½¿ç”¨ç‡ < 80%
- [ ] å®šæœŸå‚™ä»½é…ç½®

#### Producer é…ç½®

- [ ] `acks` = all
- [ ] `enable.idempotence` = true
- [ ] åˆç†çš„ `batch.size` å’Œ `linger.ms`
- [ ] å•Ÿç”¨å£“ç¸® (lz4/zstd)
- [ ] éŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶

#### Consumer é…ç½®

- [ ] æ‰‹å‹•æäº¤ offset
- [ ] åˆç†çš„ `max.poll.records`
- [ ] ç›£æ§ Consumer Lag
- [ ] å¯¦ç¾ Dead Letter Queue
- [ ] å„ªé›…é—œé–‰æ©Ÿåˆ¶

#### ç›£æ§èˆ‡å‘Šè­¦

- [ ] Prometheus + Grafana
- [ ] CPUã€å…§å­˜ã€ç£ç¢Ÿç›£æ§
- [ ] Under-Replicated Partitions å‘Šè­¦
- [ ] Consumer Lag å‘Šè­¦
- [ ] éŒ¯èª¤ç‡ç›£æ§

#### å®‰å…¨æ€§

- [ ] å•Ÿç”¨ SSL/TLS
- [ ] SASL èªè­‰
- [ ] ACL æ¬Šé™æ§åˆ¶
- [ ] ç¶²çµ¡éš”é›¢

## åƒè€ƒè³‡æº

- [Kafka Use Cases](https://kafka.apache.org/uses)
- [Confluent Best Practices](https://docs.confluent.io/platform/current/kafka/deployment.html)
- [Event Sourcing with Kafka](https://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/)
