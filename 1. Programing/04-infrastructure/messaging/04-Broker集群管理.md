# Broker é›†ç¾¤ç®¡ç†

## Kafka é›†ç¾¤æ¶æ§‹

### é›†ç¾¤çµ„æˆ

```mermaid
graph TB
    subgraph "Kafka Cluster"
        B1[Broker 1<br/>ID: 1<br/>Role: Controller]
        B2[Broker 2<br/>ID: 2]
        B3[Broker 3<br/>ID: 3]
    end
    
    subgraph "ZooKeeper Ensemble"
        Z1[ZK Node 1]
        Z2[ZK Node 2]
        Z3[ZK Node 3]
    end
    
    B1 <--> Z1
    B2 <--> Z2
    B3 <--> Z3
    
    B1 -.è¤‡è£½.-> B2
    B2 -.è¤‡è£½.-> B3
    B1 -.è¤‡è£½.-> B3
    
    style B1 fill:#4caf50,color:#fff
    style B2 fill:#2196f3,color:#fff
    style B3 fill:#2196f3,color:#fff
    style Z1 fill:#ff9800,color:#fff
    style Z2 fill:#ff9800,color:#fff
    style Z3 fill:#ff9800,color:#fff
```

### Controller è§’è‰²

**è·è²¬**:
- ç®¡ç†åˆ†å€ Leader é¸èˆ‰
- ç›£æ§ Broker ç‹€æ…‹è®ŠåŒ–
- ç¶­è­·é›†ç¾¤å…ƒæ•¸æ“š
- åˆ†å€é‡åˆ†é… (Partition Reassignment)

```mermaid
sequenceDiagram
    participant B1 as Broker 1 (Controller)
    participant B2 as Broker 2
    participant B3 as Broker 3
    participant ZK as ZooKeeper
    
    B2->>ZK: å¿ƒè·³è¶…æ™‚
    ZK->>B1: é€šçŸ¥ Broker 2 ä¸‹ç·š
    B1->>B1: è§¸ç™¼ Leader é¸èˆ‰
    B1->>B3: æ›´æ–°å…ƒæ•¸æ“š (B3 æˆç‚ºæ–° Leader)
    B1->>ZK: æ›´æ–°é›†ç¾¤ç‹€æ…‹
    B3->>B3: é–‹å§‹è™•ç†è®€å¯«è«‹æ±‚
```

## ZooKeeper vs KRaft

### ZooKeeper æ¨¡å¼ (å‚³çµ±)

**å„ªé»**:
- æˆç†Ÿç©©å®š
- å»£æ³›ä½¿ç”¨

**ç¼ºé»**:
- é¡å¤–é‹ç¶­æˆæœ¬
- é›†ç¾¤æ“´å±•å—é™
- æ¢å¾©æ™‚é–“è¼ƒé•·

### KRaft æ¨¡å¼ (Kafka 3.0+)

**å„ªé»**:
- ç§»é™¤ ZooKeeper ä¾è³´
- æ›´å¿«çš„æ•…éšœæ¢å¾©
- æ”¯æŒç™¾è¬ç´šåˆ†å€
- ç°¡åŒ–é‹ç¶­

**æ¶æ§‹**:
```mermaid
graph TB
    subgraph "KRaft Cluster"
        C1[Controller 1<br/>Broker ID: 1]
        C2[Controller 2<br/>Broker ID: 2]
        C3[Controller 3<br/>Broker ID: 3]
        B1[Broker 4]
        B2[Broker 5]
    end
    
    C1 -.Raft å”è­°.-> C2
    C2 -.Raft å”è­°.-> C3
    C1 -.Raft å”è­°.-> C3
    
    C1 --> B1
    C1 --> B2
    C2 --> B1
    C2 --> B2
    
    style C1 fill:#4caf50,color:#fff
    style C2 fill:#4caf50,color:#fff
    style C3 fill:#4caf50,color:#fff
```

## é›†ç¾¤éƒ¨ç½²

### Docker Compose éƒ¨ç½² (é–‹ç™¼ç’°å¢ƒ)

```yaml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zk-data:/var/lib/zookeeper/data
      - zk-logs:/var/lib/zookeeper/log

  broker-1:
    image: confluentinc/cp-kafka:7.5.0
    hostname: broker-1
    container_name: broker-1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker-1:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_LOG_RETENTION_HOURS: 168
    volumes:
      - broker-1-data:/var/lib/kafka/data

  broker-2:
    image: confluentinc/cp-kafka:7.5.0
    hostname: broker-2
    container_name: broker-2
    depends_on:
      - zookeeper
    ports:
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker-2:29093,PLAINTEXT_HOST://localhost:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
    volumes:
      - broker-2-data:/var/lib/kafka/data

  broker-3:
    image: confluentinc/cp-kafka:7.5.0
    hostname: broker-3
    container_name: broker-3
    depends_on:
      - zookeeper
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker-3:29094,PLAINTEXT_HOST://localhost:9094
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
    volumes:
      - broker-3-data:/var/lib/kafka/data

volumes:
  zk-data:
  zk-logs:
  broker-1-data:
  broker-2-data:
  broker-3-data:
```

å•Ÿå‹•é›†ç¾¤:
```bash
docker-compose up -d
```

### KRaft æ¨¡å¼éƒ¨ç½²

```yaml
version: '3.8'

services:
  kafka-1:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka-1
    container_name: kafka-1
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka-1:29093,2@kafka-2:29093,3@kafka-3:29093'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka-1:29092,CONTROLLER://kafka-1:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka-1:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_LOG_DIRS: '/var/lib/kafka/data'
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
    volumes:
      - kafka-1-data:/var/lib/kafka/data

  kafka-2:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka-2
    container_name: kafka-2
    ports:
      - "9093:9093"
    environment:
      KAFKA_NODE_ID: 2
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka-1:29093,2@kafka-2:29093,3@kafka-3:29093'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka-2:29092,CONTROLLER://kafka-2:29093,PLAINTEXT_HOST://0.0.0.0:9093'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka-2:29092,PLAINTEXT_HOST://localhost:9093'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_LOG_DIRS: '/var/lib/kafka/data'
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
    volumes:
      - kafka-2-data:/var/lib/kafka/data

  kafka-3:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka-3
    container_name: kafka-3
    ports:
      - "9094:9094"
    environment:
      KAFKA_NODE_ID: 3
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka-1:29093,2@kafka-2:29093,3@kafka-3:29093'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka-3:29092,CONTROLLER://kafka-3:29093,PLAINTEXT_HOST://0.0.0.0:9094'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka-3:29092,PLAINTEXT_HOST://localhost:9094'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_LOG_DIRS: '/var/lib/kafka/data'
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
    volumes:
      - kafka-3-data:/var/lib/kafka/data

volumes:
  kafka-1-data:
  kafka-2-data:
  kafka-3-data:
```

## é›†ç¾¤ç®¡ç† Python API

### æŸ¥çœ‹é›†ç¾¤ä¿¡æ¯

```python
from kafka import KafkaAdminClient, KafkaConsumer
from kafka.admin import ConfigResource, ConfigResourceType
import json

class KafkaClusterManager:
    def __init__(self, bootstrap_servers):
        self.admin_client = KafkaAdminClient(
            bootstrap_servers=bootstrap_servers,
            client_id='cluster-manager'
        )
    
    def list_brokers(self):
        metadata = self.admin_client._client.cluster
        brokers = metadata.brokers()
        
        print("=== Broker åˆ—è¡¨ ===")
        for broker in brokers:
            print(f"Broker ID: {broker.nodeId}, "
                  f"Host: {broker.host}, "
                  f"Port: {broker.port}")
        return brokers
    
    def describe_cluster(self):
        metadata = self.admin_client._client.cluster
        
        print(f"=== é›†ç¾¤ä¿¡æ¯ ===")
        print(f"Cluster ID: {metadata.cluster_id()}")
        print(f"Controller ID: {metadata.controller().nodeId if metadata.controller() else 'Unknown'}")
        print(f"Broker æ•¸é‡: {len(list(metadata.brokers()))}")
        print(f"Topic æ•¸é‡: {len(metadata.topics())}")
    
    def list_topics(self):
        topics = self.admin_client.list_topics()
        
        print(f"\n=== Topic åˆ—è¡¨ ({len(topics)} å€‹) ===")
        for topic in sorted(topics):
            if not topic.startswith('__'):
                print(f"  - {topic}")
        return topics
    
    def describe_topic(self, topic_name):
        metadata = self.admin_client.describe_topics([topic_name])
        topic_info = metadata[topic_name]
        
        print(f"\n=== Topic: {topic_name} ===")
        print(f"åˆ†å€æ•¸: {len(topic_info['partitions'])}")
        
        for partition in topic_info['partitions']:
            print(f"\nPartition {partition['partition']}:")
            print(f"  Leader: Broker {partition['leader']}")
            print(f"  Replicas: {partition['replicas']}")
            print(f"  ISR: {partition['isr']}")
    
    def get_broker_config(self, broker_id):
        resource = ConfigResource(ConfigResourceType.BROKER, str(broker_id))
        configs = self.admin_client.describe_configs([resource])
        
        print(f"\n=== Broker {broker_id} é…ç½® ===")
        for config in configs[resource]:
            if not config[1].is_default:
                print(f"{config[0]}: {config[1].value}")
    
    def close(self):
        self.admin_client.close()

if __name__ == '__main__':
    manager = KafkaClusterManager(['localhost:9092'])
    
    manager.describe_cluster()
    manager.list_brokers()
    manager.list_topics()
    manager.describe_topic('user-events')
    manager.get_broker_config(1)
    
    manager.close()
```

### å‰µå»ºå’Œç®¡ç† Topic

```python
from kafka.admin import KafkaAdminClient, NewTopic, ConfigResource, ConfigResourceType
from kafka.errors import TopicAlreadyExistsError, UnknownTopicOrPartitionError

class TopicManager:
    def __init__(self, bootstrap_servers):
        self.admin_client = KafkaAdminClient(
            bootstrap_servers=bootstrap_servers,
            client_id='topic-manager'
        )
    
    def create_topic(self, name, num_partitions=6, replication_factor=3, config=None):
        topic = NewTopic(
            name=name,
            num_partitions=num_partitions,
            replication_factor=replication_factor,
            topic_configs=config or {}
        )
        
        try:
            result = self.admin_client.create_topics([topic], validate_only=False)
            print(f"Topic '{name}' å‰µå»ºæˆåŠŸ")
            return True
        except TopicAlreadyExistsError:
            print(f"Topic '{name}' å·²å­˜åœ¨")
            return False
        except Exception as e:
            print(f"å‰µå»ºå¤±æ•—: {e}")
            return False
    
    def delete_topic(self, name):
        try:
            self.admin_client.delete_topics([name])
            print(f"Topic '{name}' åˆªé™¤æˆåŠŸ")
            return True
        except UnknownTopicOrPartitionError:
            print(f"Topic '{name}' ä¸å­˜åœ¨")
            return False
        except Exception as e:
            print(f"åˆªé™¤å¤±æ•—: {e}")
            return False
    
    def update_topic_config(self, topic_name, configs):
        resource = ConfigResource(ConfigResourceType.TOPIC, topic_name, configs=configs)
        
        try:
            self.admin_client.alter_configs([resource])
            print(f"Topic '{topic_name}' é…ç½®æ›´æ–°æˆåŠŸ")
            return True
        except Exception as e:
            print(f"æ›´æ–°å¤±æ•—: {e}")
            return False
    
    def create_partitions(self, topic_name, new_total_count):
        topic_partitions = {topic_name: new_total_count}
        
        try:
            self.admin_client.create_partitions(topic_partitions)
            print(f"Topic '{topic_name}' åˆ†å€æ•¸å¢åŠ åˆ° {new_total_count}")
            return True
        except Exception as e:
            print(f"å¢åŠ åˆ†å€å¤±æ•—: {e}")
            return False
    
    def close(self):
        self.admin_client.close()

if __name__ == '__main__':
    manager = TopicManager(['localhost:9092'])
    
    manager.create_topic(
        name='user-events',
        num_partitions=12,
        replication_factor=3,
        config={
            'retention.ms': '604800000',
            'compression.type': 'lz4',
            'min.insync.replicas': '2',
            'max.message.bytes': '1048576'
        }
    )
    
    manager.update_topic_config(
        'user-events',
        {'retention.ms': '86400000'}
    )
    
    manager.create_partitions('user-events', 18)
    
    manager.close()
```

## æ•…éšœè™•ç†èˆ‡æ¢å¾©

### Broker æ•…éšœæ¢å¾©æµç¨‹

```mermaid
flowchart TB
    A[Broker æ•…éšœ] --> B{æ˜¯å¦ç‚º Controller?}
    B -->|æ˜¯| C[é‡æ–°é¸èˆ‰ Controller]
    B -->|å¦| D[æª¢æ¸¬åˆ†å€ Leader]
    
    C --> E[æ–° Controller æ¥ç®¡]
    E --> D
    
    D --> F{Leader åœ¨æ•…éšœ Broker?}
    F -->|æ˜¯| G[å¾ ISR é¸èˆ‰æ–° Leader]
    F -->|å¦| H[ç„¡å½±éŸ¿]
    
    G --> I[æ›´æ–°å…ƒæ•¸æ“š]
    I --> J[é€šçŸ¥æ‰€æœ‰ Broker]
    J --> K[Producer/Consumer é‡é€£]
    
    H --> L[ç¹¼çºŒæœå‹™]
    K --> L
```

### å¸¸è¦‹æ•…éšœæ’æŸ¥

#### 1. Broker ç„¡æ³•å•Ÿå‹•

```python
import subprocess
import json

def check_broker_logs(container_name='broker-1'):
    try:
        result = subprocess.run(
            ['docker', 'logs', '--tail', '50', container_name],
            capture_output=True,
            text=True
        )
        
        logs = result.stdout
        
        if 'java.net.BindException' in logs:
            print("âŒ ç«¯å£å·²è¢«å ç”¨")
        elif 'org.apache.zookeeper.KeeperException$NodeExistsException' in logs:
            print("âŒ ZooKeeper ç¯€é»è¡çª")
        elif 'LEADER_NOT_AVAILABLE' in logs:
            print("âš ï¸  Leader é¸èˆ‰ä¸­")
        else:
            print("âœ… æ²’æœ‰ç™¼ç¾æ˜é¡¯éŒ¯èª¤")
        
        return logs
    except Exception as e:
        print(f"ç„¡æ³•æª¢æŸ¥æ—¥èªŒ: {e}")

check_broker_logs('broker-1')
```

#### 2. Under-Replicated Partitions (URP)

```python
from kafka import KafkaAdminClient

def find_under_replicated_partitions(bootstrap_servers):
    admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)
    
    topics = admin_client.list_topics()
    under_replicated = []
    
    for topic in topics:
        if topic.startswith('__'):
            continue
        
        metadata = admin_client.describe_topics([topic])
        topic_info = metadata[topic]
        
        for partition in topic_info['partitions']:
            replicas = set(partition['replicas'])
            isr = set(partition['isr'])
            
            if replicas != isr:
                under_replicated.append({
                    'topic': topic,
                    'partition': partition['partition'],
                    'leader': partition['leader'],
                    'replicas': partition['replicas'],
                    'isr': partition['isr'],
                    'missing': list(replicas - isr)
                })
    
    print(f"=== Under-Replicated Partitions ({len(under_replicated)}) ===")
    for urp in under_replicated:
        print(f"\nTopic: {urp['topic']}, Partition: {urp['partition']}")
        print(f"  Leader: {urp['leader']}")
        print(f"  Replicas: {urp['replicas']}")
        print(f"  ISR: {urp['isr']}")
        print(f"  âŒ Missing: {urp['missing']}")
    
    admin_client.close()
    return under_replicated

find_under_replicated_partitions(['localhost:9092'])
```

#### 3. Consumer Lag ç›£æ§

```python
from kafka import KafkaAdminClient, KafkaConsumer
from kafka.structs import TopicPartition

def check_consumer_lag(bootstrap_servers, group_id):
    admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)
    
    consumer = KafkaConsumer(
        bootstrap_servers=bootstrap_servers,
        group_id=group_id,
        enable_auto_commit=False
    )
    
    group_offsets = admin_client.list_consumer_group_offsets(group_id)
    
    print(f"=== Consumer Group: {group_id} ===\n")
    
    total_lag = 0
    for tp, offset_metadata in group_offsets.items():
        current_offset = offset_metadata.offset
        
        consumer.assign([tp])
        consumer.seek_to_end(tp)
        end_offset = consumer.position(tp)
        
        lag = end_offset - current_offset
        total_lag += lag
        
        if lag > 0:
            print(f"Topic: {tp.topic}, Partition: {tp.partition}")
            print(f"  Current Offset: {current_offset}")
            print(f"  End Offset: {end_offset}")
            print(f"  âš ï¸  Lag: {lag}\n")
    
    print(f"ç¸½å»¶é²: {total_lag} æ¢æ¶ˆæ¯")
    
    consumer.close()
    admin_client.close()
    
    return total_lag

check_consumer_lag(['localhost:9092'], 'analytics-group')
```

### æ•¸æ“šå‚™ä»½èˆ‡æ¢å¾©

```python
from kafka import KafkaConsumer, KafkaProducer
import json
from datetime import datetime

class TopicBackup:
    def __init__(self, bootstrap_servers):
        self.bootstrap_servers = bootstrap_servers
    
    def backup_topic(self, topic, output_file):
        consumer = KafkaConsumer(
            topic,
            bootstrap_servers=self.bootstrap_servers,
            auto_offset_reset='earliest',
            enable_auto_commit=False,
            value_deserializer=lambda m: m.decode('utf-8', errors='ignore')
        )
        
        messages = []
        count = 0
        
        print(f"é–‹å§‹å‚™ä»½ Topic: {topic}")
        
        for message in consumer:
            messages.append({
                'partition': message.partition,
                'offset': message.offset,
                'timestamp': message.timestamp,
                'key': message.key.decode('utf-8') if message.key else None,
                'value': message.value
            })
            count += 1
            
            if count % 1000 == 0:
                print(f"å·²å‚™ä»½ {count} æ¢æ¶ˆæ¯...")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'topic': topic,
                'backup_time': datetime.now().isoformat(),
                'message_count': len(messages),
                'messages': messages
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å‚™ä»½å®Œæˆ: {count} æ¢æ¶ˆæ¯ -> {output_file}")
        consumer.close()
    
    def restore_topic(self, input_file, target_topic=None):
        with open(input_file, 'r', encoding='utf-8') as f:
            backup_data = json.load(f)
        
        topic = target_topic or backup_data['topic']
        messages = backup_data['messages']
        
        producer = KafkaProducer(
            bootstrap_servers=self.bootstrap_servers,
            key_serializer=lambda k: k.encode('utf-8') if k else None,
            value_serializer=lambda v: v.encode('utf-8') if isinstance(v, str) else v
        )
        
        print(f"é–‹å§‹æ¢å¾©åˆ° Topic: {topic}")
        
        for i, msg in enumerate(messages):
            producer.send(
                topic,
                key=msg['key'],
                value=msg['value'],
                partition=msg['partition']
            )
            
            if (i + 1) % 1000 == 0:
                print(f"å·²æ¢å¾© {i + 1} æ¢æ¶ˆæ¯...")
        
        producer.flush()
        producer.close()
        
        print(f"âœ… æ¢å¾©å®Œæˆ: {len(messages)} æ¢æ¶ˆæ¯")

if __name__ == '__main__':
    backup = TopicBackup(['localhost:9092'])
    
    backup.backup_topic('user-events', 'user-events-backup.json')
    
    backup.restore_topic('user-events-backup.json', 'user-events-restored')
```

## é›†ç¾¤æ“´å®¹èˆ‡ç¸®å®¹

### æ·»åŠ æ–° Broker

```python
from kafka.admin import KafkaAdminClient, NewPartitions
import json

def add_broker_reassign_partitions(admin_client, topic_name):
    metadata = admin_client.describe_topics([topic_name])
    topic_info = metadata[topic_name]
    
    print(f"Topic '{topic_name}' ç•¶å‰åˆ†å€åˆ†ä½ˆ:")
    for partition in topic_info['partitions']:
        print(f"  Partition {partition['partition']}: Replicas={partition['replicas']}")
    
    print("\nâš ï¸  éœ€è¦ä½¿ç”¨ kafka-reassign-partitions.sh å·¥å…·é€²è¡Œåˆ†å€é‡åˆ†é…")
    print("ç¤ºä¾‹å‘½ä»¤:")
    print("""
    kafka-reassign-partitions.sh \\
      --bootstrap-server localhost:9092 \\
      --topics-to-move-json-file topics.json \\
      --broker-list "1,2,3,4" \\
      --generate
    """)

admin_client = KafkaAdminClient(bootstrap_servers=['localhost:9092'])
add_broker_reassign_partitions(admin_client, 'user-events')
admin_client.close()
```

### å®‰å…¨ä¸‹ç·š Broker

```bash
kafka-configs.sh --bootstrap-server localhost:9092 \
  --entity-type brokers \
  --entity-name 3 \
  --alter \
  --add-config follower.replication.throttled.rate=10485760

kafka-reassign-partitions.sh \
  --bootstrap-server localhost:9092 \
  --reassignment-json-file reassignment.json \
  --execute
```

## æ ¸å¿ƒé…ç½®åƒæ•¸

### Broker é…ç½®

```properties
broker.id=1

log.dirs=/var/lib/kafka/data

num.network.threads=8
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600

num.partitions=6
default.replication.factor=3
min.insync.replicas=2

log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000

zookeeper.connect=localhost:2181
zookeeper.connection.timeout.ms=18000

auto.create.topics.enable=false
delete.topic.enable=true

compression.type=producer
```

## æœ€ä½³å¯¦è¸

### 1. é›†ç¾¤è¦æ¨¡è¦åŠƒ

| å ´æ™¯ | Broker æ•¸é‡ | å‰¯æœ¬å› å­ | åˆ†å€æ•¸ |
|------|------------|---------|--------|
| é–‹ç™¼/æ¸¬è©¦ | 1-3 | 1-2 | 3-6 |
| å°å‹ç”Ÿç”¢ | 3-5 | 3 | 6-12 |
| ä¸­å‹ç”Ÿç”¢ | 5-10 | 3 | 12-24 |
| å¤§å‹ç”Ÿç”¢ | 10+ | 3 | 24+ |

### 2. ç›£æ§æŒ‡æ¨™

```python
from kafka import KafkaAdminClient
import time

def monitor_cluster_health(bootstrap_servers):
    admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)
    
    while True:
        metadata = admin_client._client.cluster
        
        brokers = list(metadata.brokers())
        topics = metadata.topics()
        
        print(f"\n=== {time.strftime('%Y-%m-%d %H:%M:%S')} ===")
        print(f"âœ… Active Brokers: {len(brokers)}")
        print(f"ğŸ“Š Total Topics: {len([t for t in topics if not t.startswith('__')])}")
        
        time.sleep(30)

monitor_cluster_health(['localhost:9092'])
```

### 3. å®šæœŸå·¡æª¢è…³æœ¬

```python
from kafka import KafkaAdminClient
from datetime import datetime

def cluster_health_check(bootstrap_servers):
    admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'issues': []
    }
    
    topics = admin_client.list_topics()
    
    for topic in topics:
        if topic.startswith('__'):
            continue
        
        metadata = admin_client.describe_topics([topic])
        topic_info = metadata[topic]
        
        for partition in topic_info['partitions']:
            if partition['leader'] == -1:
                report['issues'].append({
                    'type': 'NO_LEADER',
                    'topic': topic,
                    'partition': partition['partition']
                })
            
            replicas = set(partition['replicas'])
            isr = set(partition['isr'])
            
            if replicas != isr:
                report['issues'].append({
                    'type': 'UNDER_REPLICATED',
                    'topic': topic,
                    'partition': partition['partition'],
                    'missing_replicas': list(replicas - isr)
                })
    
    print(f"=== é›†ç¾¤å¥åº·æª¢æŸ¥å ±å‘Š ===")
    print(f"æª¢æŸ¥æ™‚é–“: {report['timestamp']}")
    print(f"ç™¼ç¾å•é¡Œ: {len(report['issues'])} å€‹\n")
    
    for issue in report['issues']:
        print(f"âŒ {issue['type']}")
        print(f"   Topic: {issue['topic']}, Partition: {issue['partition']}")
        if 'missing_replicas' in issue:
            print(f"   Missing Replicas: {issue['missing_replicas']}")
        print()
    
    admin_client.close()
    return report

cluster_health_check(['localhost:9092'])
```

## ä¸‹ä¸€æ­¥

- [05-æ•ˆèƒ½èª¿å„ªèˆ‡ç›£æ§](./05-æ•ˆèƒ½èª¿å„ªèˆ‡ç›£æ§.md): å­¸ç¿’æ•ˆèƒ½å„ªåŒ–æŠ€å·§
- [06-å¯¦æˆ°æ¡ˆä¾‹èˆ‡æœ€ä½³å¯¦è¸](./06-å¯¦æˆ°æ¡ˆä¾‹èˆ‡æœ€ä½³å¯¦è¸.md): ç”Ÿç”¢ç’°å¢ƒå¯¦æˆ°ç¶“é©—

## åƒè€ƒè³‡æº

- [Kafka Operations Guide](https://kafka.apache.org/documentation/#operations)
- [KRaft Documentation](https://kafka.apache.org/documentation/#kraft)
