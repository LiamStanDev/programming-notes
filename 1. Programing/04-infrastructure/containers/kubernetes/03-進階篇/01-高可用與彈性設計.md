# 01-é«˜å¯ç”¨èˆ‡å½ˆæ€§è¨­è¨ˆ

> HPA/VPA è‡ªå‹•æ“´ç¸®å®¹ã€å¥åº·æª¢æŸ¥èˆ‡æ•…éšœæ¢å¾©

---

## ğŸ“š æœ¬ç« ç›®æ¨™

- æŒæ¡ HPA/VPA è‡ªå‹•æ“´ç¸®å®¹
- é…ç½®å¥åº·æª¢æŸ¥èˆ‡å„ªé›…çµ‚æ­¢
- ç†è§£ PodDisruptionBudget
- å¯¦ç¾é«˜å¯ç”¨æ¶æ§‹è¨­è¨ˆ
- æŒæ¡è³‡æºç®¡ç†æœ€ä½³å¯¦è¸

---

## 1. è‡ªå‹•æ“´ç¸®å®¹

### 1.1 HPAï¼ˆHorizontal Pod Autoscalerï¼‰

```mermaid
graph TB
    subgraph "HPA å·¥ä½œåŸç†"
        M[Metrics Server] -->|æ”¶é›†æŒ‡æ¨™| HPA[HPA Controller]
        HPA -->|è¨ˆç®—å‰¯æœ¬æ•¸| D[Deployment]
        D -->|èª¿æ•´| P[Pods]
        
        P -.CPU/Memoryä½¿ç”¨ç‡.-> M
    end
    
    User[è² è¼‰å¢åŠ ] -.å½±éŸ¿.-> P
    
    style HPA fill:#9cf
    style M fill:#fc9
```

**åŸºæ–¼ CPU çš„ HPAï¼š**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  
  minReplicas: 3
  maxReplicas: 10
  
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Min
    
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
      selectPolicy: Max
```

**å¤šæŒ‡æ¨™ HPAï¼š**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-multi-metric
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  
  minReplicas: 3
  maxReplicas: 20
  
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
  
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        name: webapp-ingress
      target:
        type: Value
        value: "10k"
```

**æŸ¥çœ‹ HPA ç‹€æ…‹ï¼š**
```bash
kubectl get hpa

kubectl describe hpa webapp-hpa

kubectl get hpa webapp-hpa --watch
```

### 1.2 VPAï¼ˆVertical Pod Autoscalerï¼‰

```mermaid
graph LR
    A[VPA æ¨è–¦æ¨¡å¼] --> B[ç›£æ§è³‡æºä½¿ç”¨]
    B --> C[è¨ˆç®—æ¨è–¦å€¼]
    C --> D{æ›´æ–°æ¨¡å¼?}
    
    D -->|Off| E[åƒ…æ¨è–¦]
    D -->|Initial| F[å‰µå»ºæ™‚è¨­ç½®]
    D -->|Recreate| G[é‡å»º Pod]
    D -->|Auto| H[è‡ªå‹•èª¿æ•´]
    
    style A fill:#9cf
    style H fill:#f96
```

**å®‰è£ VPAï¼š**
```bash
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler
./hack/vpa-up.sh
```

**VPA é…ç½®ï¼š**
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: webapp-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: webapp
  
  updatePolicy:
    updateMode: "Auto"
  
  resourcePolicy:
    containerPolicies:
    - containerName: webapp
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 2Gi
      controlledResources: ["cpu", "memory"]
      mode: Auto
```

**æŸ¥çœ‹ VPA æ¨è–¦ï¼š**
```bash
kubectl describe vpa webapp-vpa
```

### 1.3 KEDAï¼ˆäº‹ä»¶é©…å‹•æ“´ç¸®å®¹ï¼‰

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: kafka-consumer-scaler
spec:
  scaleTargetRef:
    name: kafka-consumer
  
  minReplicaCount: 1
  maxReplicaCount: 30
  
  triggers:
  - type: kafka
    metadata:
      bootstrapServers: kafka:9092
      consumerGroup: my-group
      topic: orders
      lagThreshold: "50"
  
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: http_requests_total
      threshold: "1000"
      query: sum(rate(http_requests_total[2m]))
```

---

## 2. å¥åº·æª¢æŸ¥æ©Ÿåˆ¶

### 2.1 Probe é¡å‹å°æ¯”

```mermaid
graph TB
    subgraph "ä¸‰ç¨® Probe"
        LP[Liveness Probe<br/>å­˜æ´»æ¢é‡]
        RP[Readiness Probe<br/>å°±ç·’æ¢é‡]
        SP[Startup Probe<br/>å•Ÿå‹•æ¢é‡]
    end
    
    LP -->|å¤±æ•—| R1[é‡å•Ÿå®¹å™¨]
    RP -->|å¤±æ•—| R2[ç§»å‡º Endpoint]
    SP -->|å¤±æ•—| R3[ä¸åŸ·è¡Œå…¶ä»–æ¢é‡]
    
    SP -.æˆåŠŸå¾Œ.-> LP
    SP -.æˆåŠŸå¾Œ.-> RP
    
    style LP fill:#f96
    style RP fill:#fc9
    style SP fill:#9cf
```

### 2.2 Liveness Probeï¼ˆå­˜æ´»æ¢é‡ï¼‰

**ä½œç”¨ï¼š** æª¢æ¸¬å®¹å™¨æ˜¯å¦å­˜æ´»ï¼Œå¤±æ•—å‰‡é‡å•Ÿ

**HTTP æª¢æŸ¥ï¼š**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: app
    image: myapp:v1.0
    
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
        - name: Custom-Header
          value: Awesome
      
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      successThreshold: 1
      failureThreshold: 3
```

**TCP æª¢æŸ¥ï¼š**
```yaml
livenessProbe:
  tcpSocket:
    port: 8080
  initialDelaySeconds: 15
  periodSeconds: 10
```

**å‘½ä»¤æª¢æŸ¥ï¼š**
```yaml
livenessProbe:
  exec:
    command:
    - cat
    - /tmp/healthy
  initialDelaySeconds: 5
  periodSeconds: 5
```

**æ‡‰ç”¨ç«¯å¯¦ç¾ï¼ˆPython Flaskï¼‰ï¼š**
```python
from flask import Flask, jsonify
import psutil
import time

app = Flask(__name__)
start_time = time.time()

@app.route('/healthz')
def healthz():
    cpu_percent = psutil.cpu_percent(interval=1)
    memory_percent = psutil.virtual_memory().percent
    
    if cpu_percent > 95 or memory_percent > 95:
        return jsonify({
            'status': 'unhealthy',
            'cpu': cpu_percent,
            'memory': memory_percent
        }), 500
    
    return jsonify({
        'status': 'healthy',
        'uptime': time.time() - start_time
    }), 200
```

### 2.3 Readiness Probeï¼ˆå°±ç·’æ¢é‡ï¼‰

**ä½œç”¨ï¼š** æª¢æ¸¬å®¹å™¨æ˜¯å¦å°±ç·’æ¥æ”¶æµé‡ï¼Œå¤±æ•—å‰‡ç§»å‡º Service Endpoints

```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  
  initialDelaySeconds: 5
  periodSeconds: 5
  timeoutSeconds: 3
  successThreshold: 1
  failureThreshold: 3
```

**æ‡‰ç”¨ç«¯å¯¦ç¾ï¼š**
```python
import redis
import psycopg2

@app.route('/ready')
def ready():
    try:
        r = redis.Redis(host='redis', port=6379)
        r.ping()
        
        conn = psycopg2.connect(
            host="postgres",
            database="mydb",
            user="admin",
            password=os.getenv("DB_PASSWORD")
        )
        conn.close()
        
        return jsonify({'status': 'ready'}), 200
    
    except Exception as e:
        return jsonify({
            'status': 'not ready',
            'error': str(e)
        }), 503
```

### 2.4 Startup Probeï¼ˆå•Ÿå‹•æ¢é‡ï¼‰

**ä½œç”¨ï¼š** é©ç”¨æ–¼å•Ÿå‹•æ…¢çš„æ‡‰ç”¨ï¼Œé¿å…éæ—©åŸ·è¡Œå…¶ä»–æ¢é‡

```yaml
startupProbe:
  httpGet:
    path: /startup
    port: 8080
  
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 3
  successThreshold: 1
  failureThreshold: 30
```

**å®Œæ•´é…ç½®ç¤ºä¾‹ï¼š**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: myapp:v1.0
        
        startupProbe:
          httpGet:
            path: /startup
            port: 8080
          failureThreshold: 30
          periodSeconds: 10
        
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
```

---

## 3. å„ªé›…çµ‚æ­¢èˆ‡ç”Ÿå‘½é€±æœŸ

### 3.1 Pod çµ‚æ­¢æµç¨‹

```mermaid
sequenceDiagram
    participant K8s
    participant Pod
    participant App
    participant LB as Service/LB
    
    K8s->>Pod: 1. ç™¼é€ TERM ä¿¡è™Ÿ
    K8s->>LB: 2. ç§»å‡º Endpoints
    
    Pod->>App: preStop Hook
    App->>App: è™•ç†é€²è¡Œä¸­è«‹æ±‚
    App->>App: é—œé–‰æ–°é€£æ¥
    App->>App: æ¸…ç†è³‡æº
    
    Note over Pod: terminationGracePeriodSeconds<br/>(é»˜èª 30ç§’)
    
    alt å„ªé›…é—œé–‰æˆåŠŸ
        App-->>Pod: é€€å‡º (code 0)
        Pod-->>K8s: çµ‚æ­¢å®Œæˆ
    else è¶…æ™‚
        K8s->>Pod: 3. ç™¼é€ KILL ä¿¡è™Ÿ
        Pod-->>K8s: å¼·åˆ¶çµ‚æ­¢
    end
```

### 3.2 PreStop Hook

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  template:
    spec:
      terminationGracePeriodSeconds: 60
      
      containers:
      - name: app
        image: myapp:v1.0
        
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - |
                sleep 5
                kill -TERM 1
                while killall -0 myapp; do sleep 1; done
```

**æ‡‰ç”¨ç«¯å„ªé›…é—œé–‰ï¼ˆPythonï¼‰ï¼š**
```python
import signal
import sys
import time

shutdown_flag = False

def graceful_shutdown(signum, frame):
    global shutdown_flag
    print("Received shutdown signal, gracefully shutting down...")
    shutdown_flag = True

signal.signal(signal.SIGTERM, graceful_shutdown)
signal.signal(signal.SIGINT, graceful_shutdown)

def main():
    while not shutdown_flag:
        time.sleep(1)
    
    print("Closing database connections...")
    db.close()
    
    print("Waiting for in-flight requests to complete...")
    time.sleep(5)
    
    print("Shutdown complete")
    sys.exit(0)

if __name__ == '__main__':
    main()
```

**Go ç¤ºä¾‹ï¼š**
```go
package main

import (
    "context"
    "net/http"
    "os"
    "os/signal"
    "syscall"
    "time"
)

func main() {
    srv := &http.Server{Addr: ":8080"}
    
    go func() {
        if err := srv.ListenAndServe(); err != nil && err != http.ErrServerClosed {
            panic(err)
        }
    }()
    
    quit := make(chan os.Signal, 1)
    signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
    <-quit
    
    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()
    
    if err := srv.Shutdown(ctx); err != nil {
        panic(err)
    }
}
```

---

## 4. PodDisruptionBudget

### 4.1 æ¦‚å¿µèˆ‡ç”¨é€”

```mermaid
graph TB
    A[è¨ˆåŠƒæ€§ç¶­è­·] --> PDB[PodDisruptionBudget]
    B[ç¯€é»å‡ç´š] --> PDB
    C[è‡ªé¡˜é©…é€] --> PDB
    
    PDB --> D{æª¢æŸ¥å¯ç”¨å‰¯æœ¬}
    D -->|æ»¿è¶³| E[å…è¨±é©…é€]
    D -->|ä¸æ»¿è¶³| F[æ‹’çµ•é©…é€]
    
    style PDB fill:#9cf
    style E fill:#9f9
    style F fill:#f96
```

### 4.2 é…ç½®ç¤ºä¾‹

**æœ€å°‘å¯ç”¨å‰¯æœ¬æ•¸ï¼š**
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webapp-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: webapp
```

**æœ€å¤šä¸å¯ç”¨å‰¯æœ¬æ•¸ï¼š**
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webapp-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: webapp
```

**ç™¾åˆ†æ¯”é…ç½®ï¼š**
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webapp-pdb
spec:
  minAvailable: 75%
  selector:
    matchLabels:
      app: webapp
```

**æŸ¥çœ‹ PDB ç‹€æ…‹ï¼š**
```bash
kubectl get pdb

kubectl describe pdb webapp-pdb
```

---

## 5. è³‡æºç®¡ç†

### 5.1 Requests vs Limits

```mermaid
graph TB
    subgraph "è³‡æºé…ç½®å½±éŸ¿"
        A[Requests<br/>èª¿åº¦ä¾æ“š] --> S[Scheduler]
        B[Limits<br/>ç¡¬æ€§é™åˆ¶] --> C[Cgroup]
        
        S -->|ä¿è­‰è³‡æº| P1[Pod è¢«èª¿åº¦]
        C -->|è¶…å‡ºé™åˆ¶| P2[OOM Killed]
    end
    
    style A fill:#9cf
    style B fill:#fc9
    style P2 fill:#f96
```

**æœ€ä½³å¯¦è¸é…ç½®ï¼š**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:v1.0
        
        resources:
          requests:
            cpu: "500m"
            memory: "512Mi"
          limits:
            cpu: "1000m"
            memory: "1Gi"
```

**QoS é¡åˆ¥ï¼š**

| QoS Class | æ¢ä»¶ | å„ªå…ˆç´š | è¢«é©…é€é¢¨éšª |
|-----------|------|--------|-----------|
| **Guaranteed** | requests = limits | æœ€é«˜ | æœ€ä½ |
| **Burstable** | requests < limits | ä¸­ç­‰ | ä¸­ç­‰ |
| **BestEffort** | ç„¡ requests/limits | æœ€ä½ | æœ€é«˜ |

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: guaranteed-pod
spec:
  containers:
  - name: app
    resources:
      requests:
        cpu: "500m"
        memory: "512Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"
```

### 5.2 ResourceQuota

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: production
spec:
  hard:
    requests.cpu: "50"
    requests.memory: 100Gi
    limits.cpu: "100"
    limits.memory: 200Gi
    
    pods: "100"
    services: "50"
    persistentvolumeclaims: "20"
    
    requests.storage: 500Gi
```

### 5.3 LimitRange

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: resource-limits
  namespace: production
spec:
  limits:
  - max:
      cpu: "4"
      memory: "8Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "200m"
      memory: "256Mi"
    type: Container
  
  - max:
      storage: "100Gi"
    min:
      storage: "1Gi"
    type: PersistentVolumeClaim
```

---

## 6. é«˜å¯ç”¨æ¶æ§‹è¨­è¨ˆ

### 6.1 å¤šå‰¯æœ¬éƒ¨ç½²

```mermaid
graph TB
    subgraph "é«˜å¯ç”¨éƒ¨ç½²"
        LB[Load Balancer]
        
        subgraph "Zone A"
            P1[Pod 1]
            P2[Pod 2]
        end
        
        subgraph "Zone B"
            P3[Pod 3]
            P4[Pod 4]
        end
        
        subgraph "Zone C"
            P5[Pod 5]
        end
    end
    
    LB --> P1
    LB --> P2
    LB --> P3
    LB --> P4
    LB --> P5
    
    style LB fill:#9cf
```

**åè¦ªå’Œæ€§é…ç½®ï¼š**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 5
  
  template:
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - webapp
            topologyKey: "kubernetes.io/hostname"
          
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - webapp
              topologyKey: topology.kubernetes.io/zone
```

### 6.2 å¤šå¯ç”¨å€éƒ¨ç½²

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 9
  
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: webapp
              topologyKey: topology.kubernetes.io/zone
      
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: webapp
      
      - maxSkew: 2
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: webapp
```

---

## 7. å®Œæ•´é«˜å¯ç”¨é…ç½®ç¤ºä¾‹

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-ha
  labels:
    app: webapp
spec:
  replicas: 5
  
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  
  selector:
    matchLabels:
      app: webapp
  
  template:
    metadata:
      labels:
        app: webapp
        version: v1.0
    
    spec:
      terminationGracePeriodSeconds: 60
      
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: webapp
              topologyKey: kubernetes.io/hostname
      
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: webapp
      
      containers:
      - name: app
        image: myapp:v1.0
        
        ports:
        - containerPort: 8080
          name: http
        
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
        
        startupProbe:
          httpGet:
            path: /startup
            port: 8080
          failureThreshold: 30
          periodSeconds: 10
        
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - sleep 15

---
apiVersion: v1
kind: Service
metadata:
  name: webapp
spec:
  type: ClusterIP
  selector:
    app: webapp
  ports:
  - port: 80
    targetPort: 8080
  sessionAffinity: ClientIP

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webapp-pdb
spec:
  minAvailable: 3
  selector:
    matchLabels:
      app: webapp

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp-ha
  
  minReplicas: 5
  maxReplicas: 20
  
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Pods
        value: 2
        periodSeconds: 60
```

---

## 8. å°çµ

æœ¬ç« ä»‹ç´¹äº† Kubernetes é«˜å¯ç”¨èˆ‡å½ˆæ€§è¨­è¨ˆçš„æ ¸å¿ƒæŠ€è¡“ï¼š

**è‡ªå‹•æ“´ç¸®å®¹ï¼š**
- âœ… HPA - æ°´å¹³æ“´ç¸®å®¹ï¼ˆåŸºæ–¼ CPU/Memory/è‡ªå®šç¾©æŒ‡æ¨™ï¼‰
- âœ… VPA - å‚ç›´æ“´ç¸®å®¹ï¼ˆè‡ªå‹•èª¿æ•´è³‡æºè«‹æ±‚ï¼‰
- âœ… KEDA - äº‹ä»¶é©…å‹•æ“´ç¸®å®¹ï¼ˆKafka/æ¶ˆæ¯éšŠåˆ—ï¼‰

**å¥åº·æª¢æŸ¥ï¼š**
- âœ… Liveness Probe - å®¹å™¨å­˜æ´»æª¢æŸ¥
- âœ… Readiness Probe - æœå‹™å°±ç·’æª¢æŸ¥
- âœ… Startup Probe - æ…¢å•Ÿå‹•æ‡‰ç”¨æ”¯æŒ

**é«˜å¯ç”¨ä¿éšœï¼š**
- âœ… PodDisruptionBudget - è¨ˆåŠƒæ€§ç¶­è­·ä¿è­·
- âœ… å¤šå‰¯æœ¬ + åè¦ªå’Œæ€§ - é¿å…å–®é»æ•…éšœ
- âœ… å¤šå¯ç”¨å€éƒ¨ç½² - å€åŸŸç´šå®¹ç½
- âœ… å„ªé›…çµ‚æ­¢ - é›¶åœæ©Ÿæ›´æ–°

**è³‡æºç®¡ç†ï¼š**
- âœ… Requests/Limits - è³‡æºéš”é›¢èˆ‡ä¿éšœ
- âœ… QoS - æœå‹™è³ªé‡åˆ†ç´š
- âœ… ResourceQuota - å‘½åç©ºé–“é…é¡
- âœ… LimitRange - è³‡æºç¯„åœé™åˆ¶

ä¸‹ä¸€ç« å°‡æ·±å…¥å­¸ç¿’ç›£æ§èˆ‡å¯è§€æ¸¬æ€§ï¼ŒåŒ…æ‹¬ Prometheusã€Grafanaã€ELK å’Œåˆ†ä½ˆå¼è¿½è¹¤ã€‚
