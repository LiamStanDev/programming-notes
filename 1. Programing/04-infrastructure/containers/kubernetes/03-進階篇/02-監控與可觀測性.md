# 02-ç›£æ§èˆ‡å¯è§€æ¸¬æ€§

> Prometheus + Grafana ç›£æ§ã€ELK æ—¥èªŒã€Jaeger è¿½è¹¤èˆ‡ SLO/SLI

---

## ğŸ“š æœ¬ç« ç›®æ¨™

- æŒæ¡ Prometheus + Grafana ç›£æ§é«”ç³»
- é…ç½® ELK/Loki æ—¥èªŒæ”¶é›†
- å¯¦ç¾ Jaeger åˆ†ä½ˆå¼è¿½è¹¤
- ç†è§£ SLO/SLI/SLA æ¦‚å¿µ
- å»ºç«‹å®Œæ•´çš„å¯è§€æ¸¬æ€§ç³»çµ±

---

## 1. å¯è§€æ¸¬æ€§ä¸‰æ”¯æŸ±

```mermaid
graph TB
    subgraph "å¯è§€æ¸¬æ€§ä¸‰æ”¯æŸ±"
        M[Metrics<br/>æŒ‡æ¨™]
        L[Logs<br/>æ—¥èªŒ]
        T[Traces<br/>è¿½è¹¤]
    end
    
    M --> P[Prometheus]
    M --> G[Grafana]
    
    L --> E[Elasticsearch]
    L --> K[Kibana]
    L --> Loki[Loki]
    
    T --> J[Jaeger]
    T --> Z[Zipkin]
    
    P --> Alert[å‘Šè­¦ç³»çµ±]
    G --> Dash[å„€è¡¨æ¿]
    J --> Trace[éˆè·¯åˆ†æ]
    
    style M fill:#9cf
    style L fill:#fc9
    style T fill:#f9c
```

### 1.1 ä¸‰æ”¯æŸ±ä½œç”¨

| é¡å‹ | ä½œç”¨ | å…¸å‹å•é¡Œ | å·¥å…· |
|-----|------|---------|------|
| **Metrics** | ç³»çµ±å¥åº·ç‹€æ³ | CPU/å…§å­˜ä½¿ç”¨ç‡ã€è«‹æ±‚é€Ÿç‡ | Prometheus |
| **Logs** | äº‹ä»¶è¨˜éŒ„ | éŒ¯èª¤è¨Šæ¯ã€å¯©è¨ˆæ—¥èªŒ | ELK/Loki |
| **Traces** | è«‹æ±‚éˆè·¯ | æ…¢æŸ¥è©¢åˆ†æã€ä¾è³´é—œä¿‚ | Jaeger |

---

## 2. Prometheus ç›£æ§é«”ç³»

### 2.1 æ¶æ§‹æ¦‚è¦½

```mermaid
graph TB
    subgraph "Prometheus æ¶æ§‹"
        P[Prometheus Server]
        AM[Alertmanager]
        PG[Pushgateway]
        
        E1[Node Exporter]
        E2[kube-state-metrics]
        E3[Application Metrics]
    end
    
    P -.pull.-> E1
    P -.pull.-> E2
    P -.pull.-> E3
    
    E3 -.push.-> PG
    PG -.pull.-> P
    
    P -->|è§¸ç™¼å‘Šè­¦| AM
    AM -->|é€šçŸ¥| Slack[Slack/Email/PagerDuty]
    
    Graf[Grafana] -.æŸ¥è©¢.-> P
    
    style P fill:#9cf
    style AM fill:#fc9
```

### 2.2 å®‰è£ Prometheus Stack

**ä½¿ç”¨ Helm å®‰è£ï¼š**
```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheus.prometheusSpec.retention=30d \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=50Gi \
  --set grafana.enabled=true \
  --set alertmanager.enabled=true
```

**é©—è­‰å®‰è£ï¼š**
```bash
kubectl get pods -n monitoring

kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090

kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
```

### 2.3 æ‡‰ç”¨é›†æˆ Prometheus

**Python Flask ç¤ºä¾‹ï¼š**
```python
from flask import Flask
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import time

app = Flask(__name__)

request_count = Counter(
    'app_requests_total',
    'Total number of requests',
    ['method', 'endpoint', 'status']
)

request_duration = Histogram(
    'app_request_duration_seconds',
    'Request duration in seconds',
    ['method', 'endpoint']
)

active_users = Gauge(
    'app_active_users',
    'Number of active users'
)

@app.route('/api/users')
def get_users():
    start_time = time.time()
    
    try:
        users = fetch_users()
        
        request_count.labels(
            method='GET',
            endpoint='/api/users',
            status='200'
        ).inc()
        
        return users, 200
    
    except Exception as e:
        request_count.labels(
            method='GET',
            endpoint='/api/users',
            status='500'
        ).inc()
        
        return str(e), 500
    
    finally:
        duration = time.time() - start_time
        request_duration.labels(
            method='GET',
            endpoint='/api/users'
        ).observe(duration)

@app.route('/metrics')
def metrics():
    return generate_latest()
```

**ServiceMonitor é…ç½®ï¼š**
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: webapp-metrics
  namespace: default
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app: webapp
  
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
```

**Service é…ç½®ï¼š**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  selector:
    app: webapp
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: metrics
    port: 9090
    targetPort: 9090
```

### 2.4 PromQL æŸ¥è©¢

**å¸¸ç”¨æŸ¥è©¢ç¤ºä¾‹ï¼š**

```promql
# CPU ä½¿ç”¨ç‡
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# å…§å­˜ä½¿ç”¨ç‡
(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100

# Pod CPU ä½¿ç”¨é‡
sum(rate(container_cpu_usage_seconds_total{pod=~"webapp-.*"}[5m])) by (pod)

# è«‹æ±‚é€Ÿç‡ï¼ˆQPSï¼‰
rate(http_requests_total[5m])

# éŒ¯èª¤ç‡
rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])

# P50/P90/P99 å»¶é²
histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))
histogram_quantile(0.90, rate(http_request_duration_seconds_bucket[5m]))
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))

# å¯ç”¨æ€§
sum(up{job="webapp"}) / count(up{job="webapp"})
```

### 2.5 å‘Šè­¦è¦å‰‡

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: webapp-alerts
  namespace: monitoring
  labels:
    release: prometheus
spec:
  groups:
  - name: webapp
    interval: 30s
    rules:
    
    - alert: HighErrorRate
      expr: |
        rate(http_requests_total{status=~"5.."}[5m]) 
        / 
        rate(http_requests_total[5m]) > 0.05
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "High error rate detected"
        description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.pod }}"
    
    - alert: HighMemoryUsage
      expr: |
        (container_memory_usage_bytes{pod=~"webapp-.*"} 
        / 
        container_spec_memory_limit_bytes{pod=~"webapp-.*"}) > 0.9
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Pod memory usage is high"
        description: "Pod {{ $labels.pod }} memory usage is {{ $value | humanizePercentage }}"
    
    - alert: PodDown
      expr: up{job="webapp"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Pod is down"
        description: "Pod {{ $labels.pod }} is down"
    
    - alert: HighLatency
      expr: |
        histogram_quantile(0.99, 
          rate(http_request_duration_seconds_bucket[5m])
        ) > 2
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High latency detected"
        description: "P99 latency is {{ $value }}s"
```

**Alertmanager é…ç½®ï¼š**
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-config
  namespace: monitoring
stringData:
  alertmanager.yaml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    
    route:
      group_by: ['alertname', 'cluster']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'default'
      
      routes:
      - match:
          severity: critical
        receiver: 'critical'
        continue: true
      
      - match:
          severity: warning
        receiver: 'warning'
    
    receivers:
    - name: 'default'
      slack_configs:
      - channel: '#alerts'
        title: 'Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    
    - name: 'critical'
      slack_configs:
      - channel: '#critical-alerts'
        title: 'ğŸš¨ CRITICAL: {{ .GroupLabels.alertname }}'
      
      pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_KEY'
    
    - name: 'warning'
      slack_configs:
      - channel: '#warnings'
        title: 'âš ï¸ Warning: {{ .GroupLabels.alertname }}'
```

---

## 3. Grafana å„€è¡¨æ¿

### 3.1 è¨ªå• Grafana

```bash
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80

# ç²å–é»˜èªå¯†ç¢¼
kubectl get secret -n monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 --decode
```

### 3.2 å°å…¥å„€è¡¨æ¿

**å¸¸ç”¨å„€è¡¨æ¿ IDï¼š**
- **Kubernetes Cluster Monitoring** - 315
- **Node Exporter Full** - 1860
- **Pod Monitoring** - 6417
- **Ingress Nginx** - 9614

**è‡ªå®šç¾©å„€è¡¨æ¿ç¤ºä¾‹ï¼š**
```json
{
  "dashboard": {
    "title": "Webapp Dashboard",
    "panels": [
      {
        "title": "Request Rate (QPS)",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{app=\"webapp\"}[5m]))"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{app=\"webapp\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{app=\"webapp\"}[5m]))"
          }
        ],
        "type": "graph"
      },
      {
        "title": "P99 Latency",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{app=\"webapp\"}[5m]))"
          }
        ],
        "type": "graph"
      }
    ]
  }
}
```

---

## 4. æ—¥èªŒç®¡ç†

### 4.1 ELK Stack

```mermaid
graph LR
    A[Application] -->|æ—¥èªŒ| F[Fluentd/Filebeat]
    F --> E[Elasticsearch]
    E --> K[Kibana]
    
    B[å®¹å™¨æ—¥èªŒ] --> F
    C[ç³»çµ±æ—¥èªŒ] --> F
    
    style E fill:#9cf
    style K fill:#fc9
```

**å®‰è£ ELKï¼š**
```bash
helm repo add elastic https://helm.elastic.co
helm repo update

helm install elasticsearch elastic/elasticsearch \
  --namespace logging \
  --create-namespace \
  --set replicas=3 \
  --set volumeClaimTemplate.resources.requests.storage=100Gi

helm install kibana elastic/kibana \
  --namespace logging

helm install filebeat elastic/filebeat \
  --namespace logging
```

**Fluentd DaemonSetï¼š**
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      serviceAccountName: fluentd
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch-master"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 256Mi
      
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

### 4.2 Lokiï¼ˆè¼•é‡ç´šæ—¥èªŒæ–¹æ¡ˆï¼‰

```bash
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

helm install loki grafana/loki-stack \
  --namespace logging \
  --create-namespace \
  --set grafana.enabled=true \
  --set prometheus.enabled=false \
  --set loki.persistence.enabled=true \
  --set loki.persistence.size=50Gi
```

**æ‡‰ç”¨æ—¥èªŒæ ¼å¼åŒ–ï¼š**
```python
import logging
import json
import sys

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            'timestamp': self.formatTime(record),
            'level': record.levelname,
            'message': record.getMessage(),
            'logger': record.name,
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }
        
        if hasattr(record, 'user_id'):
            log_data['user_id'] = record.user_id
        
        if hasattr(record, 'request_id'):
            log_data['request_id'] = record.request_id
        
        if record.exc_info:
            log_data['exception'] = self.formatException(record.exc_info)
        
        return json.dumps(log_data)

handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(JSONFormatter())

logger = logging.getLogger()
logger.addHandler(handler)
logger.setLevel(logging.INFO)

logger.info('Application started', extra={'user_id': 123, 'request_id': 'abc'})
```

---

## 5. åˆ†ä½ˆå¼è¿½è¹¤

### 5.1 Jaeger æ¶æ§‹

```mermaid
graph TB
    subgraph "æ‡‰ç”¨å±¤"
        A1[Service A]
        A2[Service B]
        A3[Service C]
    end
    
    subgraph "Jaeger"
        Agent[Jaeger Agent]
        Collector[Jaeger Collector]
        Query[Jaeger Query]
        UI[Jaeger UI]
        
        Storage[(Storage<br/>Cassandra/ES)]
    end
    
    A1 -->|UDP| Agent
    A2 -->|UDP| Agent
    A3 -->|UDP| Agent
    
    Agent -->|gRPC| Collector
    Collector --> Storage
    Query --> Storage
    UI --> Query
    
    style Collector fill:#9cf
    style UI fill:#fc9
```

**å®‰è£ Jaegerï¼š**
```bash
kubectl create namespace observability

kubectl apply -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.50.0/jaeger-operator.yaml -n observability

kubectl apply -f - <<EOF
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger
  namespace: observability
spec:
  strategy: production
  storage:
    type: elasticsearch
    options:
      es:
        server-urls: http://elasticsearch:9200
EOF
```

### 5.2 æ‡‰ç”¨é›†æˆ Jaeger

**Python Flask + OpenTelemetryï¼š**
```python
from flask import Flask, request
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor

app = Flask(__name__)

trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

jaeger_exporter = JaegerExporter(
    agent_host_name="jaeger-agent",
    agent_port=6831,
)

trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(jaeger_exporter)
)

FlaskInstrumentor().instrument_app(app)
RequestsInstrumentor().instrument()

@app.route('/api/users')
def get_users():
    with tracer.start_as_current_span("get_users") as span:
        span.set_attribute("user.count", 100)
        
        users = fetch_users_from_db()
        
        enrich_users(users)
        
        return users

def fetch_users_from_db():
    with tracer.start_as_current_span("database_query") as span:
        span.set_attribute("db.system", "postgresql")
        span.set_attribute("db.statement", "SELECT * FROM users")
        
        return query_database("SELECT * FROM users")

def enrich_users(users):
    with tracer.start_as_current_span("enrich_users") as span:
        import requests
        
        response = requests.get("http://profile-service/profiles")
        
        span.set_attribute("http.status_code", response.status_code)
```

**å‚³éè¿½è¹¤ Headersï¼š**
```python
import requests
from opentelemetry.propagate import inject

def call_downstream_service():
    headers = {}
    inject(headers)
    
    response = requests.get(
        "http://downstream-service/api/data",
        headers=headers
    )
    
    return response.json()
```

---

## 6. SLO/SLI/SLA

### 6.1 æ¦‚å¿µå®šç¾©

```mermaid
graph TB
    SLA[SLA<br/>Service Level Agreement<br/>æœå‹™ç­‰ç´šå”è­°]
    SLO[SLO<br/>Service Level Objective<br/>æœå‹™ç­‰ç´šç›®æ¨™]
    SLI[SLI<br/>Service Level Indicator<br/>æœå‹™ç­‰ç´šæŒ‡æ¨™]
    
    SLA -->|åŒ…å«| SLO
    SLO -->|ç”±... æ¸¬é‡| SLI
    
    SLI --> M1[å¯ç”¨æ€§: 99.9%]
    SLI --> M2[å»¶é²: P99 < 200ms]
    SLI --> M3[éŒ¯èª¤ç‡: < 0.1%]
    
    style SLA fill:#f96
    style SLO fill:#fc9
    style SLI fill:#9cf
```

### 6.2 å®šç¾© SLO

**å¯ç”¨æ€§ SLOï¼š**
```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: slo-availability
spec:
  groups:
  - name: slo
    interval: 30s
    rules:
    
    - record: slo:availability:ratio_rate5m
      expr: |
        (
          sum(rate(http_requests_total{status!~"5.."}[5m]))
          /
          sum(rate(http_requests_total[5m]))
        )
    
    - alert: SLOBurnRateCritical
      expr: |
        (
          slo:availability:ratio_rate5m < 0.999
          and
          slo:availability:ratio_rate1h < 0.999
        )
      labels:
        severity: critical
      annotations:
        summary: "SLO burn rate is critical"
        description: "Availability is {{ $value | humanizePercentage }}, below 99.9% target"
```

**å»¶é² SLOï¼š**
```yaml
- record: slo:latency:p99_5m
  expr: |
    histogram_quantile(0.99,
      rate(http_request_duration_seconds_bucket[5m])
    )

- alert: SLOLatencyViolation
  expr: slo:latency:p99_5m > 0.2
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "P99 latency exceeds SLO"
    description: "P99 latency is {{ $value }}s, target is 200ms"
```

### 6.3 éŒ¯èª¤é ç®—

```promql
# 30 å¤©éŒ¯èª¤é ç®—
(1 - 0.999) * 30 * 24 * 60 * 60 = 2592 ç§’ = 43.2 åˆ†é˜

# ç•¶å‰éŒ¯èª¤é ç®—æ¶ˆè€—
sum(increase(http_requests_total{status=~"5.."}[30d]))
/
sum(increase(http_requests_total[30d]))
```

**éŒ¯èª¤é ç®—å‘Šè­¦ï¼š**
```yaml
- alert: ErrorBudgetExhausted
  expr: |
    (
      1 - (
        sum(increase(http_requests_total{status!~"5.."}[30d]))
        /
        sum(increase(http_requests_total[30d]))
      )
    ) > 0.001
  labels:
    severity: critical
  annotations:
    summary: "Error budget exhausted"
    description: "30-day error budget is {{ $value | humanizePercentage }} consumed"
```

---

## 7. å®Œæ•´å¯è§€æ¸¬æ€§é…ç½®

**æ‡‰ç”¨ Deploymentï¼š**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-observable
  labels:
    app: webapp
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: webapp
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    
    spec:
      containers:
      - name: app
        image: myapp:v1.0
        
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        
        env:
        - name: JAEGER_AGENT_HOST
          value: "jaeger-agent.observability"
        - name: JAEGER_AGENT_PORT
          value: "6831"
        - name: LOG_LEVEL
          value: "info"
        - name: LOG_FORMAT
          value: "json"
        
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
```

---

## 8. å°çµ

æœ¬ç« ä»‹ç´¹äº† Kubernetes å¯è§€æ¸¬æ€§çš„å®Œæ•´é«”ç³»ï¼š

**Metricsï¼ˆæŒ‡æ¨™ï¼‰ï¼š**
- âœ… Prometheus - æ™‚åºæ•¸æ“šåº«èˆ‡å‘Šè­¦
- âœ… Grafana - å¯è¦–åŒ–å„€è¡¨æ¿
- âœ… PromQL - å¼·å¤§çš„æŸ¥è©¢èªè¨€
- âœ… ServiceMonitor - è‡ªå‹•æœå‹™ç™¼ç¾

**Logsï¼ˆæ—¥èªŒï¼‰ï¼š**
- âœ… ELK Stack - å®Œæ•´æ—¥èªŒæ–¹æ¡ˆ
- âœ… Loki - è¼•é‡ç´šæ›¿ä»£æ–¹æ¡ˆ
- âœ… Fluentd/Filebeat - æ—¥èªŒæ”¶é›†
- âœ… JSON æ ¼å¼åŒ– - çµæ§‹åŒ–æ—¥èªŒ

**Tracesï¼ˆè¿½è¹¤ï¼‰ï¼š**
- âœ… Jaeger - åˆ†ä½ˆå¼è¿½è¹¤ç³»çµ±
- âœ… OpenTelemetry - æ¨™æº–åŒ–è¿½è¹¤
- âœ… Span å‚³é - è·¨æœå‹™è¿½è¹¤

**SLO/SLIï¼š**
- âœ… å¯ç”¨æ€§ç›®æ¨™ - 99.9% uptime
- âœ… å»¶é²ç›®æ¨™ - P99 < 200ms
- âœ… éŒ¯èª¤é ç®— - é‡åŒ–å¯é æ€§
- âœ… å‘Šè­¦ç­–ç•¥ - åŸºæ–¼ SLO çš„å‘Šè­¦

ä¸‹ä¸€ç« å°‡å­¸ç¿’ GitOps å¯¦æ–½ï¼Œä½¿ç”¨ ArgoCD å¯¦ç¾è‡ªå‹•åŒ–éƒ¨ç½²èˆ‡å¤šç’°å¢ƒç®¡ç†ã€‚
