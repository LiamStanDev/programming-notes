# 13 - ç¶“å…¸æ¡ˆä¾‹åˆ†æ (Classic Case Studies)

## ğŸ¯ å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬ç« å¾Œ,ä½ å°‡èƒ½å¤ :
- åˆ†æçœŸå¯¦ç³»çµ±çš„è¨­è¨ˆæ±ºç­–
- ç†è§£ä¸åŒè¦æ¨¡ç³»çµ±çš„æ¼”é€²è·¯å¾‘
- æŒæ¡å¸¸è¦‹ç³»çµ±çš„è¨­è¨ˆæ¨¡å¼
- æ‡‰ç”¨æ‰€å­¸çŸ¥è­˜åˆ°å¯¦éš›ç³»çµ±è¨­è¨ˆ

---

## ğŸ’¡ æ¡ˆä¾‹å­¸ç¿’æ–¹æ³•

### ç³»çµ±è¨­è¨ˆåˆ†ææ¡†æ¶

```mermaid
graph TD
    A["ç³»çµ±åˆ†æ"] --> B["éœ€æ±‚åˆ†æ"]
    A --> C["æ¶æ§‹è¨­è¨ˆ"]
    A --> D["æŠ€è¡“é¸å‹"]
    A --> E["æ¬Šè¡¡åˆ†æ"]
    A --> F["æ¼”é€²è·¯å¾‘"]
    
    B --> B1["åŠŸèƒ½éœ€æ±‚"]
    B --> B2["éåŠŸèƒ½éœ€æ±‚<br/>(QPS, å»¶é²ç­‰)"]
    
    C --> C1["é«˜å±¤æ¶æ§‹"]
    C --> C2["è³‡æ–™æµ"]
    
    D --> D1["å„²å­˜é¸æ“‡"]
    D --> D2["è¨ˆç®—é¸æ“‡"]
    
    E --> E1["CAP é¸æ“‡"]
    E --> E2["æˆæœ¬æ•ˆç›Š"]
    
    F --> F1["åˆæœŸ MVP"]
    F --> F2["è¦æ¨¡åŒ–"]
    F --> F3["å„ªåŒ–è¿­ä»£"]
```

---

## ğŸ”— æ¡ˆä¾‹ 1: URL çŸ­ç¶²å€æœå‹™ (URL Shortener)

### éœ€æ±‚åˆ†æ

**åŠŸèƒ½éœ€æ±‚**:
- ç”ŸæˆçŸ­ç¶²å€: `https://example.com/abc123`
- é‡å®šå‘: è¨ªå•çŸ­ç¶²å€è·³è½‰åˆ°åŸç¶²å€
- è‡ªè¨‚çŸ­ç¶²å€ (å¯é¸)
- éæœŸæ™‚é–“ (å¯é¸)

**éåŠŸèƒ½éœ€æ±‚**:
- QPS: 10,000 (è®€) / 100 (å¯«)
- å»¶é²: P99 < 100ms
- å¯ç”¨æ€§: 99.9%
- çŸ­ç¶²å€é•·åº¦: 6-8 å­—å…ƒ

### å®¹é‡ä¼°ç®—

```python
class CapacityEstimation:
    """å®¹é‡ä¼°ç®—"""
    
    def __init__(self):
        self.write_qps = 100  # æ¯ç§’å¯«å…¥
        self.read_qps = 10000  # æ¯ç§’è®€å–
        self.read_write_ratio = 100  # è®€å¯«æ¯”
    
    def estimate_storage(self, years=5):
        """ä¼°ç®—å­˜å„²éœ€æ±‚"""
        
        # æ¯å¹´æ–°å¢çŸ­ç¶²å€æ•¸é‡
        urls_per_year = self.write_qps * 86400 * 365
        print(f"æ¯å¹´æ–°å¢ URL: {urls_per_year:,}")
        # è¼¸å‡º: 3,153,600,000 (~30å„„)
        
        # 5 å¹´ç¸½é‡
        total_urls = urls_per_year * years
        print(f"5 å¹´ç¸½ URL: {total_urls:,}")
        # è¼¸å‡º: 15,768,000,000 (~150å„„)
        
        # å­˜å„²å¤§å° (å‡è¨­æ¯æ¢è¨˜éŒ„ 500 bytes)
        storage_bytes = total_urls * 500
        storage_gb = storage_bytes / (1024 ** 3)
        print(f"å­˜å„²éœ€æ±‚: {storage_gb:,.0f} GB")
        # è¼¸å‡º: 7,350 GB (~7.5 TB)
        
        return storage_gb
    
    def estimate_bandwidth(self):
        """ä¼°ç®—é »å¯¬éœ€æ±‚"""
        
        # å¯«å…¥é »å¯¬
        write_bandwidth = self.write_qps * 500  # bytes/s
        print(f"å¯«å…¥é »å¯¬: {write_bandwidth / 1024:.2f} KB/s")
        
        # è®€å–é »å¯¬ (åŒ…å«é‡å®šå‘)
        read_bandwidth = self.read_qps * 500  # bytes/s
        print(f"è®€å–é »å¯¬: {read_bandwidth / 1024:.2f} KB/s")
        
        return write_bandwidth, read_bandwidth

estimator = CapacityEstimation()
estimator.estimate_storage()
estimator.estimate_bandwidth()
```

### æ ¸å¿ƒè¨­è¨ˆ: çŸ­ç¶²å€ç”Ÿæˆ

**æ–¹æ³• 1: é›œæ¹Š + Base62 ç·¨ç¢¼**

```python
import hashlib
import base64

class URLShortener:
    """çŸ­ç¶²å€ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.base62_chars = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
    
    def shorten_hash(self, long_url):
        """ä½¿ç”¨é›œæ¹Šç”ŸæˆçŸ­ç¶²å€"""
        
        # MD5 é›œæ¹Š
        hash_value = hashlib.md5(long_url.encode()).hexdigest()
        
        # å–å‰ 8 å€‹å­—å…ƒ
        short_hash = hash_value[:8]
        
        # è½‰æ›ç‚º Base62
        num = int(short_hash, 16)
        short_url = self._base62_encode(num)[:6]
        
        return short_url
    
    def _base62_encode(self, num):
        """Base62 ç·¨ç¢¼"""
        if num == 0:
            return self.base62_chars[0]
        
        result = []
        while num:
            num, rem = divmod(num, 62)
            result.append(self.base62_chars[rem])
        
        return ''.join(reversed(result))
    
    def _base62_decode(self, string):
        """Base62 è§£ç¢¼"""
        num = 0
        for char in string:
            num = num * 62 + self.base62_chars.index(char)
        return num

# æ¸¬è©¦
shortener = URLShortener()
short_url = shortener.shorten_hash("https://www.example.com/very/long/url")
print(f"çŸ­ç¶²å€: {short_url}")  # è¼¸å‡º: abc123

# å•é¡Œ: å¯èƒ½ç”¢ç”Ÿè¡çª!
```

**æ–¹æ³• 2: è‡ªå¢ ID + Base62**

```python
class URLShortenerWithID:
    """ä½¿ç”¨è‡ªå¢ ID ç”ŸæˆçŸ­ç¶²å€ (ç„¡è¡çª)"""
    
    def __init__(self, db):
        self.db = db
        self.base62_chars = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
    
    def shorten(self, long_url):
        """ç”ŸæˆçŸ­ç¶²å€"""
        
        # 1. ç²å–è‡ªå¢ ID
        url_id = self.db.get_next_id()
        
        # 2. Base62 ç·¨ç¢¼
        short_code = self._base62_encode(url_id)
        
        # 3. å­˜å„²æ˜ å°„
        self.db.save({
            'id': url_id,
            'short_code': short_code,
            'long_url': long_url,
            'created_at': datetime.now()
        })
        
        return short_code
    
    def expand(self, short_code):
        """å±•é–‹çŸ­ç¶²å€"""
        
        # è§£ç¢¼ ID
        url_id = self._base62_decode(short_code)
        
        # æŸ¥è©¢é•·ç¶²å€
        record = self.db.get_by_id(url_id)
        
        return record['long_url'] if record else None
    
    def _base62_encode(self, num):
        """Base62 ç·¨ç¢¼"""
        if num == 0:
            return self.base62_chars[0]
        
        result = []
        while num:
            num, rem = divmod(num, 62)
            result.append(self.base62_chars[rem])
        
        return ''.join(reversed(result))
    
    def _base62_decode(self, string):
        """Base62 è§£ç¢¼"""
        num = 0
        for char in string:
            num = num * 62 + self.base62_chars.index(char)
        return num

# 6 ä½ Base62: 62^6 = 568å„„å€‹ URL (è¶³å¤ !)
```

### æ¶æ§‹è¨­è¨ˆ

```mermaid
graph TD
    A[Client] --> B[Load Balancer]
    B --> C1[App Server 1]
    B --> C2[App Server 2]
    B --> C3[App Server N]
    
    C1 --> D[Redis Cache]
    C2 --> D
    C3 --> D
    
    D --> E[(MySQL Primary)]
    E --> F[(MySQL Replica 1)]
    E --> G[(MySQL Replica 2)]
    
    C1 --> H[ID Generator<br/>Snowflake]
    
    I[Analytics] -.-> E
```

**è³‡æ–™åº«è¨­è¨ˆ**:

```sql
CREATE TABLE urls (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    short_code VARCHAR(10) UNIQUE NOT NULL,
    long_url TEXT NOT NULL,
    user_id BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP,
    click_count INT DEFAULT 0,
    INDEX idx_short_code (short_code),
    INDEX idx_user_id (user_id)
);

CREATE TABLE clicks (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    short_code VARCHAR(10),
    ip_address VARCHAR(45),
    user_agent TEXT,
    referer TEXT,
    clicked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_short_code (short_code),
    INDEX idx_clicked_at (clicked_at)
);
```

### å®Œæ•´å¯¦ä½œ

```python
from flask import Flask, request, redirect
import redis
import mysql.connector

app = Flask(__name__)

class URLShortenerService:
    def __init__(self):
        self.redis = redis.Redis(host='localhost', decode_responses=True)
        self.mysql = mysql.connector.connect(
            host='localhost',
            user='root',
            password='password',
            database='url_shortener'
        )
        self.cache_ttl = 3600  # 1 å°æ™‚
    
    def create_short_url(self, long_url, custom_code=None):
        """å‰µå»ºçŸ­ç¶²å€"""
        
        cursor = self.mysql.cursor()
        
        if custom_code:
            # æª¢æŸ¥è‡ªè¨‚ä»£ç¢¼æ˜¯å¦å¯ç”¨
            cursor.execute(
                "SELECT id FROM urls WHERE short_code = %s",
                (custom_code,)
            )
            if cursor.fetchone():
                raise ValueError("è‡ªè¨‚ä»£ç¢¼å·²è¢«ä½¿ç”¨")
            
            short_code = custom_code
        else:
            # ç”ŸæˆçŸ­ä»£ç¢¼
            cursor.execute("SELECT MAX(id) FROM urls")
            max_id = cursor.fetchone()[0] or 0
            next_id = max_id + 1
            short_code = self._base62_encode(next_id)
        
        # å­˜å„²åˆ°è³‡æ–™åº«
        cursor.execute("""
            INSERT INTO urls (short_code, long_url) 
            VALUES (%s, %s)
        """, (short_code, long_url))
        
        self.mysql.commit()
        
        # é ç†±å¿«å–
        self.redis.setex(
            f"url:{short_code}",
            self.cache_ttl,
            long_url
        )
        
        return short_code
    
    def get_long_url(self, short_code):
        """ç²å–é•·ç¶²å€"""
        
        # 1. æŸ¥è©¢å¿«å–
        cache_key = f"url:{short_code}"
        long_url = self.redis.get(cache_key)
        
        if long_url:
            print("å¿«å–å‘½ä¸­")
            return long_url
        
        # 2. æŸ¥è©¢è³‡æ–™åº«
        cursor = self.mysql.cursor()
        cursor.execute(
            "SELECT long_url FROM urls WHERE short_code = %s",
            (short_code,)
        )
        
        result = cursor.fetchone()
        if not result:
            return None
        
        long_url = result[0]
        
        # 3. å¯«å…¥å¿«å–
        self.redis.setex(cache_key, self.cache_ttl, long_url)
        
        # 4. ç•°æ­¥æ›´æ–°é»æ“Šçµ±è¨ˆ
        self._increment_click_count(short_code)
        
        return long_url
    
    def _increment_click_count(self, short_code):
        """å¢åŠ é»æ“Šè¨ˆæ•¸ (ç•°æ­¥)"""
        cursor = self.mysql.cursor()
        cursor.execute(
            "UPDATE urls SET click_count = click_count + 1 WHERE short_code = %s",
            (short_code,)
        )
        self.mysql.commit()
    
    def _base62_encode(self, num):
        """Base62 ç·¨ç¢¼"""
        chars = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
        if num == 0:
            return chars[0]
        
        result = []
        while num:
            num, rem = divmod(num, 62)
            result.append(chars[rem])
        
        return ''.join(reversed(result))

# Flask è·¯ç”±
service = URLShortenerService()

@app.route('/shorten', methods=['POST'])
def shorten():
    """å‰µå»ºçŸ­ç¶²å€ API"""
    data = request.json
    long_url = data.get('url')
    custom_code = data.get('custom_code')
    
    try:
        short_code = service.create_short_url(long_url, custom_code)
        return {
            'short_url': f"https://short.url/{short_code}",
            'short_code': short_code
        }
    except Exception as e:
        return {'error': str(e)}, 400

@app.route('/<short_code>')
def redirect_url(short_code):
    """é‡å®šå‘çŸ­ç¶²å€"""
    long_url = service.get_long_url(short_code)
    
    if not long_url:
        return "URL not found", 404
    
    return redirect(long_url, code=301)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### å„ªåŒ–èˆ‡æ“´å±•

**1. å¿«å–ç­–ç•¥å„ªåŒ–**:

```python
class CacheWarmer:
    """å¿«å–é ç†±å™¨"""
    
    def warm_popular_urls(self):
        """é ç†±ç†±é–€ URL"""
        
        cursor = self.mysql.cursor()
        cursor.execute("""
            SELECT short_code, long_url 
            FROM urls 
            ORDER BY click_count DESC 
            LIMIT 10000
        """)
        
        for short_code, long_url in cursor:
            self.redis.setex(
                f"url:{short_code}",
                3600,
                long_url
            )
```

**2. åˆ†æ•£å¼ ID ç”Ÿæˆ (Snowflake)**:

```python
import time
import threading

class SnowflakeIDGenerator:
    """Twitter Snowflake ID ç”Ÿæˆå™¨"""
    
    def __init__(self, datacenter_id, worker_id):
        self.datacenter_id = datacenter_id  # 5 bits
        self.worker_id = worker_id          # 5 bits
        self.sequence = 0                    # 12 bits
        self.last_timestamp = -1
        self.lock = threading.Lock()
        
        # Epoch (è‡ªè¨‚èµ·å§‹æ™‚é–“)
        self.epoch = 1609459200000  # 2021-01-01 00:00:00
    
    def generate_id(self):
        """ç”Ÿæˆå”¯ä¸€ ID"""
        with self.lock:
            timestamp = int(time.time() * 1000)
            
            if timestamp < self.last_timestamp:
                raise Exception("æ™‚é˜å›æ’¥")
            
            if timestamp == self.last_timestamp:
                # åŒä¸€æ¯«ç§’å…§,åºè™Ÿéå¢
                self.sequence = (self.sequence + 1) & 0xFFF
                if self.sequence == 0:
                    # åºè™Ÿæº¢å‡º,ç­‰å¾…ä¸‹ä¸€æ¯«ç§’
                    while timestamp <= self.last_timestamp:
                        timestamp = int(time.time() * 1000)
            else:
                self.sequence = 0
            
            self.last_timestamp = timestamp
            
            # çµ„åˆ ID
            id = ((timestamp - self.epoch) << 22) | \
                 (self.datacenter_id << 17) | \
                 (self.worker_id << 12) | \
                 self.sequence
            
            return id

# ä½¿ç”¨ç¯„ä¾‹
id_gen = SnowflakeIDGenerator(datacenter_id=1, worker_id=1)
url_id = id_gen.generate_id()
print(f"ç”Ÿæˆ ID: {url_id}")
```

**3. åˆ†æçµ±è¨ˆ**:

```python
class URLAnalytics:
    """URL åˆ†ææœå‹™"""
    
    def get_statistics(self, short_code):
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        
        # æŸ¥è©¢é»æ“Šè¨˜éŒ„
        cursor = self.mysql.cursor()
        
        # ç¸½é»æ“Šæ•¸
        cursor.execute("""
            SELECT COUNT(*) FROM clicks 
            WHERE short_code = %s
        """, (short_code,))
        total_clicks = cursor.fetchone()[0]
        
        # æ¯æ—¥é»æ“Šè¶¨å‹¢
        cursor.execute("""
            SELECT DATE(clicked_at) as date, COUNT(*) as count
            FROM clicks 
            WHERE short_code = %s 
                AND clicked_at >= DATE_SUB(NOW(), INTERVAL 30 DAY)
            GROUP BY DATE(clicked_at)
            ORDER BY date
        """, (short_code,))
        daily_clicks = cursor.fetchall()
        
        # ä¾†æºåˆ†æ
        cursor.execute("""
            SELECT referer, COUNT(*) as count
            FROM clicks 
            WHERE short_code = %s 
            GROUP BY referer 
            ORDER BY count DESC 
            LIMIT 10
        """, (short_code,))
        top_referers = cursor.fetchall()
        
        return {
            'total_clicks': total_clicks,
            'daily_clicks': [{'date': str(date), 'count': count} for date, count in daily_clicks],
            'top_referers': [{'referer': ref, 'count': count} for ref, count in top_referers]
        }
```

---

## ğŸ“± æ¡ˆä¾‹ 2: Instagram ç…§ç‰‡å„²å­˜

### éœ€æ±‚åˆ†æ

**åŠŸèƒ½éœ€æ±‚**:
- ä¸Šå‚³ç…§ç‰‡
- ç€è¦½ç…§ç‰‡
- ç”Ÿæˆå¤šç¨®å°ºå¯¸ (ç¸®åœ–ã€ä¸­åœ–ã€åŸåœ–)

**éåŠŸèƒ½éœ€æ±‚**:
- ä½¿ç”¨è€…: 10å„„
- æ¯æ—¥ä¸Šå‚³: 1å„„å¼µç…§ç‰‡
- å¹³å‡ç…§ç‰‡å¤§å°: 3MB
- å­˜å„²: PB ç´šåˆ¥

### å®¹é‡ä¼°ç®—

```python
class InstagramStorageEstimation:
    """Instagram å­˜å„²ä¼°ç®—"""
    
    def __init__(self):
        self.daily_uploads = 100_000_000  # 1å„„å¼µ/å¤©
        self.avg_photo_size_mb = 3  # 3MB
        self.thumbnail_size_kb = 50  # 50KB
        self.medium_size_kb = 500  # 500KB
    
    def estimate_daily_storage(self):
        """æ¯æ—¥å­˜å„²å¢é•·"""
        
        # åŸåœ–
        original_gb = (self.daily_uploads * self.avg_photo_size_mb) / 1024
        
        # ç¸®åœ–
        thumbnail_gb = (self.daily_uploads * self.thumbnail_size_kb / 1024) / 1024
        
        # ä¸­åœ–
        medium_gb = (self.daily_uploads * self.medium_size_kb / 1024) / 1024
        
        total_gb = original_gb + thumbnail_gb + medium_gb
        
        print(f"æ¯æ—¥å­˜å„²å¢é•·: {total_gb:,.0f} GB")
        print(f"  - åŸåœ–: {original_gb:,.0f} GB")
        print(f"  - ä¸­åœ–: {medium_gb:,.0f} GB")
        print(f"  - ç¸®åœ–: {thumbnail_gb:,.0f} GB")
        
        # æ¯å¹´
        yearly_pb = (total_gb * 365) / 1024
        print(f"\næ¯å¹´å­˜å„²å¢é•·: {yearly_pb:.1f} PB")
        
        return total_gb

estimator = InstagramStorageEstimation()
estimator.estimate_daily_storage()
# è¼¸å‡º: ~293 TB/å¤©, ~104 PB/å¹´
```

### æ¶æ§‹è¨­è¨ˆ

```mermaid
graph TD
    A[User] --> B[CDN]
    B --> C[Load Balancer]
    C --> D[App Servers]
    
    D --> E[Metadata DB<br/>Cassandra]
    D --> F[Object Storage<br/>S3]
    
    D --> G[Image Processing<br/>Queue]
    G --> H[Workers]
    H --> F
    
    D --> I[Cache<br/>Redis]
```

**è³‡æ–™æ¨¡å‹**:

```python
# Cassandra Schema
"""
CREATE TABLE photos (
    photo_id UUID PRIMARY KEY,
    user_id UUID,
    caption TEXT,
    location TEXT,
    upload_time TIMESTAMP,
    original_url TEXT,
    medium_url TEXT,
    thumbnail_url TEXT,
    likes_count COUNTER,
    comments_count COUNTER
);

CREATE TABLE user_photos (
    user_id UUID,
    photo_id UUID,
    upload_time TIMESTAMP,
    PRIMARY KEY (user_id, upload_time, photo_id)
) WITH CLUSTERING ORDER BY (upload_time DESC);

CREATE TABLE photo_likes (
    photo_id UUID,
    user_id UUID,
    liked_at TIMESTAMP,
    PRIMARY KEY (photo_id, user_id)
);
"""
```

### æ ¸å¿ƒå¯¦ä½œ: ç…§ç‰‡ä¸Šå‚³

```python
import uuid
import boto3
from PIL import Image
import io

class PhotoUploadService:
    """ç…§ç‰‡ä¸Šå‚³æœå‹™"""
    
    def __init__(self):
        self.s3 = boto3.client('s3')
        self.bucket = 'instagram-photos'
        self.cassandra = CassandraClient()
        self.queue = MessageQueue()
    
    def upload_photo(self, user_id, image_file, caption=None):
        """ä¸Šå‚³ç…§ç‰‡"""
        
        # 1. ç”Ÿæˆå”¯ä¸€ ID
        photo_id = uuid.uuid4()
        
        # 2. ä¸Šå‚³åŸåœ–åˆ° S3
        original_key = f"original/{photo_id}.jpg"
        self.s3.upload_fileobj(
            image_file,
            self.bucket,
            original_key,
            ExtraArgs={'ContentType': 'image/jpeg'}
        )
        
        original_url = f"https://cdn.instagram.com/{original_key}"
        
        # 3. ç™¼é€åˆ°è™•ç†ä½‡åˆ— (ç•°æ­¥ç”Ÿæˆç¸®åœ–)
        self.queue.publish({
            'photo_id': str(photo_id),
            'original_key': original_key,
            'user_id': str(user_id)
        })
        
        # 4. å­˜å„²å…ƒè³‡æ–™
        self.cassandra.execute("""
            INSERT INTO photos (photo_id, user_id, caption, upload_time, original_url)
            VALUES (?, ?, ?, ?, ?)
        """, (photo_id, user_id, caption, datetime.now(), original_url))
        
        self.cassandra.execute("""
            INSERT INTO user_photos (user_id, photo_id, upload_time)
            VALUES (?, ?, ?)
        """, (user_id, photo_id, datetime.now()))
        
        return {
            'photo_id': str(photo_id),
            'url': original_url
        }
    
    def process_photo(self, message):
        """è™•ç†ç…§ç‰‡ (Worker)"""
        
        photo_id = message['photo_id']
        original_key = message['original_key']
        
        # 1. å¾ S3 ä¸‹è¼‰åŸåœ–
        response = self.s3.get_object(Bucket=self.bucket, Key=original_key)
        image_data = response['Body'].read()
        image = Image.open(io.BytesIO(image_data))
        
        # 2. ç”Ÿæˆç¸®åœ– (150x150)
        thumbnail = image.copy()
        thumbnail.thumbnail((150, 150))
        thumbnail_key = f"thumbnail/{photo_id}.jpg"
        self._upload_processed_image(thumbnail, thumbnail_key)
        
        # 3. ç”Ÿæˆä¸­åœ– (640x640)
        medium = image.copy()
        medium.thumbnail((640, 640))
        medium_key = f"medium/{photo_id}.jpg"
        self._upload_processed_image(medium, medium_key)
        
        # 4. æ›´æ–°å…ƒè³‡æ–™
        thumbnail_url = f"https://cdn.instagram.com/{thumbnail_key}"
        medium_url = f"https://cdn.instagram.com/{medium_key}"
        
        self.cassandra.execute("""
            UPDATE photos 
            SET thumbnail_url = ?, medium_url = ?
            WHERE photo_id = ?
        """, (thumbnail_url, medium_url, uuid.UUID(photo_id)))
    
    def _upload_processed_image(self, image, key):
        """ä¸Šå‚³è™•ç†å¾Œçš„åœ–ç‰‡"""
        buffer = io.BytesIO()
        image.save(buffer, format='JPEG', quality=85)
        buffer.seek(0)
        
        self.s3.upload_fileobj(
            buffer,
            self.bucket,
            key,
            ExtraArgs={'ContentType': 'image/jpeg'}
        )
    
    def get_user_photos(self, user_id, limit=20):
        """ç²å–ç”¨æˆ¶ç…§ç‰‡"""
        
        rows = self.cassandra.execute("""
            SELECT photo_id, upload_time 
            FROM user_photos 
            WHERE user_id = ? 
            LIMIT ?
        """, (user_id, limit))
        
        photo_ids = [row.photo_id for row in rows]
        
        # æ‰¹æ¬¡æŸ¥è©¢ç…§ç‰‡è³‡è¨Š
        photos = self.cassandra.execute("""
            SELECT * FROM photos 
            WHERE photo_id IN ?
        """, (photo_ids,))
        
        return [self._photo_to_dict(photo) for photo in photos]
    
    def _photo_to_dict(self, photo):
        """è½‰æ›ç‚ºå­—å…¸"""
        return {
            'photo_id': str(photo.photo_id),
            'user_id': str(photo.user_id),
            'caption': photo.caption,
            'thumbnail_url': photo.thumbnail_url,
            'medium_url': photo.medium_url,
            'original_url': photo.original_url,
            'likes_count': photo.likes_count,
            'upload_time': photo.upload_time.isoformat()
        }
```

### å„ªåŒ–: CDN èˆ‡å¿«å–

```python
class PhotoCDNOptimization:
    """CDN èˆ‡å¿«å–å„ªåŒ–"""
    
    def __init__(self):
        self.cdn_domain = "cdn.instagram.com"
        self.cache = RedisClient()
    
    def get_photo_url(self, photo_id, size='medium'):
        """ç²å–ç…§ç‰‡ URL (å¸¶ CDN)"""
        
        # 1. æª¢æŸ¥å¿«å–
        cache_key = f"photo:{photo_id}:{size}"
        cached_url = self.cache.get(cache_key)
        
        if cached_url:
            return cached_url
        
        # 2. æŸ¥è©¢è³‡æ–™åº«
        photo = self.cassandra.execute("""
            SELECT thumbnail_url, medium_url, original_url 
            FROM photos 
            WHERE photo_id = ?
        """, (uuid.UUID(photo_id),)).one()
        
        # 3. æ ¹æ“šå°ºå¯¸è¿”å› URL
        if size == 'thumbnail':
            url = photo.thumbnail_url
        elif size == 'medium':
            url = photo.medium_url
        else:
            url = photo.original_url
        
        # 4. å¿«å– URL (1 å°æ™‚)
        self.cache.setex(cache_key, 3600, url)
        
        return url
    
    def prefetch_feed_photos(self, user_id):
        """é å– Feed ç…§ç‰‡åˆ°å¿«å–"""
        
        # ç²å–ç”¨æˆ¶ Feed
        feed_photo_ids = self.get_user_feed(user_id, limit=50)
        
        # æ‰¹æ¬¡é å–åˆ°å¿«å–
        for photo_id in feed_photo_ids:
            for size in ['thumbnail', 'medium']:
                self.get_photo_url(photo_id, size)
```

---

## ğŸ“š ç¸½çµ

### è¨­è¨ˆæ¨¡å¼ç¸½çµ

```mermaid
mindmap
  root((ç¶“å…¸è¨­è¨ˆ<br/>æ¨¡å¼))
    è³‡æ–™å­˜å„²
      ç†±è³‡æ–™: å¿«å–
      æº«è³‡æ–™: è³‡æ–™åº«
      å†·è³‡æ–™: ç‰©ä»¶å­˜å„²
    ID ç”Ÿæˆ
      è‡ªå¢ ID
      UUID
      Snowflake
    è™•ç†æ¨¡å¼
      åŒæ­¥: é—œéµè·¯å¾‘
      ç•°æ­¥: éé—œéµè·¯å¾‘
    æ“´å±•ç­–ç•¥
      è®€å¤šå¯«å°‘: å¿«å–
      å¯«å¤šè®€å°‘: ä½‡åˆ—
```

### é—œéµè¦é»

1. **å®¹é‡ä¼°ç®—**: å…ˆä¼°ç®—,å¾Œè¨­è¨ˆ
2. **æ¬Šè¡¡åˆ†æ**: æ˜ç¢ºå–æ¨
3. **æ¼”é€²æ€ç¶­**: å¾ç°¡å–®é–‹å§‹,é€æ­¥å„ªåŒ–
4. **çœŸå¯¦æ•¸æ“š**: åŸºæ–¼å¯¦éš›æµé‡è¨­è¨ˆ

---

## ğŸ”— åƒè€ƒè³‡æ–™

1. **ç³»çµ±è¨­è¨ˆ**:
   - [System Design Primer](https://github.com/donnemartin/system-design-primer)
   - [Grokking the System Design Interview](https://www.educative.io/courses/grokking-the-system-design-interview)

2. **çœŸå¯¦æ¡ˆä¾‹**:
   - [Instagram Engineering Blog](https://instagram-engineering.com/)
   - [Twitter Engineering Blog](https://blog.twitter.com/engineering)
   - [Uber Engineering Blog](https://eng.uber.com/)

3. **æ›¸ç±**:
   - Alex Xu, *System Design Interview*
   - Martin Kleppmann, *Designing Data-Intensive Applications*
