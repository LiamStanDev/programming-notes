# 10 - ä¸²æµè™•ç† (Stream Processing)

## ğŸ¯ å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬ç« å¾Œ,ä½ å°‡èƒ½å¤ :
- ç†è§£ä¸²æµè™•ç†èˆ‡æ‰¹æ¬¡è™•ç†çš„å·®ç•°
- æŒæ¡äº‹ä»¶é©…å‹•æ¶æ§‹çš„è¨­è¨ˆ
- ç†è§£è¦–çª— (Window) æ“ä½œèˆ‡æ™‚é–“èªç¾©
- å¯¦ä½œå®¹éŒ¯çš„ä¸²æµè™•ç†æ‡‰ç”¨

---

## ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ

### ä»€éº¼æ˜¯ä¸²æµè™•ç†?

**ä¸²æµè™•ç† (Stream Processing)**: æŒçºŒè™•ç†ç„¡ç•Œçš„äº‹ä»¶æµ,ä½å»¶é²è¼¸å‡ºçµæœã€‚

```mermaid
graph LR
    A["äº‹ä»¶æº"] -->|æŒçºŒæµå…¥| B["ä¸²æµè™•ç†"]
    B -->|å³æ™‚è¼¸å‡º| C["çµæœ"]
    
    A1["é»æ“Šäº‹ä»¶"] --> A
    A2["äº¤æ˜“äº‹ä»¶"] --> A
    A3["æ„Ÿæ¸¬å™¨è³‡æ–™"] --> A
```

**ç‰¹æ€§**:
- **ç„¡ç•Œè³‡æ–™**: æ°¸ä¸çµæŸçš„äº‹ä»¶æµ
- **ä½å»¶é²**: æ¯«ç§’åˆ°ç§’ç´š
- **å¢é‡è¨ˆç®—**: é€å€‹è™•ç†äº‹ä»¶

---

## ğŸ“¨ è¨Šæ¯å‚³éèªç¾©

### ä¸‰ç¨®ä¿è­‰ç´šåˆ¥

```mermaid
graph TD
    A["è¨Šæ¯å‚³éèªç¾©"] --> B["At-Most-Once<br/>æœ€å¤šä¸€æ¬¡"]
    A --> C["At-Least-Once<br/>è‡³å°‘ä¸€æ¬¡"]
    A --> D["Exactly-Once<br/>æ°å¥½ä¸€æ¬¡"]
    
    B --> B1["å¯èƒ½ä¸Ÿå¤±<br/>ä¸é‡è¤‡"]
    C --> C1["ä¸ä¸Ÿå¤±<br/>å¯èƒ½é‡è¤‡"]
    D --> D1["ä¸ä¸Ÿå¤±<br/>ä¸é‡è¤‡"]
```

### ğŸ’» å¯¦ä½œç¯„ä¾‹

**At-Least-Once with Idempotency**:

```python
import hashlib

class IdempotentProcessor:
    def __init__(self):
        self.processed_ids = set()
    
    def process_event(self, event):
        # ç”Ÿæˆäº‹ä»¶æŒ‡ç´‹
        event_id = hashlib.md5(
            str(event).encode()
        ).hexdigest()
        
        # æª¢æŸ¥æ˜¯å¦å·²è™•ç†
        if event_id in self.processed_ids:
            print(f"è·³éé‡è¤‡äº‹ä»¶: {event_id}")
            return
        
        # è™•ç†äº‹ä»¶
        self.do_processing(event)
        
        # æ¨™è¨˜å·²è™•ç†
        self.processed_ids.add(event_id)
    
    def do_processing(self, event):
        print(f"è™•ç†äº‹ä»¶: {event}")
```

---

## â° æ™‚é–“èªç¾©

### Event Time vs Processing Time

```mermaid
graph LR
    A["äº‹ä»¶åœ¨æºé ­<br/>ç”¢ç”Ÿæ™‚é–“<br/>(Event Time)"] -->|ç¶²è·¯å»¶é²| B["äº‹ä»¶åˆ°é”<br/>è™•ç†ç³»çµ±æ™‚é–“<br/>(Processing Time)"]
    
    style A fill:#c8e6c9
    style B fill:#fff9c4
```

**å•é¡Œ: äº‚åºèˆ‡å»¶é²**

```python
# äº‹ä»¶æµ (å¯¦éš›åˆ°é”é †åº)
events = [
    {"user": "Alice", "event_time": 100, "processing_time": 105},
    {"user": "Bob",   "event_time": 102, "processing_time": 103},  # äº‚åº!
    {"user": "Alice", "event_time": 101, "processing_time": 108}   # å»¶é²
]

# å¦‚æœæŒ‰ processing_time çµ±è¨ˆ,æœƒæ¼è¨ˆå»¶é²äº‹ä»¶
# å¿…é ˆä½¿ç”¨ event_time + Watermark
```

### Watermark æ©Ÿåˆ¶

**Watermark**: è¡¨ç¤º"æ—©æ–¼æ­¤æ™‚é–“æˆ³çš„äº‹ä»¶å·²å…¨éƒ¨åˆ°é”"ã€‚

```mermaid
sequenceDiagram
    participant Source
    participant Processor
    
    Note over Source: Event Time = 100
    Source->>Processor: Event (time=100)
    
    Note over Source: Event Time = 105
    Source->>Processor: Event (time=105)
    
    Note over Source: Watermark = 103
    Source->>Processor: Watermark(103)
    
    Note over Processor: è§¸ç™¼è¨ˆç®— [0-100]
```

**Apache Flink å¯¦ä½œ**:

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.common import WatermarkStrategy, Duration

env = StreamExecutionEnvironment.get_execution_environment()

# å®šç¾© Watermark ç­–ç•¥
watermark_strategy = (
    WatermarkStrategy
    .for_bounded_out_of_orderness(Duration.of_seconds(5))  # å®¹å¿ 5 ç§’äº‚åº
    .with_timestamp_assigner(lambda event, timestamp: event['event_time'])
)

stream = env.from_collection([...])
stream = stream.assign_timestamps_and_watermarks(watermark_strategy)
```

---

## ğŸªŸ è¦–çª—æ“ä½œ (Windowing)

### è¦–çª—é¡å‹

```mermaid
graph TD
    A["è¦–çª—é¡å‹"] --> B["Tumbling Window<br/>æ»¾å‹•è¦–çª—"]
    A --> C["Sliding Window<br/>æ»‘å‹•è¦–çª—"]
    A --> D["Session Window<br/>æœƒè©±è¦–çª—"]
    
    B --> B1["å›ºå®šå¤§å°<br/>ç„¡é‡ç–Š"]
    C --> C1["å›ºå®šå¤§å°<br/>æœ‰é‡ç–Š"]
    D --> D1["å‹•æ…‹å¤§å°<br/>åŸºæ–¼æ´»å‹•é–“éš”"]
```

### æ»¾å‹•è¦–çª— (Tumbling Window)

```python
# Kafka Streams ç¯„ä¾‹
from kafka import KafkaConsumer
from collections import defaultdict
import time

def tumbling_window_count(stream, window_size=60):
    """æ¯ 60 ç§’çµ±è¨ˆä¸€æ¬¡"""
    window_start = int(time.time() / window_size) * window_size
    counts = defaultdict(int)
    
    for event in stream:
        event_window = int(event['timestamp'] / window_size) * window_size
        
        if event_window > window_start:
            # è¼¸å‡ºç•¶å‰è¦–çª—çµæœ
            print(f"Window [{window_start}-{window_start + window_size}]: {dict(counts)}")
            
            # é–‹å§‹æ–°è¦–çª—
            window_start = event_window
            counts.clear()
        
        counts[event['user']] += 1

# Flink SQL ç¯„ä¾‹
"""
SELECT 
    user_id,
    COUNT(*) as event_count,
    TUMBLE_START(event_time, INTERVAL '1' MINUTE) as window_start
FROM events
GROUP BY user_id, TUMBLE(event_time, INTERVAL '1' MINUTE)
"""
```

### æ»‘å‹•è¦–çª— (Sliding Window)

```python
# éå» 5 åˆ†é˜å…§,æ¯åˆ†é˜æ›´æ–°ä¸€æ¬¡çš„çµ±è¨ˆ
"""
SELECT 
    user_id,
    COUNT(*) as event_count,
    HOP_START(event_time, INTERVAL '1' MINUTE, INTERVAL '5' MINUTE) as window_start
FROM events
GROUP BY user_id, HOP(event_time, INTERVAL '1' MINUTE, INTERVAL '5' MINUTE)
"""
```

### æœƒè©±è¦–çª— (Session Window)

```python
# ç”¨æˆ¶æ´»å‹•è¶…é 30 åˆ†é˜ç„¡å‹•ä½œå‰‡çµæŸæœƒè©±
"""
SELECT 
    user_id,
    COUNT(*) as event_count,
    SESSION_START(event_time, INTERVAL '30' MINUTE) as session_start,
    SESSION_END(event_time, INTERVAL '30' MINUTE) as session_end
FROM events
GROUP BY user_id, SESSION(event_time, INTERVAL '30' MINUTE)
"""
```

---

## ğŸ“Š ç‹€æ…‹ç®¡ç†

### æœ‰ç‹€æ…‹è™•ç†ç¯„ä¾‹

```python
class StatefulCounter:
    def __init__(self):
        self.state = {}  # {key: count}
    
    def process(self, key, value):
        """ç´¯åŠ è¨ˆæ•¸"""
        if key not in self.state:
            self.state[key] = 0
        
        self.state[key] += value
        return self.state[key]

# Apache Flink Managed State
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.state import ValueStateDescriptor

class CountFunction(KeyedProcessFunction):
    def __init__(self):
        self.count_state = None
    
    def open(self, runtime_context):
        # è¨»å†Šç‹€æ…‹
        descriptor = ValueStateDescriptor("count", Types.LONG())
        self.count_state = runtime_context.get_state(descriptor)
    
    def process_element(self, value, ctx):
        # è®€å–ç‹€æ…‹
        current_count = self.count_state.value()
        if current_count is None:
            current_count = 0
        
        # æ›´æ–°ç‹€æ…‹
        current_count += 1
        self.count_state.update(current_count)
        
        yield (value, current_count)
```

### Checkpoint æ©Ÿåˆ¶

**ç›®çš„**: å¯¦ç¾ Exactly-Once èªç¾©ã€‚

```mermaid
sequenceDiagram
    participant JM as JobManager
    participant Task1
    participant Task2
    participant State as State Backend
    
    JM->>Task1: è§¸ç™¼ Checkpoint(n)
    JM->>Task2: è§¸ç™¼ Checkpoint(n)
    
    Task1->>State: ä¿å­˜ç‹€æ…‹
    Task2->>State: ä¿å­˜ç‹€æ…‹
    
    Task1->>JM: ç¢ºèªå®Œæˆ
    Task2->>JM: ç¢ºèªå®Œæˆ
    
    Note over JM: Checkpoint(n) æˆåŠŸ
```

---

## ğŸ”„ è®Šæ›´è³‡æ–™æ•ç² (CDC)

### ä»€éº¼æ˜¯ CDC?

**CDC (Change Data Capture)**: æ•ç²è³‡æ–™åº«çš„è®Šæ›´äº‹ä»¶,ç™¼é€åˆ°ä¸²æµè™•ç†ç³»çµ±ã€‚

```mermaid
graph LR
    A["MySQL"] -->|Binlog| B["Debezium"]
    B --> C["Kafka"]
    C --> D["Flink/Spark<br/>ä¸²æµè™•ç†"]
    D --> E["ElasticSearch<br/>Redis<br/>ç­‰"]
```

### ğŸ’» Debezium ç¯„ä¾‹

```json
// MySQL è®Šæ›´äº‹ä»¶
{
  "before": {
    "id": 1,
    "name": "Alice",
    "balance": 100
  },
  "after": {
    "id": 1,
    "name": "Alice",
    "balance": 150
  },
  "op": "u",  // update
  "ts_ms": 1609459200000
}
```

**è™•ç† CDC äº‹ä»¶**:

```python
def process_cdc_event(event):
    op = event['op']
    
    if op == 'c':  # create
        insert_to_cache(event['after'])
    elif op == 'u':  # update
        update_cache(event['after'])
    elif op == 'd':  # delete
        delete_from_cache(event['before']['id'])
```

---

## ğŸ¢ çœŸå¯¦ä¸–ç•Œæ¡ˆä¾‹

### æ¡ˆä¾‹ 1: LinkedIn çš„ Kafka + Samza

**æ¶æ§‹**:

```mermaid
graph LR
    A["æ‡‰ç”¨æœå‹™"] --> B["Kafka<br/>(è¨Šæ¯ä½‡åˆ—)"]
    B --> C["Samza<br/>(ä¸²æµè™•ç†)"]
    C --> D["è¡ç”Ÿè³‡æ–™"]
    
    D --> D1["æœç´¢ç´¢å¼•"]
    D --> D2["æ¨è–¦ç³»çµ±"]
    D --> D3["ç›£æ§å‘Šè­¦"]
```

**ä½¿ç”¨å ´æ™¯**:
- å¯¦æ™‚æ¨è–¦
- æ–°è Feed ç”Ÿæˆ
- ç›£æ§å‘Šè­¦

---

### æ¡ˆä¾‹ 2: Uber çš„å¯¦æ™‚å®šåƒ¹

**éœ€æ±‚**:
- æ ¹æ“šä¾›éœ€å¯¦æ™‚èª¿æ•´åƒ¹æ ¼
- å»¶é² < 1 ç§’

**æŠ€è¡“æ£§**:
- Kafka: äº‹ä»¶æµ
- Flink: ä¸²æµè™•ç†
- Redis: ç‹€æ…‹å­˜å„²

**æµç¨‹**:

```python
# å½ä»£ç¢¼
def calculate_surge_pricing(region, time_window):
    # çµ±è¨ˆè©²å€åŸŸçš„ä¾›éœ€
    demand = count_ride_requests(region, time_window)
    supply = count_available_drivers(region, time_window)
    
    # è¨ˆç®—å€æ•¸
    if supply == 0:
        surge_multiplier = 2.0
    else:
        ratio = demand / supply
        surge_multiplier = min(1.0 + ratio * 0.5, 3.0)
    
    return surge_multiplier
```

---

### æ¡ˆä¾‹ 3: Netflix çš„å¯¦æ™‚ç•°å¸¸æª¢æ¸¬

**ç›£æ§æŒ‡æ¨™**:
- è¦–é »æ’­æ”¾å¤±æ•—ç‡
- API å»¶é²
- éŒ¯èª¤ç‡

**Apache Flink å¯¦ä½œ**:

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.window import TumblingEventTimeWindows
from pyflink.common import Time

env = StreamExecutionEnvironment.get_execution_environment()

# è®€å–äº‹ä»¶æµ
events = env.add_source(KafkaSource(...))

# è¨ˆç®—æ¯åˆ†é˜çš„éŒ¯èª¤ç‡
error_rate = (
    events
    .key_by(lambda e: e['service'])
    .window(TumblingEventTimeWindows.of(Time.minutes(1)))
    .apply(lambda key, window, events: {
        'service': key,
        'error_rate': sum(1 for e in events if e['status'] == 'error') / len(events),
        'window': window
    })
)

# æª¢æ¸¬ç•°å¸¸
alerts = error_rate.filter(lambda e: e['error_rate'] > 0.05)

# ç™¼é€å‘Šè­¦
alerts.add_sink(AlertSink())
```

---

## ğŸ¤” Lambda vs Kappa æ¶æ§‹

### Lambda æ¶æ§‹

**è¨­è¨ˆ**: æ‰¹æ¬¡å±¤ + é€Ÿåº¦å±¤ + æœå‹™å±¤

```mermaid
graph TD
    A["è³‡æ–™æº"] --> B["æ‰¹æ¬¡å±¤<br/>(Hadoop/Spark)"]
    A --> C["é€Ÿåº¦å±¤<br/>(Flink/Storm)"]
    
    B --> D["æœå‹™å±¤<br/>(æŸ¥è©¢çµæœ)"]
    C --> D
```

**å„ªé»**: æ‰¹æ¬¡å±¤æä¾›å®Œæ•´æº–ç¢ºçš„çµæœ
**ç¼ºé»**: ç¶­è­·å…©å¥—ç¨‹å¼ç¢¼

---

### Kappa æ¶æ§‹

**è¨­è¨ˆ**: åªæœ‰ä¸²æµå±¤

```mermaid
graph LR
    A["è³‡æ–™æº"] --> B["Kafka<br/>(æŒä¹…åŒ–)"]
    B --> C["Flink<br/>(ä¸²æµè™•ç†)"]
    C --> D["çµæœ"]
```

**å„ªé»**: ç¨‹å¼ç¢¼ç°¡åŒ–
**ç¼ºé»**: éœ€è¦æ”¯æŒé‡æ–°è™•ç†æ­·å²è³‡æ–™

---

## ğŸ“š ç¸½çµ

### æ ¸å¿ƒè¦é»

```mermaid
graph TD
    A["ä¸²æµè™•ç†"] --> B["æ™‚é–“èªç¾©<br/>Event Time + Watermark"]
    A --> C["è¦–çª—æ“ä½œ<br/>Tumbling/Sliding/Session"]
    A --> D["ç‹€æ…‹ç®¡ç†<br/>Checkpoint"]
    A --> E["å®¹éŒ¯èªç¾©<br/>Exactly-Once"]
```

### é¸æ“‡æŒ‡å—

| éœ€æ±‚ | æ¨è–¦æ–¹æ¡ˆ |
|------|---------|
| å¯¦æ™‚ç›£æ§ | Kafka + Flink |
| å¯¦æ™‚æ¨è–¦ | Kafka + Spark Streaming |
| CDC åŒæ­¥ | Debezium + Kafka |
| è¤‡é›œäº‹ä»¶è™•ç† | Flink CEP |

---

## ğŸ”— åƒè€ƒè³‡æ–™

1. **æ›¸ç±**:
   - Martin Kleppmann, *Designing Data-Intensive Applications*, Chapter 11
   - *Streaming Systems* by Tyler Akidau

2. **è«–æ–‡**:
   - [The Dataflow Model](https://research.google/pubs/pub43864/)
   - [Kafka: a Distributed Messaging System](https://notes.stephenholiday.com/Kafka.pdf)

3. **æ–‡ä»¶**:
   - [Apache Flink Documentation](https://flink.apache.org/)
   - [Kafka Streams](https://kafka.apache.org/documentation/streams/)
