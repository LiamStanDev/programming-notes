# 09 - æ‰¹æ¬¡è™•ç† (Batch Processing)

## ğŸ¯ å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬ç« å¾Œ,ä½ å°‡èƒ½å¤ :
- ç†è§£æ‰¹æ¬¡è™•ç†çš„æ¦‚å¿µèˆ‡æ‡‰ç”¨å ´æ™¯
- æŒæ¡ MapReduce ç·¨ç¨‹æ¨¡å‹
- ç†è§£ç¾ä»£è³‡æ–™æµå¼•æ“çš„è¨­è¨ˆ
- é¸æ“‡åˆé©çš„æ‰¹æ¬¡è™•ç†æ¡†æ¶

---

## ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ

### ä»€éº¼æ˜¯æ‰¹æ¬¡è™•ç†?

**æ‰¹æ¬¡è™•ç† (Batch Processing)**: å®šæœŸè™•ç†å¤§é‡ç´¯ç©çš„è³‡æ–™,é€šå¸¸ä»¥å°æ™‚ã€å¤©ç‚ºå–®ä½ã€‚

```mermaid
graph LR
    A["è¼¸å…¥è³‡æ–™<br/>(TB ç´š)"] --> B["æ‰¹æ¬¡è™•ç†<br/>(æ•¸å°æ™‚)"]
    B --> C["è¼¸å‡ºçµæœ<br/>(å ±è¡¨ã€æ¨¡å‹ç­‰)"]
    
    style A fill:#e1f5ff
    style B fill:#fff9c4
    style C fill:#c8e6c9
```

**ç‰¹æ€§**:
- **å»¶é²**: åˆ†é˜åˆ°å°æ™‚ç´šåˆ¥
- **ååé‡**: æ¥µé«˜ (TB/PB ç´šè³‡æ–™)
- **å®¹éŒ¯**: å¤±æ•—å¯é‡è©¦

**vs ä¸²æµè™•ç†**:

| ç‰¹æ€§ | æ‰¹æ¬¡è™•ç† | ä¸²æµè™•ç† |
|------|---------|---------|
| **å»¶é²** | é«˜ (å°æ™‚) | ä½ (ç§’/æ¯«ç§’) |
| **è³‡æ–™é‡** | å¤§ (TB/PB) | å° (æŒçºŒæµå…¥) |
| **ä½¿ç”¨å ´æ™¯** | é›¢ç·šåˆ†æã€è¨“ç·´æ¨¡å‹ | å¯¦æ™‚ç›£æ§ã€å³æ™‚æ¨è–¦ |

---

## ğŸ“Š MapReduce ç·¨ç¨‹æ¨¡å‹

### åŸºæœ¬æ¦‚å¿µ

**MapReduce**: Google æå‡ºçš„åˆ†æ•£å¼è¨ˆç®—æ¨¡å‹,å°‡è¨ˆç®—åˆ†ç‚º Map å’Œ Reduce å…©å€‹éšæ®µã€‚

```mermaid
graph TD
    A["è¼¸å…¥è³‡æ–™"] --> B["Map éšæ®µ"]
    B --> C["ä¸­é–“è³‡æ–™<br/>(Key-Value)"]
    C --> D["Shuffle éšæ®µ<br/>(åˆ†çµ„æ’åº)"]
    D --> E["Reduce éšæ®µ"]
    E --> F["è¼¸å‡ºçµæœ"]
```

### ğŸ’» Word Count ç¯„ä¾‹

**å•é¡Œ**: çµ±è¨ˆæ–‡æœ¬ä¸­æ¯å€‹å–®è©çš„å‡ºç¾æ¬¡æ•¸ã€‚

**Python å¯¦ä½œ**:

```python
from collections import defaultdict
from typing import Iterator, Tuple

def map_function(document: str) -> Iterator[Tuple[str, int]]:
    """Map: æ‹†åˆ†æ–‡æª”,ç™¼å‡º (word, 1)"""
    words = document.lower().split()
    for word in words:
        yield (word, 1)

def reduce_function(word: str, counts: Iterator[int]) -> Tuple[str, int]:
    """Reduce: ç´¯åŠ ç›¸åŒå–®è©çš„è¨ˆæ•¸"""
    return (word, sum(counts))

# ç¯„ä¾‹åŸ·è¡Œ
documents = [
    "hello world",
    "hello hadoop",
    "hello mapreduce"
]

# Map éšæ®µ
intermediate = []
for doc in documents:
    for key_value in map_function(doc):
        intermediate.append(key_value)

print("Map è¼¸å‡º:", intermediate)
# [('hello', 1), ('world', 1), ('hello', 1), ('hadoop', 1), ('hello', 1), ('mapreduce', 1)]

# Shuffle éšæ®µ: æŒ‰ Key åˆ†çµ„
grouped = defaultdict(list)
for key, value in intermediate:
    grouped[key].append(value)

# Reduce éšæ®µ
results = []
for word, counts in grouped.items():
    results.append(reduce_function(word, counts))

print("æœ€çµ‚çµæœ:", dict(results))
# {'hello': 3, 'world': 1, 'hadoop': 1, 'mapreduce': 1}
```

### Hadoop MapReduce å¯¦ä½œ

```python
# Hadoop Streaming ç¯„ä¾‹: mapper.py
import sys

for line in sys.stdin:
    words = line.strip().split()
    for word in words:
        print(f"{word}\t1")

# reducer.py
import sys
from collections import defaultdict

current_word = None
current_count = 0

for line in sys.stdin:
    word, count = line.strip().split('\t')
    count = int(count)
    
    if word == current_word:
        current_count += count
    else:
        if current_word:
            print(f"{current_word}\t{current_count}")
        current_word = word
        current_count = count

if current_word:
    print(f"{current_word}\t{current_count}")

# åŸ·è¡Œ
# hadoop jar hadoop-streaming.jar \
#   -input /input/data.txt \
#   -output /output/ \
#   -mapper mapper.py \
#   -reducer reducer.py
```

---

## ğŸ”— Join æ“ä½œ

### Sort-Merge Join

**å ´æ™¯**: é—œè¯å…©å€‹å¤§å‹è³‡æ–™é›†ã€‚

```python
# ä½¿ç”¨è€…è³‡æ–™
users = [
    {"user_id": 1, "name": "Alice"},
    {"user_id": 2, "name": "Bob"}
]

# è¨‚å–®è³‡æ–™
orders = [
    {"order_id": 101, "user_id": 1, "amount": 50},
    {"order_id": 102, "user_id": 2, "amount": 30},
    {"order_id": 103, "user_id": 1, "amount": 70}
]

# Map éšæ®µ: æ¨™è¨˜è³‡æ–™ä¾†æº
def map_users(user):
    return (user['user_id'], ('user', user))

def map_orders(order):
    return (order['user_id'], ('order', order))

# Shuffle: æŒ‰ user_id åˆ†çµ„
# {1: [('user', {...}), ('order', {...}), ('order', {...})],
#  2: [('user', {...}), ('order', {...})]}

# Reduce: Join
def reduce_join(user_id, records):
    user_data = None
    orders_data = []
    
    for source, record in records:
        if source == 'user':
            user_data = record
        else:
            orders_data.append(record)
    
    # è¼¸å‡ºé—œè¯çµæœ
    for order in orders_data:
        yield {
            'user_id': user_id,
            'user_name': user_data['name'],
            'order_id': order['order_id'],
            'amount': order['amount']
        }
```

### Broadcast Join

**å„ªåŒ–**: ç•¶ä¸€å€‹è³‡æ–™é›†å¾ˆå°æ™‚,å¯ä»¥å»£æ’­åˆ°æ‰€æœ‰ç¯€é»ã€‚

```python
# PySpark ç¯„ä¾‹
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("BroadcastJoin").getOrCreate()

# å¤§è¡¨
orders = spark.read.parquet("/data/orders")

# å°è¡¨ (å¯ä»¥æ”¾å…¥å…§å­˜)
users = spark.read.parquet("/data/users")

# å»£æ’­ Join
from pyspark.sql.functions import broadcast

result = orders.join(broadcast(users), "user_id")
```

---

## ğŸš€ ç¾ä»£æ‰¹æ¬¡è™•ç†æ¡†æ¶

### Apache Spark

**ç‰¹é»**:
- å…§å­˜è¨ˆç®—,æ¯” Hadoop å¿« 10-100 å€
- æ”¯æŒ SQLã€MLã€åœ–è¨ˆç®—
- æƒ°æ€§æ±‚å€¼ (Lazy Evaluation)

**ç¯„ä¾‹: æ—¥èªŒåˆ†æ**:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, hour

spark = SparkSession.builder.appName("LogAnalysis").getOrCreate()

# è®€å–æ—¥èªŒ
logs = spark.read.json("/logs/access.log")

# çµ±è¨ˆæ¯å°æ™‚çš„è«‹æ±‚æ•¸
hourly_stats = (
    logs
    .withColumn("hour", hour(col("timestamp")))
    .groupBy("hour")
    .agg(count("*").alias("request_count"))
    .orderBy("hour")
)

hourly_stats.show()

# +----+-------------+
# |hour|request_count|
# +----+-------------+
# |   0|        1250 |
# |   1|         980 |
# ...
```

**DataFrame vs RDD**:

```python
# RDD (ä½éš API)
rdd = spark.sparkContext.textFile("/data/users.csv")
result = (
    rdd
    .map(lambda line: line.split(','))
    .filter(lambda fields: int(fields[2]) > 18)
    .count()
)

# DataFrame (é«˜éš API, æ¨è–¦)
df = spark.read.csv("/data/users.csv", header=True)
result = df.filter(col("age") > 18).count()
```

---

### Apache Flink

**ç‰¹é»**:
- çµ±ä¸€æ‰¹æ¬¡å’Œä¸²æµè™•ç†
- çœŸæ­£çš„ä¸²æµè™•ç†å¼•æ“ (éå¾®æ‰¹æ¬¡)
- ç‹€æ…‹ç®¡ç†èˆ‡ Exactly-Once èªç¾©

**ç¯„ä¾‹**:

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment

# ç’°å¢ƒè¨­ç½®
env = StreamExecutionEnvironment.get_execution_environment()
table_env = StreamTableEnvironment.create(env)

# å®šç¾©è¡¨
table_env.execute_sql("""
    CREATE TABLE orders (
        order_id BIGINT,
        user_id BIGINT,
        amount DECIMAL(10, 2),
        order_time TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'orders',
        'properties.bootstrap.servers' = 'localhost:9092'
    )
""")

# æŸ¥è©¢
result = table_env.sql_query("""
    SELECT 
        user_id,
        COUNT(*) as order_count,
        SUM(amount) as total_amount
    FROM orders
    WHERE order_time > CURRENT_TIMESTAMP - INTERVAL '1' HOUR
    GROUP BY user_id
""")

result.execute().print()
```

---

## ğŸ¢ çœŸå¯¦ä¸–ç•Œæ¡ˆä¾‹

### æ¡ˆä¾‹ 1: Netflix æ¨è–¦ç³»çµ±

**æ‰¹æ¬¡è™•ç†éƒ¨åˆ†**:
- æ¯å¤©è™•ç†æ•¸ç™¾ TB çš„è§€çœ‹æ—¥èªŒ
- ä½¿ç”¨ Spark è¨ˆç®—ç”¨æˆ¶ç›¸ä¼¼åº¦
- è¨“ç·´æ©Ÿå™¨å­¸ç¿’æ¨¡å‹

**æ¶æ§‹**:

```mermaid
graph LR
    A["è§€çœ‹æ—¥èªŒ<br/>(S3)"] --> B["Spark Job<br/>(EMR)"]
    B --> C["ç‰¹å¾µå·¥ç¨‹"]
    C --> D["æ¨¡å‹è¨“ç·´"]
    D --> E["æ¨è–¦çµæœ<br/>(Cassandra)"]
```

---

### æ¡ˆä¾‹ 2: Uber è³‡æ–™æ¹–

**æŠ€è¡“æ£§**:
- **å­˜å„²**: HDFS / S3
- **è¨ˆç®—**: Spark / Presto
- **èª¿åº¦**: Airflow
- **ç›®éŒ„**: Hive Metastore

**ETL æµç¨‹**:

```python
from airflow import DAG
from airflow.operators.spark_submit_operator import SparkSubmitOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'trip_analytics',
    default_args=default_args,
    schedule_interval='@daily'
)

# ä»»å‹™ 1: æå–åŸå§‹è³‡æ–™
extract = SparkSubmitOperator(
    task_id='extract_trips',
    application='/jobs/extract_trips.py',
    dag=dag
)

# ä»»å‹™ 2: è½‰æ›
transform = SparkSubmitOperator(
    task_id='transform_trips',
    application='/jobs/transform_trips.py',
    dag=dag
)

# ä»»å‹™ 3: åŠ è¼‰åˆ°è³‡æ–™å€‰å„²
load = SparkSubmitOperator(
    task_id='load_to_warehouse',
    application='/jobs/load_trips.py',
    dag=dag
)

extract >> transform >> load
```

---

## ğŸ“š ç¸½çµ

### æ ¸å¿ƒè¦é»

```mermaid
graph TD
    A["æ‰¹æ¬¡è™•ç†"] --> B["MapReduce<br/>ç¶“å…¸æ¨¡å‹"]
    A --> C["Spark<br/>å…§å­˜è¨ˆç®—"]
    A --> D["Flink<br/>çµ±ä¸€æ‰¹æµ"]
    
    B --> B1["ç°¡å–®ä½†æ…¢"]
    C --> C1["å¿«é€Ÿä¸”æ˜“ç”¨"]
    D --> D1["å¯¦æ™‚æ€§æœ€ä½³"]
```

### é¸æ“‡æŒ‡å—

| å ´æ™¯ | æ¨è–¦æ¡†æ¶ |
|------|---------|
| ç°¡å–® ETL | Spark |
| è¤‡é›œåˆ†æ | Spark + SQL |
| å¯¦æ™‚ + æ‰¹æ¬¡ | Flink |
| è¶…å¤§è¦æ¨¡ | Spark / Hadoop |

---

## ğŸ”— åƒè€ƒè³‡æ–™

1. **æ›¸ç±**:
   - Martin Kleppmann, *Designing Data-Intensive Applications*, Chapter 10
   - *Hadoop: The Definitive Guide*

2. **è«–æ–‡**:
   - [MapReduce: Simplified Data Processing on Large Clusters](https://research.google/pubs/pub62/)
   - [Spark: Cluster Computing with Working Sets](https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf)

3. **æ–‡ä»¶**:
   - [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
   - [Apache Flink Documentation](https://flink.apache.org/docs/stable/)
