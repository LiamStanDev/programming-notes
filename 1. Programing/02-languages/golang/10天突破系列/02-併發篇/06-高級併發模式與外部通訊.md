# Day 6ï¼šé«˜ç´šä½µç™¼æ¨¡å¼èˆ‡å¤–éƒ¨é€šè¨Š

## ğŸ“š å­¸ç¿’ç›®æ¨™

- å¯¦ç¾ Fan-in/Fan-out æ•¸æ“šè™•ç†ç®¡é“
- æŒæ¡ Redis ç·©å­˜èˆ‡ Pub/Sub æ¨¡å¼
- ä½¿ç”¨ Kafka é€²è¡Œç•°æ­¥æ¶ˆæ¯ç™¼é€
- å¯¦ç¾åˆ†å¸ƒå¼ç³»çµ±ä¸­çš„ä½µç™¼æ§åˆ¶

---

## 1. Fan-out / Fan-in æ¨¡å¼æ·±å…¥

### 1.1 æ•¸æ“šè™•ç†ç®¡é“

```go
// ç”Ÿæˆæ•¸æ“š
func generator(nums ...int) <-chan int {
    out := make(chan int)
    go func() {
        defer close(out)
        for _, n := range nums {
            out <- n
        }
    }()
    return out
}

// Fan-outï¼šåˆ†ç™¼åˆ°å¤šå€‹è™•ç†å™¨
func process(in <-chan int, workers int) []<-chan int {
    channels := make([]<-chan int, workers)
    for i := 0; i < workers; i++ {
        channels[i] = worker(in, i)
    }
    return channels
}

func worker(in <-chan int, id int) <-chan int {
    out := make(chan int)
    go func() {
        defer close(out)
        for n := range in {
            fmt.Printf("Worker %d processing %d\n", id, n)
            out <- n * n
        }
    }()
    return out
}

// Fan-inï¼šåˆä½µçµæœ
func merge(channels ...<-chan int) <-chan int {
    var wg sync.WaitGroup
    out := make(chan int)
    
    output := func(c <-chan int) {
        defer wg.Done()
        for n := range c {
            out <- n
        }
    }
    
    wg.Add(len(channels))
    for _, c := range channels {
        go output(c)
    }
    
    go func() {
        wg.Wait()
        close(out)
    }()
    
    return out
}
```

---

## 2. Redis æ•´åˆ

### 2.1 å®‰è£èˆ‡é€£æ¥

```bash
go get github.com/redis/go-redis/v9
```

```go
import (
    "context"
    "github.com/redis/go-redis/v9"
)

func NewRedisClient() *redis.Client {
    return redis.NewClient(&redis.Options{
        Addr:     "localhost:6379",
        Password: "",
        DB:       0,
        PoolSize: 10,
    })
}
```

### 2.2 ç·©å­˜æ¨¡å¼

```go
type UserCache struct {
    client *redis.Client
    repo   UserRepository
}

func (c *UserCache) GetUser(ctx context.Context, id int) (*User, error) {
    key := fmt.Sprintf("user:%d", id)
    
    // å˜—è©¦å¾ç·©å­˜è®€å–
    data, err := c.client.Get(ctx, key).Bytes()
    if err == nil {
        var user User
        if err := json.Unmarshal(data, &user); err == nil {
            return &user, nil
        }
    }
    
    // ç·©å­˜æœªå‘½ä¸­ï¼Œå¾æ•¸æ“šåº«è®€å–
    user, err := c.repo.GetByID(id)
    if err != nil {
        return nil, err
    }
    
    // å¯«å…¥ç·©å­˜
    data, _ = json.Marshal(user)
    c.client.Set(ctx, key, data, 10*time.Minute)
    
    return user, nil
}
```

### 2.3 Pub/Sub æ¨¡å¼

```go
// ç™¼å¸ƒè€…
func publishMessages(ctx context.Context, client *redis.Client) {
    for i := 0; i < 10; i++ {
        msg := fmt.Sprintf("Message %d", i)
        err := client.Publish(ctx, "notifications", msg).Err()
        if err != nil {
            log.Println("Publish error:", err)
        }
        time.Sleep(time.Second)
    }
}

// è¨‚é–±è€…
func subscribe(ctx context.Context, client *redis.Client) {
    pubsub := client.Subscribe(ctx, "notifications")
    defer pubsub.Close()
    
    ch := pubsub.Channel()
    
    for {
        select {
        case msg := <-ch:
            fmt.Printf("Received: %s\n", msg.Payload)
        case <-ctx.Done():
            return
        }
    }
}
```

---

## 3. Kafka æ•´åˆ

### 3.1 å®‰è£

```bash
go get github.com/segmentio/kafka-go
```

### 3.2 Producer

```go
import "github.com/segmentio/kafka-go"

type EventProducer struct {
    writer *kafka.Writer
}

func NewEventProducer(brokers []string, topic string) *EventProducer {
    return &EventProducer{
        writer: &kafka.Writer{
            Addr:     kafka.TCP(brokers...),
            Topic:    topic,
            Balancer: &kafka.LeastBytes{},
        },
    }
}

func (p *EventProducer) PublishEvent(ctx context.Context, key, value string) error {
    return p.writer.WriteMessages(ctx, kafka.Message{
        Key:   []byte(key),
        Value: []byte(value),
    })
}

func (p *EventProducer) Close() error {
    return p.writer.Close()
}
```

### 3.3 Consumer

```go
type EventConsumer struct {
    reader *kafka.Reader
}

func NewEventConsumer(brokers []string, topic, groupID string) *EventConsumer {
    return &EventConsumer{
        reader: kafka.NewReader(kafka.ReaderConfig{
            Brokers:  brokers,
            Topic:    topic,
            GroupID:  groupID,
            MinBytes: 10e3,
            MaxBytes: 10e6,
        }),
    }
}

func (c *EventConsumer) Consume(ctx context.Context) {
    for {
        msg, err := c.reader.ReadMessage(ctx)
        if err != nil {
            log.Println("Error reading message:", err)
            break
        }
        
        fmt.Printf("Key: %s, Value: %s\n", string(msg.Key), string(msg.Value))
        
        // è™•ç†æ¶ˆæ¯
        if err := c.processMessage(msg); err != nil {
            log.Println("Error processing:", err)
        }
    }
}

func (c *EventConsumer) processMessage(msg kafka.Message) error {
    // æ¥­å‹™é‚è¼¯
    return nil
}
```

---

## 4. åˆ†å¸ƒå¼é–ï¼ˆRedisï¼‰

### 4.1 å¯¦ç¾åˆ†å¸ƒå¼é–

```go
type DistributedLock struct {
    client *redis.Client
    key    string
    value  string
    ttl    time.Duration
}

func NewDistributedLock(client *redis.Client, key string, ttl time.Duration) *DistributedLock {
    return &DistributedLock{
        client: client,
        key:    key,
        value:  uuid.New().String(),
        ttl:    ttl,
    }
}

func (l *DistributedLock) Acquire(ctx context.Context) (bool, error) {
    result, err := l.client.SetNX(ctx, l.key, l.value, l.ttl).Result()
    return result, err
}

func (l *DistributedLock) Release(ctx context.Context) error {
    script := `
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("del", KEYS[1])
        else
            return 0
        end
    `
    
    _, err := l.client.Eval(ctx, script, []string{l.key}, l.value).Result()
    return err
}

// ä½¿ç”¨ç¤ºä¾‹
func processWithLock(ctx context.Context, client *redis.Client) error {
    lock := NewDistributedLock(client, "task:process", 30*time.Second)
    
    acquired, err := lock.Acquire(ctx)
    if err != nil {
        return err
    }
    if !acquired {
        return fmt.Errorf("failed to acquire lock")
    }
    defer lock.Release(ctx)
    
    // åŸ·è¡Œéœ€è¦é–ä¿è­·çš„æ“ä½œ
    return doWork()
}
```

---

## 5. å¯¦æˆ°ï¼šæ—¥èªŒèšåˆç³»çµ±

### 5.1 ç³»çµ±æ¶æ§‹

```go
// æ—¥èªŒæ”¶é›†å™¨
type LogCollector struct {
    producer *EventProducer
}

func (c *LogCollector) Collect(ctx context.Context, logs []LogEntry) error {
    g, ctx := errgroup.WithContext(ctx)
    
    for _, log := range logs {
        log := log
        g.Go(func() error {
            data, _ := json.Marshal(log)
            return c.producer.PublishEvent(ctx, log.Level, string(data))
        })
    }
    
    return g.Wait()
}

// æ—¥èªŒè™•ç†å™¨
type LogProcessor struct {
    consumer *EventConsumer
    storage  LogStorage
}

func (p *LogProcessor) Process(ctx context.Context) error {
    return p.consumer.Consume(ctx, func(msg kafka.Message) error {
        var log LogEntry
        if err := json.Unmarshal(msg.Value, &log); err != nil {
            return err
        }
        
        return p.storage.Save(ctx, &log)
    })
}
```

### 5.2 æ•¸æ“šç®¡é“

```go
func BuildLogPipeline(ctx context.Context, input <-chan LogEntry) <-chan ProcessedLog {
    // Stage 1: éæ¿¾
    filtered := filter(ctx, input, func(log LogEntry) bool {
        return log.Level >= LevelWarn
    })
    
    // Stage 2: å¢å¼·ï¼ˆæ·»åŠ å…ƒæ•¸æ“šï¼‰
    enriched := enrich(ctx, filtered)
    
    // Stage 3: è½‰æ›
    transformed := transform(ctx, enriched)
    
    return transformed
}

func filter(ctx context.Context, in <-chan LogEntry, fn func(LogEntry) bool) <-chan LogEntry {
    out := make(chan LogEntry)
    go func() {
        defer close(out)
        for log := range orDone(ctx, in) {
            if fn(log.(LogEntry)) {
                select {
                case out <- log.(LogEntry):
                case <-ctx.Done():
                    return
                }
            }
        }
    }()
    return out
}

func enrich(ctx context.Context, in <-chan LogEntry) <-chan LogEntry {
    out := make(chan LogEntry)
    go func() {
        defer close(out)
        for log := range orDone(ctx, in) {
            enriched := log.(LogEntry)
            enriched.Hostname, _ = os.Hostname()
            enriched.Timestamp = time.Now()
            
            select {
            case out <- enriched:
            case <-ctx.Done():
                return
            }
        }
    }()
    return out
}

func transform(ctx context.Context, in <-chan LogEntry) <-chan ProcessedLog {
    out := make(chan ProcessedLog)
    go func() {
        defer close(out)
        for log := range orDone(ctx, in) {
            processed := ProcessedLog{
                ID:        uuid.New().String(),
                Log:       log.(LogEntry),
                ProcessedAt: time.Now(),
            }
            
            select {
            case out <- processed:
            case <-ctx.Done():
                return
            }
        }
    }()
    return out
}
```

---

## 6. å¯¦æˆ°ç·´ç¿’

### ç·´ç¿’ 1ï¼šå¯¦ç¾é™æµå™¨

```go
type RateLimiter struct {
    client *redis.Client
    key    string
    limit  int
    window time.Duration
}

func (r *RateLimiter) Allow(ctx context.Context, userID string) (bool, error) {
    // TODO: ä½¿ç”¨ Redis å¯¦ç¾æ»‘å‹•çª—å£é™æµ
    // æç¤ºï¼šä½¿ç”¨ ZSET å­˜å„²æ™‚é–“æˆ³
}
```

### ç·´ç¿’ 2ï¼šå¯¦ç¾äº‹ä»¶æº¯æº

```go
type EventStore struct {
    producer *EventProducer
}

func (s *EventStore) Append(ctx context.Context, event Event) error {
    // TODO: å°‡äº‹ä»¶æŒä¹…åŒ–åˆ° Kafka
}

type EventProcessor struct {
    consumer *EventConsumer
}

func (p *EventProcessor) Replay(ctx context.Context, aggregateID string) ([]Event, error) {
    // TODO: é‡æ”¾ç‰¹å®šèšåˆçš„æ‰€æœ‰äº‹ä»¶
}
```

### ç·´ç¿’ 3ï¼šå¯¦ç¾æ‰¹é‡æ•¸æ“šå°å‡º

```go
type DataExporter struct {
    repo      DataRepository
    batchSize int
}

func (e *DataExporter) Export(ctx context.Context, query Query) (<-chan []Record, error) {
    // TODO: ä½¿ç”¨ Fan-out æ¨¡å¼ä¸¦ç™¼æŸ¥è©¢æ•¸æ“š
    // TODO: ä½¿ç”¨ Fan-in æ¨¡å¼åˆä½µçµæœ
    // TODO: åˆ†æ‰¹è¿”å›æ•¸æ“šåˆ° Channel
}
```

---

## 7. æœ€ä½³å¯¦è¸ç¸½çµ

### âœ… Do's
1. **ä½¿ç”¨ Context æ§åˆ¶ Goroutine ç”Ÿå‘½é€±æœŸ**
2. **åˆç†è¨­ç½®é€£æ¥æ± å¤§å°**
3. **ä½¿ç”¨ Pipeline æé«˜è™•ç†æ•ˆç‡**
4. **å¯¦ç¾å„ªé›…é—œé–‰æ©Ÿåˆ¶**
5. **è¨˜éŒ„è©³ç´°çš„éŒ¯èª¤æ—¥èªŒ**

### âŒ Don'ts
1. **ä¸è¦å¿½ç•¥ Redis/Kafka é€£æ¥éŒ¯èª¤**
2. **ä¸è¦åœ¨ç†±è·¯å¾‘ä¸­é€²è¡Œåºåˆ—åŒ–æ“ä½œ**
3. **ä¸è¦ç„¡é™åˆ¶å‰µå»º Goroutine**
4. **ä¸è¦å¿˜è¨˜è™•ç†æ¶ˆæ¯é‡è¤‡**
5. **ä¸è¦åœ¨äº‹å‹™ä¸­åŸ·è¡Œé•·æ™‚é–“æ“ä½œ**

---

## 8. å»¶ä¼¸é–±è®€

- [go-redis Documentation](https://redis.uptrace.dev/)
- [kafka-go Documentation](https://github.com/segmentio/kafka-go)
- [Distributed Systems Patterns](https://www.youtube.com/watch?v=Y6Ev8GIlbxc)

---

**ä¸Šä¸€ç¯‡**: [Day 5 - Context èˆ‡ä½µç™¼æ¨¡å¼](05-Contextèˆ‡ä½µç™¼æ¨¡å¼.md)  
**ä¸‹ä¸€ç¯‡**: [Day 7 - Web æ¡†æ¶èˆ‡ä¸­é–“ä»¶è¨­è¨ˆ](../03-Webé–‹ç™¼ç¯‡/07-Webæ¡†æ¶èˆ‡ä¸­é–“ä»¶è¨­è¨ˆ.md)
