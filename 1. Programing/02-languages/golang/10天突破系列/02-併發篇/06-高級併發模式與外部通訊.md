# Day 6ï¼šé«˜ç´šä½µç™¼æ¨¡å¼èˆ‡å¤–éƒ¨é€šè¨Š

## ğŸ“š å­¸ç¿’ç›®æ¨™

- å¯¦ç¾ Fan-in/Fan-out æ•¸æ“šè™•ç†ç®¡é“
- æŒæ¡ Redis ç·©å­˜èˆ‡ Pub/Sub æ¨¡å¼
- ä½¿ç”¨ Kafka é€²è¡Œç•°æ­¥æ¶ˆæ¯ç™¼é€
- å¯¦ç¾åˆ†å¸ƒå¼ç³»çµ±ä¸­çš„ä½µç™¼æ§åˆ¶

---

## 1. Fan-out / Fan-in æ¨¡å¼æ·±å…¥

### 1.1 æ•¸æ“šè™•ç†ç®¡é“

```go
// ç”Ÿæˆæ•¸æ“š
func generator(nums ...int) <-chan int {
    out := make(chan int)
    go func() {
        defer close(out)
        for _, n := range nums {
            out <- n
        }
    }()
    return out
}

// Fan-outï¼šåˆ†ç™¼åˆ°å¤šå€‹è™•ç†å™¨
func process(in <-chan int, workers int) []<-chan int {
    channels := make([]<-chan int, workers)
    for i := 0; i < workers; i++ {
        channels[i] = worker(in, i)
    }
    return channels
}

func worker(in <-chan int, id int) <-chan int {
    out := make(chan int)
    go func() {
        defer close(out)
        for n := range in {
            fmt.Printf("Worker %d processing %d\n", id, n)
            out <- n * n
        }
    }()
    return out
}

// Fan-inï¼šåˆä½µçµæœ
func merge(channels ...<-chan int) <-chan int {
    var wg sync.WaitGroup
    out := make(chan int)
    
    output := func(c <-chan int) {
        defer wg.Done()
        for n := range c {
            out <- n
        }
    }
    
    wg.Add(len(channels))
    for _, c := range channels {
        go output(c)
    }
    
    go func() {
        wg.Wait()
        close(out)
    }()
    
    return out
}
```

---

## 2. Redis æ•´åˆ

### 2.1 å®‰è£èˆ‡é€£æ¥

```bash
go get github.com/redis/go-redis/v9
```

```go
import (
    "context"
    "github.com/redis/go-redis/v9"
)

func NewRedisClient() *redis.Client {
    return redis.NewClient(&redis.Options{
        Addr:     "localhost:6379",
        Password: "",
        DB:       0,
        PoolSize: 10,
    })
}
```

### 2.2 ç·©å­˜æ¨¡å¼

```go
type UserCache struct {
    client *redis.Client
    repo   UserRepository
}

func (c *UserCache) GetUser(ctx context.Context, id int) (*User, error) {
    key := fmt.Sprintf("user:%d", id)
    
    // å˜—è©¦å¾ç·©å­˜è®€å–
    data, err := c.client.Get(ctx, key).Bytes()
    if err == nil {
        var user User
        if err := json.Unmarshal(data, &user); err == nil {
            return &user, nil
        }
    }
    
    // ç·©å­˜æœªå‘½ä¸­ï¼Œå¾æ•¸æ“šåº«è®€å–
    user, err := c.repo.GetByID(id)
    if err != nil {
        return nil, err
    }
    
    // å¯«å…¥ç·©å­˜
    data, _ = json.Marshal(user)
    c.client.Set(ctx, key, data, 10*time.Minute)
    
    return user, nil
}
```

### 2.3 Pub/Sub æ¨¡å¼

```go
// ç™¼å¸ƒè€…
func publishMessages(ctx context.Context, client *redis.Client) {
    for i := 0; i < 10; i++ {
        msg := fmt.Sprintf("Message %d", i)
        err := client.Publish(ctx, "notifications", msg).Err()
        if err != nil {
            log.Println("Publish error:", err)
        }
        time.Sleep(time.Second)
    }
}

// è¨‚é–±è€…
func subscribe(ctx context.Context, client *redis.Client) {
    pubsub := client.Subscribe(ctx, "notifications")
    defer pubsub.Close()
    
    ch := pubsub.Channel()
    
    for {
        select {
        case msg := <-ch:
            fmt.Printf("Received: %s\n", msg.Payload)
        case <-ctx.Done():
            return
        }
    }
}
```

---

## 3. Kafka æ•´åˆ

### 3.1 å®‰è£

```bash
go get github.com/segmentio/kafka-go
```

### 3.2 Producer

```go
import "github.com/segmentio/kafka-go"

type EventProducer struct {
    writer *kafka.Writer
}

func NewEventProducer(brokers []string, topic string) *EventProducer {
    return &EventProducer{
        writer: &kafka.Writer{
            Addr:     kafka.TCP(brokers...),
            Topic:    topic,
            Balancer: &kafka.LeastBytes{},
        },
    }
}

func (p *EventProducer) PublishEvent(ctx context.Context, key, value string) error {
    return p.writer.WriteMessages(ctx, kafka.Message{
        Key:   []byte(key),
        Value: []byte(value),
    })
}

func (p *EventProducer) Close() error {
    return p.writer.Close()
}
```

### 3.3 Consumer

```go
type EventConsumer struct {
    reader *kafka.Reader
}

func NewEventConsumer(brokers []string, topic, groupID string) *EventConsumer {
    return &EventConsumer{
        reader: kafka.NewReader(kafka.ReaderConfig{
            Brokers:  brokers,
            Topic:    topic,
            GroupID:  groupID,
            MinBytes: 10e3,
            MaxBytes: 10e6,
        }),
    }
}

func (c *EventConsumer) Consume(ctx context.Context) {
    for {
        msg, err := c.reader.ReadMessage(ctx)
        if err != nil {
            log.Println("Error reading message:", err)
            break
        }
        
        fmt.Printf("Key: %s, Value: %s\n", string(msg.Key), string(msg.Value))
        
        // è™•ç†æ¶ˆæ¯
        if err := c.processMessage(msg); err != nil {
            log.Println("Error processing:", err)
        }
    }
}

func (c *EventConsumer) processMessage(msg kafka.Message) error {
    // æ¥­å‹™é‚è¼¯
    return nil
}
```

---

## 4. å¯¦æˆ°ç·´ç¿’

è¦‹å¾ŒçºŒå®Œæ•´ç‰ˆæœ¬...

---

**ä¸Šä¸€ç¯‡**: [Day 5 - Context èˆ‡ä½µç™¼æ¨¡å¼](05-Contextèˆ‡ä½µç™¼æ¨¡å¼.md)  
**ä¸‹ä¸€ç¯‡**: [Day 7 - Web æ¡†æ¶èˆ‡ä¸­é–“ä»¶è¨­è¨ˆ](../03-Webé–‹ç™¼ç¯‡/07-Webæ¡†æ¶èˆ‡ä¸­é–“ä»¶è¨­è¨ˆ.md)
