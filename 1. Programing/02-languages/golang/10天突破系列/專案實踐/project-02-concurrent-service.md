# Project 02ï¼šé«˜æ€§èƒ½ä½µç™¼æœå‹™

## ğŸ¯ é …ç›®ç›®æ¨™

æ§‹å»ºä¸€å€‹**ç”Ÿç”¢ç´šåˆ¥**çš„ä½µç™¼æ•¸æ“šè™•ç†æœå‹™ï¼Œæ¶µè“‹ï¼š
- âœ… Worker Pool æ¨¡å¼å¯¦ç¾
- âœ… Context è¶…æ™‚èˆ‡å–æ¶ˆæ§åˆ¶
- âœ… Fan-out/Fan-in æ•¸æ“šè™•ç†ç®¡é“
- âœ… errgroup éŒ¯èª¤æ”¶é›†
- âœ… å„ªé›…é—œé–‰æ©Ÿåˆ¶
- âœ… æ€§èƒ½ç›£æ§èˆ‡æŒ‡æ¨™
- âœ… å–®å…ƒæ¸¬è©¦èˆ‡åŸºæº–æ¸¬è©¦

---

## ğŸ“ é …ç›®çµæ§‹

```
project-02-concurrent-service/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ worker/
â”‚       â””â”€â”€ main.go                 # æ‡‰ç”¨å…¥å£
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ worker/                     # Worker Pool å¯¦ç¾
â”‚   â”‚   â”œâ”€â”€ pool.go                 # Worker Pool
â”‚   â”‚   â””â”€â”€ worker.go               # Worker
â”‚   â”œâ”€â”€ pipeline/                   # æ•¸æ“šè™•ç†ç®¡é“
â”‚   â”‚   â”œâ”€â”€ pipeline.go             # ç®¡é“å”èª¿å™¨
â”‚   â”‚   â”œâ”€â”€ processor.go            # æ•¸æ“šè™•ç†å™¨
â”‚   â”‚   â””â”€â”€ aggregator.go           # çµæœèšåˆå™¨
â”‚   â””â”€â”€ task/                       # ä»»å‹™å®šç¾©
â”‚       â””â”€â”€ task.go
â”œâ”€â”€ pkg/
â”‚   â”œâ”€â”€ logger/
â”‚   â”‚   â””â”€â”€ logger.go
â”‚   â””â”€â”€ metrics/                    # æ€§èƒ½æŒ‡æ¨™
â”‚       â””â”€â”€ metrics.go
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ worker_test.go
â”‚   â””â”€â”€ benchmark_test.go
â”œâ”€â”€ .env.example
â”œâ”€â”€ Makefile
â”œâ”€â”€ go.mod
â””â”€â”€ README.md
```

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½å¯¦ç¾

### 1. ä»»å‹™å®šç¾©

**internal/task/task.go**
```go
package task

import (
	"context"
	"fmt"
	"time"
)

// Task ä»£è¡¨ä¸€å€‹å¾…è™•ç†çš„ä»»å‹™
type Task struct {
	ID        string
	Data      interface{}
	CreatedAt time.Time
	Priority  int // å„ªå…ˆç´š (1-10)
}

// Result ä»»å‹™è™•ç†çµæœ
type Result struct {
	TaskID    string
	Success   bool
	Data      interface{}
	Error     error
	Duration  time.Duration
	StartTime time.Time
	EndTime   time.Time
}

// Processor ä»»å‹™è™•ç†å™¨æ¥å£
type Processor interface {
	Process(ctx context.Context, task *Task) (*Result, error)
}

// SimpleProcessor ç°¡å–®è™•ç†å™¨å¯¦ç¾
type SimpleProcessor struct{}

func NewSimpleProcessor() *SimpleProcessor {
	return &SimpleProcessor{}
}

// Process è™•ç†ä»»å‹™ï¼ˆæ¨¡æ“¬è€—æ™‚æ“ä½œï¼‰
func (p *SimpleProcessor) Process(ctx context.Context, task *Task) (*Result, error) {
	startTime := time.Now()
	
	// æª¢æŸ¥ context æ˜¯å¦å·²å–æ¶ˆ
	select {
	case <-ctx.Done():
		return nil, ctx.Err()
	default:
	}
	
	// æ¨¡æ“¬è™•ç†æ™‚é–“
	processingTime := time.Duration(50+task.Priority*10) * time.Millisecond
	
	select {
	case <-time.After(processingTime):
		// è™•ç†æˆåŠŸ
		result := &Result{
			TaskID:    task.ID,
			Success:   true,
			Data:      fmt.Sprintf("processed: %v", task.Data),
			StartTime: startTime,
			EndTime:   time.Now(),
			Duration:  time.Since(startTime),
		}
		return result, nil
		
	case <-ctx.Done():
		// Context è¶…æ™‚æˆ–å–æ¶ˆ
		return nil, ctx.Err()
	}
}
```

---

### 2. Worker Pool å¯¦ç¾

**internal/worker/worker.go**
```go
package worker

import (
	"context"
	"fmt"
	"sync"
	
	"project-02/internal/task"
	"project-02/pkg/logger"
	"project-02/pkg/metrics"
)

// Worker å·¥ä½œå”ç¨‹
type Worker struct {
	ID        int
	taskChan  <-chan *task.Task
	resultChan chan<- *task.Result
	processor task.Processor
	logger    *logger.Logger
	metrics   *metrics.Metrics
	wg        *sync.WaitGroup
}

// NewWorker å‰µå»ºæ–°çš„ Worker
func NewWorker(
	id int,
	taskChan <-chan *task.Task,
	resultChan chan<- *task.Result,
	processor task.Processor,
	logger *logger.Logger,
	metrics *metrics.Metrics,
	wg *sync.WaitGroup,
) *Worker {
	return &Worker{
		ID:         id,
		taskChan:   taskChan,
		resultChan: resultChan,
		processor:  processor,
		logger:     logger,
		metrics:    metrics,
		wg:         wg,
	}
}

// Start å•Ÿå‹• Worker
func (w *Worker) Start(ctx context.Context) {
	defer w.wg.Done()
	
	w.logger.Info("worker started", "worker_id", w.ID)
	
	for {
		select {
		case <-ctx.Done():
			w.logger.Info("worker stopping", "worker_id", w.ID)
			return
			
		case t, ok := <-w.taskChan:
			if !ok {
				w.logger.Info("task channel closed, worker exiting", "worker_id", w.ID)
				return
			}
			
			// è™•ç†ä»»å‹™
			w.processTask(ctx, t)
		}
	}
}

func (w *Worker) processTask(ctx context.Context, t *task.Task) {
	w.logger.Debug("processing task", "worker_id", w.ID, "task_id", t.ID)
	w.metrics.IncrementTasksProcessing()
	
	// åŸ·è¡Œè™•ç†
	result, err := w.processor.Process(ctx, t)
	
	if err != nil {
		w.logger.Error("task processing failed",
			"worker_id", w.ID,
			"task_id", t.ID,
			"error", err,
		)
		
		result = &task.Result{
			TaskID:  t.ID,
			Success: false,
			Error:   err,
		}
		w.metrics.IncrementTasksFailed()
	} else {
		w.logger.Debug("task completed",
			"worker_id", w.ID,
			"task_id", t.ID,
			"duration", result.Duration,
		)
		w.metrics.IncrementTasksCompleted()
		w.metrics.RecordTaskDuration(result.Duration)
	}
	
	// ç™¼é€çµæœ
	select {
	case w.resultChan <- result:
	case <-ctx.Done():
		w.logger.Warn("context cancelled while sending result", "worker_id", w.ID)
	}
	
	w.metrics.DecrementTasksProcessing()
}
```

**internal/worker/pool.go**
```go
package worker

import (
	"context"
	"sync"
	"time"
	
	"project-02/internal/task"
	"project-02/pkg/logger"
	"project-02/pkg/metrics"
)

// Pool Worker Pool
type Pool struct {
	workerCount int
	taskChan    chan *task.Task
	resultChan  chan *task.Result
	processor   task.Processor
	logger      *logger.Logger
	metrics     *metrics.Metrics
	wg          sync.WaitGroup
	ctx         context.Context
	cancel      context.CancelFunc
}

// Config Pool é…ç½®
type Config struct {
	WorkerCount    int
	TaskBufferSize int
	ResultBufferSize int
}

// NewPool å‰µå»º Worker Pool
func NewPool(cfg Config, processor task.Processor, logger *logger.Logger, metrics *metrics.Metrics) *Pool {
	ctx, cancel := context.WithCancel(context.Background())
	
	return &Pool{
		workerCount: cfg.WorkerCount,
		taskChan:    make(chan *task.Task, cfg.TaskBufferSize),
		resultChan:  make(chan *task.Result, cfg.ResultBufferSize),
		processor:   processor,
		logger:      logger,
		metrics:     metrics,
		ctx:         ctx,
		cancel:      cancel,
	}
}

// Start å•Ÿå‹• Worker Pool
func (p *Pool) Start() {
	p.logger.Info("starting worker pool", "worker_count", p.workerCount)
	
	// å•Ÿå‹•æ‰€æœ‰ Worker
	for i := 0; i < p.workerCount; i++ {
		p.wg.Add(1)
		worker := NewWorker(i, p.taskChan, p.resultChan, p.processor, p.logger, p.metrics, &p.wg)
		go worker.Start(p.ctx)
	}
	
	p.logger.Info("worker pool started")
}

// Submit æäº¤ä»»å‹™
func (p *Pool) Submit(t *task.Task) error {
	select {
	case p.taskChan <- t:
		p.metrics.IncrementTasksSubmitted()
		return nil
	case <-p.ctx.Done():
		return p.ctx.Err()
	}
}

// SubmitWithTimeout æäº¤ä»»å‹™ï¼ˆå¸¶è¶…æ™‚ï¼‰
func (p *Pool) SubmitWithTimeout(t *task.Task, timeout time.Duration) error {
	ctx, cancel := context.WithTimeout(p.ctx, timeout)
	defer cancel()
	
	select {
	case p.taskChan <- t:
		p.metrics.IncrementTasksSubmitted()
		return nil
	case <-ctx.Done():
		return ctx.Err()
	}
}

// Results ç²å–çµæœ channel
func (p *Pool) Results() <-chan *task.Result {
	return p.resultChan
}

// Shutdown å„ªé›…é—œé–‰
func (p *Pool) Shutdown(timeout time.Duration) error {
	p.logger.Info("shutting down worker pool", "timeout", timeout)
	
	// 1. é—œé–‰ä»»å‹™ channelï¼ˆä¸å†æ¥å—æ–°ä»»å‹™ï¼‰
	close(p.taskChan)
	
	// 2. ç­‰å¾…æ‰€æœ‰ Worker å®Œæˆï¼ˆå¸¶è¶…æ™‚ï¼‰
	done := make(chan struct{})
	go func() {
		p.wg.Wait()
		close(done)
	}()
	
	select {
	case <-done:
		p.logger.Info("all workers finished gracefully")
	case <-time.After(timeout):
		p.logger.Warn("shutdown timeout, forcing cancellation")
		p.cancel()
		p.wg.Wait()
	}
	
	// 3. é—œé–‰çµæœ channel
	close(p.resultChan)
	
	p.logger.Info("worker pool shutdown complete")
	return nil
}

// Stop ç«‹å³åœæ­¢
func (p *Pool) Stop() {
	p.logger.Warn("force stopping worker pool")
	p.cancel()
	close(p.taskChan)
	p.wg.Wait()
	close(p.resultChan)
}
```

---

### 3. æ•¸æ“šè™•ç†ç®¡é“ï¼ˆFan-out/Fan-inï¼‰

**internal/pipeline/pipeline.go**
```go
package pipeline

import (
	"context"
	"sync"
	
	"golang.org/x/sync/errgroup"
	"project-02/internal/task"
	"project-02/pkg/logger"
)

// Pipeline æ•¸æ“šè™•ç†ç®¡é“
type Pipeline struct {
	logger    *logger.Logger
	processor task.Processor
}

// NewPipeline å‰µå»ºç®¡é“
func NewPipeline(logger *logger.Logger, processor task.Processor) *Pipeline {
	return &Pipeline{
		logger:    logger,
		processor: processor,
	}
}

// ProcessBatch æ‰¹é‡è™•ç†ï¼ˆFan-out/Fan-inï¼‰
func (p *Pipeline) ProcessBatch(ctx context.Context, tasks []*task.Task, concurrency int) ([]*task.Result, error) {
	// Fan-out: åˆ†ç™¼ä»»å‹™åˆ°å¤šå€‹ goroutine
	taskChan := make(chan *task.Task, len(tasks))
	resultChan := make(chan *task.Result, len(tasks))
	
	// å‰µå»º errgroup ç”¨æ–¼éŒ¯èª¤æ”¶é›†
	g, ctx := errgroup.WithContext(ctx)
	
	// å•Ÿå‹• Worker goroutines
	for i := 0; i < concurrency; i++ {
		workerID := i
		g.Go(func() error {
			for t := range taskChan {
				select {
				case <-ctx.Done():
					return ctx.Err()
				default:
					result, err := p.processor.Process(ctx, t)
					if err != nil {
						p.logger.Error("task processing failed",
							"worker_id", workerID,
							"task_id", t.ID,
							"error", err,
						)
						// å°‡éŒ¯èª¤åŒ…è£åˆ° result ä¸­
						result = &task.Result{
							TaskID:  t.ID,
							Success: false,
							Error:   err,
						}
					}
					
					select {
					case resultChan <- result:
					case <-ctx.Done():
						return ctx.Err()
					}
				}
			}
			return nil
		})
	}
	
	// ç™¼é€ä»»å‹™
	go func() {
		for _, t := range tasks {
			select {
			case taskChan <- t:
			case <-ctx.Done():
				break
			}
		}
		close(taskChan)
	}()
	
	// Fan-in: æ”¶é›†çµæœ
	results := make([]*task.Result, 0, len(tasks))
	var mu sync.Mutex
	
	// æ”¶é›†å™¨ goroutine
	go func() {
		for result := range resultChan {
			mu.Lock()
			results = append(results, result)
			mu.Unlock()
		}
	}()
	
	// ç­‰å¾…æ‰€æœ‰ Worker å®Œæˆ
	if err := g.Wait(); err != nil {
		close(resultChan)
		return nil, err
	}
	
	close(resultChan)
	
	// ç­‰å¾…æ”¶é›†å®Œæˆ
	// æ³¨æ„ï¼šé€™è£¡éœ€è¦ç¢ºä¿ resultChan è¢«å®Œå…¨æ¶ˆè²»
	
	return results, nil
}

// StreamProcess æµå¼è™•ç†ï¼ˆç„¡é™æµï¼‰
func (p *Pipeline) StreamProcess(ctx context.Context, input <-chan *task.Task, concurrency int) <-chan *task.Result {
	output := make(chan *task.Result)
	
	var wg sync.WaitGroup
	
	// å•Ÿå‹• Worker goroutines
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		workerID := i
		
		go func() {
			defer wg.Done()
			
			for {
				select {
				case <-ctx.Done():
					p.logger.Info("stream worker stopping", "worker_id", workerID)
					return
					
				case t, ok := <-input:
					if !ok {
						p.logger.Info("input channel closed", "worker_id", workerID)
						return
					}
					
					result, err := p.processor.Process(ctx, t)
					if err != nil {
						result = &task.Result{
							TaskID:  t.ID,
							Success: false,
							Error:   err,
						}
					}
					
					select {
					case output <- result:
					case <-ctx.Done():
						return
					}
				}
			}
		}()
	}
	
	// é—œé–‰ output channel ç•¶æ‰€æœ‰ worker å®Œæˆ
	go func() {
		wg.Wait()
		close(output)
		p.logger.Info("stream processing complete")
	}()
	
	return output
}
```

**internal/pipeline/aggregator.go**
```go
package pipeline

import (
	"context"
	"sync"
	"time"
	
	"project-02/internal/task"
	"project-02/pkg/logger"
)

// Aggregator çµæœèšåˆå™¨
type Aggregator struct {
	logger  *logger.Logger
	results []*task.Result
	mu      sync.RWMutex
}

// NewAggregator å‰µå»ºèšåˆå™¨
func NewAggregator(logger *logger.Logger) *Aggregator {
	return &Aggregator{
		logger:  logger,
		results: make([]*task.Result, 0),
	}
}

// Collect æ”¶é›†çµæœ
func (a *Aggregator) Collect(ctx context.Context, resultChan <-chan *task.Result) {
	for {
		select {
		case <-ctx.Done():
			a.logger.Info("aggregator stopping")
			return
			
		case result, ok := <-resultChan:
			if !ok {
				a.logger.Info("result channel closed, aggregator exiting")
				return
			}
			
			a.mu.Lock()
			a.results = append(a.results, result)
			a.mu.Unlock()
			
			a.logger.Debug("result collected",
				"task_id", result.TaskID,
				"success", result.Success,
			)
		}
	}
}

// GetResults ç²å–æ‰€æœ‰çµæœ
func (a *Aggregator) GetResults() []*task.Result {
	a.mu.RLock()
	defer a.mu.RUnlock()
	
	// è¿”å›å‰¯æœ¬ä»¥é¿å…ä½µç™¼å•é¡Œ
	results := make([]*task.Result, len(a.results))
	copy(results, a.results)
	
	return results
}

// GetStats ç²å–çµ±è¨ˆä¿¡æ¯
func (a *Aggregator) GetStats() Stats {
	a.mu.RLock()
	defer a.mu.RUnlock()
	
	stats := Stats{
		Total: len(a.results),
	}
	
	var totalDuration time.Duration
	
	for _, result := range a.results {
		if result.Success {
			stats.Successful++
		} else {
			stats.Failed++
		}
		totalDuration += result.Duration
	}
	
	if stats.Total > 0 {
		stats.AvgDuration = totalDuration / time.Duration(stats.Total)
	}
	
	return stats
}

// Stats çµ±è¨ˆä¿¡æ¯
type Stats struct {
	Total       int
	Successful  int
	Failed      int
	AvgDuration time.Duration
}
```

---

### 4. æ€§èƒ½æŒ‡æ¨™

**pkg/metrics/metrics.go**
```go
package metrics

import (
	"sync"
	"sync/atomic"
	"time"
)

// Metrics æ€§èƒ½æŒ‡æ¨™
type Metrics struct {
	tasksSubmitted  uint64
	tasksProcessing uint64
	tasksCompleted  uint64
	tasksFailed     uint64
	
	durations []time.Duration
	mu        sync.RWMutex
}

// New å‰µå»º Metrics
func New() *Metrics {
	return &Metrics{
		durations: make([]time.Duration, 0, 1000),
	}
}

// IncrementTasksSubmitted å¢åŠ æäº¤è¨ˆæ•¸
func (m *Metrics) IncrementTasksSubmitted() {
	atomic.AddUint64(&m.tasksSubmitted, 1)
}

// IncrementTasksProcessing å¢åŠ è™•ç†ä¸­è¨ˆæ•¸
func (m *Metrics) IncrementTasksProcessing() {
	atomic.AddUint64(&m.tasksProcessing, 1)
}

// DecrementTasksProcessing æ¸›å°‘è™•ç†ä¸­è¨ˆæ•¸
func (m *Metrics) DecrementTasksProcessing() {
	atomic.AddUint64(&m.tasksProcessing, ^uint64(0)) // -1
}

// IncrementTasksCompleted å¢åŠ å®Œæˆè¨ˆæ•¸
func (m *Metrics) IncrementTasksCompleted() {
	atomic.AddUint64(&m.tasksCompleted, 1)
}

// IncrementTasksFailed å¢åŠ å¤±æ•—è¨ˆæ•¸
func (m *Metrics) IncrementTasksFailed() {
	atomic.AddUint64(&m.tasksFailed, 1)
}

// RecordTaskDuration è¨˜éŒ„ä»»å‹™è€—æ™‚
func (m *Metrics) RecordTaskDuration(d time.Duration) {
	m.mu.Lock()
	defer m.mu.Unlock()
	m.durations = append(m.durations, d)
}

// GetSnapshot ç²å–æŒ‡æ¨™å¿«ç…§
func (m *Metrics) GetSnapshot() Snapshot {
	m.mu.RLock()
	defer m.mu.RUnlock()
	
	snapshot := Snapshot{
		TasksSubmitted:  atomic.LoadUint64(&m.tasksSubmitted),
		TasksProcessing: atomic.LoadUint64(&m.tasksProcessing),
		TasksCompleted:  atomic.LoadUint64(&m.tasksCompleted),
		TasksFailed:     atomic.LoadUint64(&m.tasksFailed),
	}
	
	if len(m.durations) > 0 {
		var total time.Duration
		for _, d := range m.durations {
			total += d
		}
		snapshot.AvgDuration = total / time.Duration(len(m.durations))
	}
	
	return snapshot
}

// Reset é‡ç½®æŒ‡æ¨™
func (m *Metrics) Reset() {
	atomic.StoreUint64(&m.tasksSubmitted, 0)
	atomic.StoreUint64(&m.tasksProcessing, 0)
	atomic.StoreUint64(&m.tasksCompleted, 0)
	atomic.StoreUint64(&m.tasksFailed, 0)
	
	m.mu.Lock()
	m.durations = m.durations[:0]
	m.mu.Unlock()
}

// Snapshot æŒ‡æ¨™å¿«ç…§
type Snapshot struct {
	TasksSubmitted  uint64
	TasksProcessing uint64
	TasksCompleted  uint64
	TasksFailed     uint64
	AvgDuration     time.Duration
}
```

---

### 5. ä¸»ç¨‹åº

**cmd/worker/main.go**
```go
package main

import (
	"context"
	"fmt"
	"os"
	"os/signal"
	"syscall"
	"time"
	
	"project-02/internal/pipeline"
	"project-02/internal/task"
	"project-02/internal/worker"
	"project-02/pkg/logger"
	"project-02/pkg/metrics"
)

func main() {
	// 1. åˆå§‹åŒ–æ—¥èªŒ
	log := logger.New("development")
	log.Info("starting concurrent service")
	
	// 2. åˆå§‹åŒ–æŒ‡æ¨™
	m := metrics.New()
	
	// 3. å‰µå»ºè™•ç†å™¨
	processor := task.NewSimpleProcessor()
	
	// 4. å‰µå»º Worker Pool
	poolCfg := worker.Config{
		WorkerCount:      10,
		TaskBufferSize:   100,
		ResultBufferSize: 100,
	}
	pool := worker.NewPool(poolCfg, processor, log, m)
	
	// 5. å•Ÿå‹• Worker Pool
	pool.Start()
	
	// 6. å‰µå»ºèšåˆå™¨
	aggregator := pipeline.NewAggregator(log)
	
	// 7. å•Ÿå‹•çµæœæ”¶é›†
	ctx, cancel := context.WithCancel(context.Background())
	go aggregator.Collect(ctx, pool.Results())
	
	// 8. æäº¤æ¸¬è©¦ä»»å‹™
	go submitTasks(pool, log)
	
	// 9. å®šæœŸæ‰“å°çµ±è¨ˆä¿¡æ¯
	go printStats(ctx, m, aggregator, log)
	
	// 10. ç­‰å¾…çµ‚æ­¢ä¿¡è™Ÿ
	quit := make(chan os.Signal, 1)
	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
	
	<-quit
	log.Info("received shutdown signal")
	
	// 11. å„ªé›…é—œé–‰
	cancel() // åœæ­¢çµ±è¨ˆå’Œèšåˆ
	
	if err := pool.Shutdown(10 * time.Second); err != nil {
		log.Error("pool shutdown failed", "error", err)
	}
	
	// 12. æ‰“å°æœ€çµ‚çµ±è¨ˆ
	finalStats := aggregator.GetStats()
	log.Info("final statistics",
		"total", finalStats.Total,
		"successful", finalStats.Successful,
		"failed", finalStats.Failed,
		"avg_duration", finalStats.AvgDuration,
	)
	
	log.Info("service stopped")
}

func submitTasks(pool *worker.Pool, log *logger.Logger) {
	for i := 0; i < 100; i++ {
		t := &task.Task{
			ID:        fmt.Sprintf("task-%d", i),
			Data:      fmt.Sprintf("data-%d", i),
			CreatedAt: time.Now(),
			Priority:  i % 10,
		}
		
		if err := pool.Submit(t); err != nil {
			log.Error("failed to submit task", "task_id", t.ID, "error", err)
			break
		}
		
		// æ¨¡æ“¬ä»»å‹™æäº¤é–“éš”
		time.Sleep(50 * time.Millisecond)
	}
	
	log.Info("all tasks submitted")
}

func printStats(ctx context.Context, m *metrics.Metrics, agg *pipeline.Aggregator, log *logger.Logger) {
	ticker := time.NewTicker(5 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			snapshot := m.GetSnapshot()
			stats := agg.GetStats()
			
			log.Info("statistics",
				"submitted", snapshot.TasksSubmitted,
				"processing", snapshot.TasksProcessing,
				"completed", snapshot.TasksCompleted,
				"failed", snapshot.TasksFailed,
				"avg_duration", stats.AvgDuration,
			)
		}
	}
}
```

---

### 6. æ¸¬è©¦

**tests/worker_test.go**
```go
package tests

import (
	"context"
	"testing"
	"time"
	
	"github.com/stretchr/testify/assert"
	"project-02/internal/task"
	"project-02/internal/worker"
	"project-02/pkg/logger"
	"project-02/pkg/metrics"
)

func TestWorkerPool_BasicFunctionality(t *testing.T) {
	// æº–å‚™
	log := logger.New("test")
	m := metrics.New()
	processor := task.NewSimpleProcessor()
	
	cfg := worker.Config{
		WorkerCount:      5,
		TaskBufferSize:   10,
		ResultBufferSize: 10,
	}
	pool := worker.NewPool(cfg, processor, log, m)
	
	// å•Ÿå‹•
	pool.Start()
	
	// æäº¤ä»»å‹™
	taskCount := 20
	for i := 0; i < taskCount; i++ {
		t := &task.Task{
			ID:       string(rune('A' + i)),
			Data:     i,
			Priority: 5,
		}
		err := pool.Submit(t)
		assert.NoError(t, err)
	}
	
	// æ”¶é›†çµæœ
	results := make([]*task.Result, 0, taskCount)
	done := make(chan struct{})
	
	go func() {
		for result := range pool.Results() {
			results = append(results, result)
			if len(results) == taskCount {
				close(done)
				return
			}
		}
	}()
	
	// ç­‰å¾…å®Œæˆï¼ˆå¸¶è¶…æ™‚ï¼‰
	select {
	case <-done:
		// æˆåŠŸ
	case <-time.After(10 * time.Second):
		t.Fatal("timeout waiting for results")
	}
	
	// é©—è­‰
	assert.Equal(t, taskCount, len(results))
	
	successCount := 0
	for _, r := range results {
		if r.Success {
			successCount++
		}
	}
	assert.Equal(t, taskCount, successCount)
	
	// é—œé–‰
	err := pool.Shutdown(5 * time.Second)
	assert.NoError(t, err)
}

func TestWorkerPool_ContextCancellation(t *testing.T) {
	log := logger.New("test")
	m := metrics.New()
	processor := task.NewSimpleProcessor()
	
	cfg := worker.Config{
		WorkerCount:      3,
		TaskBufferSize:   5,
		ResultBufferSize: 5,
	}
	pool := worker.NewPool(cfg, processor, log, m)
	pool.Start()
	
	// æäº¤ä»»å‹™å¾Œç«‹å³é—œé–‰
	for i := 0; i < 5; i++ {
		pool.Submit(&task.Task{ID: string(rune('A' + i)), Data: i, Priority: 1})
	}
	
	pool.Stop()
	
	// é©—è­‰ç„¡æ³•å†æäº¤ä»»å‹™
	err := pool.Submit(&task.Task{ID: "X", Data: 999})
	assert.Error(t, err)
}
```

**tests/benchmark_test.go**
```go
package tests

import (
	"fmt"
	"testing"
	"time"
	
	"project-02/internal/task"
	"project-02/internal/worker"
	"project-02/pkg/logger"
	"project-02/pkg/metrics"
)

func BenchmarkWorkerPool(b *testing.B) {
	log := logger.New("test")
	m := metrics.New()
	processor := task.NewSimpleProcessor()
	
	workerCounts := []int{1, 5, 10, 20, 50}
	
	for _, workerCount := range workerCounts {
		b.Run(fmt.Sprintf("Workers-%d", workerCount), func(b *testing.B) {
			cfg := worker.Config{
				WorkerCount:      workerCount,
				TaskBufferSize:   1000,
				ResultBufferSize: 1000,
			}
			pool := worker.NewPool(cfg, processor, log, m)
			pool.Start()
			
			b.ResetTimer()
			
			for i := 0; i < b.N; i++ {
				t := &task.Task{
					ID:       fmt.Sprintf("task-%d", i),
					Data:     i,
					Priority: 5,
				}
				pool.Submit(t)
			}
			
			// ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
			pool.Shutdown(30 * time.Second)
			
			b.StopTimer()
		})
	}
}
```

---

### 7. Makefile

```makefile
.PHONY: help run build test bench clean

help: ## é¡¯ç¤ºå¹«åŠ©
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-20s\033[0m %s\n", $$1, $$2}'

run: ## é‹è¡Œæœå‹™
	go run cmd/worker/main.go

build: ## æ§‹å»ºäºŒé€²åˆ¶
	go build -o bin/worker cmd/worker/main.go

test: ## é‹è¡Œæ¸¬è©¦
	go test -v -race ./tests/

bench: ## é‹è¡ŒåŸºæº–æ¸¬è©¦
	go test -v -bench=. -benchmem ./tests/

clean: ## æ¸…ç†æ§‹å»ºç”¢ç‰©
	rm -rf bin/

lint: ## ä»£ç¢¼æª¢æŸ¥
	golangci-lint run

coverage: ## æ¸¬è©¦è¦†è“‹ç‡
	go test -coverprofile=coverage.out ./tests/
	go tool cover -html=coverage.out
```

---

## ğŸ“ å­¸ç¿’è¦é»

### 1. Worker Pool æ¨¡å¼
- **å›ºå®šæ•¸é‡çš„ Worker**ï¼šé¿å…ç„¡é™åˆ¶å‰µå»º goroutine
- **ä»»å‹™éšŠåˆ—**ï¼šä½¿ç”¨ buffered channel ä½œç‚ºä»»å‹™éšŠåˆ—
- **çµæœæ”¶é›†**ï¼šé€šé channel æ”¶é›†è™•ç†çµæœ
- **å„ªé›…é—œé–‰**ï¼šå…ˆé—œé–‰ä»»å‹™ channelï¼Œç­‰å¾… Worker å®Œæˆ

### 2. Context æ§åˆ¶
- **è¶…æ™‚æ§åˆ¶**ï¼š`context.WithTimeout` æ§åˆ¶ä»»å‹™åŸ·è¡Œæ™‚é–“
- **å–æ¶ˆå‚³æ’­**ï¼šçˆ¶ context å–æ¶ˆæ™‚ï¼Œæ‰€æœ‰å­ context è‡ªå‹•å–æ¶ˆ
- **è³‡æºæ¸…ç†**ï¼šé€šé `defer cancel()` ç¢ºä¿è³‡æºé‡‹æ”¾

### 3. Fan-out/Fan-in æ¨¡å¼
- **Fan-out**ï¼šå°‡ä»»å‹™åˆ†ç™¼åˆ°å¤šå€‹ goroutine ä¸¦è¡Œè™•ç†
- **Fan-in**ï¼šå¾å¤šå€‹ goroutine æ”¶é›†çµæœåˆ°å–®ä¸€ channel
- **errgroup**ï¼šä½¿ç”¨ `golang.org/x/sync/errgroup` é€²è¡ŒéŒ¯èª¤æ”¶é›†

### 4. ä½µç™¼å®‰å…¨
- **sync.Mutex**ï¼šä¿è­·å…±äº«è³‡æº
- **atomic æ“ä½œ**ï¼šç”¨æ–¼è¨ˆæ•¸å™¨ç­‰ç°¡å–®æ“ä½œ
- **Channel é€šä¿¡**ï¼šæ¨è–¦ä½¿ç”¨ channel è€Œéå…±äº«å…§å­˜

### 5. æ€§èƒ½ç›£æ§
- **æŒ‡æ¨™æ”¶é›†**ï¼šè¨˜éŒ„ä»»å‹™æ•¸é‡ã€è€—æ™‚ç­‰é—œéµæŒ‡æ¨™
- **å®šæœŸå ±å‘Š**ï¼šä½¿ç”¨ `time.Ticker` å®šæœŸè¼¸å‡ºçµ±è¨ˆ
- **åŸºæº–æ¸¬è©¦**ï¼šä½¿ç”¨ `testing.B` é€²è¡Œæ€§èƒ½æ¸¬è©¦

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

```bash
# 1. é‹è¡Œæœå‹™
make run

# 2. é‹è¡Œæ¸¬è©¦
make test

# 3. é‹è¡ŒåŸºæº–æ¸¬è©¦
make bench

# 4. æŸ¥çœ‹æ¸¬è©¦è¦†è“‹ç‡
make coverage
```

---

## ğŸ“Š æ€§èƒ½æ¸¬è©¦çµæœç¤ºä¾‹

```
BenchmarkWorkerPool/Workers-1    1000   1200000 ns/op
BenchmarkWorkerPool/Workers-5    5000    300000 ns/op
BenchmarkWorkerPool/Workers-10  10000    150000 ns/op
BenchmarkWorkerPool/Workers-20  15000    100000 ns/op
BenchmarkWorkerPool/Workers-50  20000     80000 ns/op
```

---

## ğŸ“ æ“´å±•æ–¹å‘

- [ ] æ·»åŠ å„ªå…ˆç´šéšŠåˆ—
- [ ] å¯¦ç¾å‹•æ…‹èª¿æ•´ Worker æ•¸é‡
- [ ] æ·»åŠ ä»»å‹™é‡è©¦æ©Ÿåˆ¶
- [ ] é›†æˆ Prometheus æŒ‡æ¨™å°å‡º
- [ ] å¯¦ç¾åˆ†å¸ƒå¼ä»»å‹™éšŠåˆ—ï¼ˆRedis/Kafkaï¼‰
- [ ] æ·»åŠ ä»»å‹™æŒä¹…åŒ–ï¼ˆå¤±æ•—é‡è©¦ï¼‰
