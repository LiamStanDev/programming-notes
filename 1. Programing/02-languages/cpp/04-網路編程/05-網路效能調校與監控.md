# 網路效能調校與監控 (Network Performance Tuning and Monitoring)

## 概述

HFT 系統對網路延遲極度敏感,微秒級的優化至關重要。本章涵蓋 Linux 網路協定棧調校、硬體優化、監控工具與常見瓶頸分析。

## 系統層級調校

### 中斷親和性 (IRQ Affinity)

```bash
# 檢視網卡中斷
cat /proc/interrupts | grep eth0

# 將網卡中斷綁定到特定 CPU
echo 1 > /proc/irq/125/smp_affinity  # CPU 0
echo 2 > /proc/irq/126/smp_affinity  # CPU 1
echo 4 > /proc/irq/127/smp_affinity  # CPU 2

# 或使用 irqbalance 停用自動平衡
systemctl stop irqbalance

# 使用 set_irq_affinity 腳本 (Intel 提供)
./set_irq_affinity.sh 0-3 eth0  # 綁定到 CPU 0-3
```

### CPU 頻率與省電模式

```bash
# 檢視 CPU 頻率調節器
cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# 設定為 performance (固定最高頻率)
echo performance | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# 停用 Intel Turbo Boost (可選,降低抖動)
echo 1 > /sys/devices/system/cpu/intel_pstate/no_turbo

# 停用 C-States (省電狀態)
cpupower idle-set -D 0
```

### 程序 CPU 綁定

```cpp
#include <sched.h>
#include <pthread.h>

// 將程序綁定到特定 CPU
void pin_to_cpu(int cpu_id) {
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    CPU_SET(cpu_id, &cpuset);
    
    if (sched_setaffinity(0, sizeof(cpuset), &cpuset) < 0) {
        throw std::runtime_error("sched_setaffinity() failed");
    }
}

// 將執行緒綁定到特定 CPU
void pin_thread_to_cpu(pthread_t thread, int cpu_id) {
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    CPU_SET(cpu_id, &cpuset);
    
    if (pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset) < 0) {
        throw std::runtime_error("pthread_setaffinity_np() failed");
    }
}

// 使用範例
void network_thread() {
    pin_to_cpu(0);  // 綁定到 CPU 0
    
    // 網路處理邏輯...
}
```

### NUMA 配置

```bash
# 檢視 NUMA 拓樸
numactl --hardware

# 將程序綁定到特定 NUMA 節點
numactl --cpunodebind=0 --membind=0 ./my_hft_app

# 檢視 NUMA 統計
numastat -p $(pidof my_hft_app)
```

```cpp
#include <numa.h>

// 分配 NUMA 本地記憶體
void* allocate_numa_memory(size_t size, int node) {
    if (numa_available() < 0) {
        throw std::runtime_error("NUMA not available");
    }
    
    return numa_alloc_onnode(size, node);
}

// 使用範例
void numa_aware_allocation() {
    int cpu = sched_getcpu();
    int node = numa_node_of_cpu(cpu);
    
    // 分配本地記憶體
    void* buffer = allocate_numa_memory(1024 * 1024, node);
    
    // ... 使用 buffer ...
    
    numa_free(buffer, 1024 * 1024);
}
```

## 網路協定棧調校

### TCP 參數優化

```bash
# /etc/sysctl.conf 或 sysctl -w

# 1. TCP 緩衝區大小
net.core.rmem_max = 134217728           # 128 MB
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 87380 67108864  # min default max
net.ipv4.tcp_wmem = 4096 65536 67108864

# 2. TCP 擁塞控制
net.ipv4.tcp_congestion_control = bbr   # 或 cubic

# 3. TCP Fast Open (TFO)
net.ipv4.tcp_fastopen = 3               # Client & Server

# 4. TCP timestamps (可選停用,減少開銷)
net.ipv4.tcp_timestamps = 0

# 5. TCP SACK (選擇性確認)
net.ipv4.tcp_sack = 1

# 6. 連線追蹤表大小 (高並發)
net.netfilter.nf_conntrack_max = 1000000

# 7. SYN backlog
net.ipv4.tcp_max_syn_backlog = 8192

# 8. 本地埠範圍
net.ipv4.ip_local_port_range = 10000 65535

# 9. TIME_WAIT socket 重用
net.ipv4.tcp_tw_reuse = 1

# 10. FIN timeout
net.ipv4.tcp_fin_timeout = 15
```

### UDP 參數優化

```bash
# UDP 接收緩衝區 (避免丟包)
net.core.rmem_default = 16777216
net.core.rmem_max = 33554432

# 增大 netdev backlog
net.core.netdev_max_backlog = 5000

# 檢查 UDP 丟包統計
netstat -su | grep "packet receive errors"
```

### Socket 緩衝區動態調整

```cpp
#include <sys/socket.h>

void optimize_socket_buffer(int sockfd) {
    // 讀取當前緩衝區大小
    int current_size;
    socklen_t len = sizeof(current_size);
    getsockopt(sockfd, SOL_SOCKET, SO_RCVBUF, &current_size, &len);
    
    printf("Current RX buffer: %d bytes\n", current_size);
    
    // 設定更大的緩衝區
    int new_size = 16 * 1024 * 1024;  // 16 MB
    if (setsockopt(sockfd, SOL_SOCKET, SO_RCVBUF, &new_size, sizeof(new_size)) < 0) {
        perror("setsockopt(SO_RCVBUF)");
    }
    
    // 驗證實際大小 (內核可能調整)
    getsockopt(sockfd, SOL_SOCKET, SO_RCVBUF, &current_size, &len);
    printf("New RX buffer: %d bytes\n", current_size);
}
```

## NIC (網卡) 硬體調校

### Ring Buffer 大小

```bash
# 檢視當前 ring buffer 大小
ethtool -g eth0

# 設定最大 ring buffer
ethtool -G eth0 rx 4096 tx 4096
```

### 中斷合併 (Interrupt Coalescing)

```bash
# 檢視中斷合併設定
ethtool -c eth0

# 降低延遲: 減少中斷合併
ethtool -C eth0 rx-usecs 0 tx-usecs 0

# 或針對 HFT: 極低延遲模式
ethtool -C eth0 adaptive-rx off adaptive-tx off rx-usecs 0 tx-usecs 0
```

### RSS (Receive Side Scaling)

```bash
# 啟用 RSS (多佇列網卡)
ethtool -L eth0 combined 4  # 4 個佇列

# 檢視 RSS 配置
ethtool -x eth0

# 配置 RSS hash key (可選)
ethtool -X eth0 hkey <hash_key>
```

### TSO/GSO/GRO 調整

```bash
# 檢視 offload 功能
ethtool -k eth0

# HFT 場景: 停用 offload (降低延遲)
ethtool -K eth0 tso off gso off gro off

# 或僅停用 GRO (Generic Receive Offload)
ethtool -K eth0 gro off
```

### Flow Director (Flow Steering)

```bash
# 將特定流量導向特定 CPU 佇列
ethtool -N eth0 flow-type tcp4 src-ip 192.168.1.100 dst-port 8080 action 0

# 檢視 Flow Director 規則
ethtool -n eth0
```

## 低延遲監控

### 延遲測量: 硬體時間戳

```cpp
#include <linux/net_tstamp.h>
#include <linux/sockios.h>

// 啟用硬體時間戳
void enable_hardware_timestamp(int sockfd) {
    int flags = SOF_TIMESTAMPING_RX_HARDWARE | 
                SOF_TIMESTAMPING_TX_HARDWARE |
                SOF_TIMESTAMPING_RAW_HARDWARE;
    
    if (setsockopt(sockfd, SOL_SOCKET, SO_TIMESTAMPING, &flags, sizeof(flags)) < 0) {
        throw std::runtime_error("setsockopt(SO_TIMESTAMPING) failed");
    }
}

// 接收並讀取時間戳
void recv_with_timestamp(int sockfd) {
    char buffer[2048];
    char control[1024];
    
    iovec iov{};
    iov.iov_base = buffer;
    iov.iov_len = sizeof(buffer);
    
    msghdr msg{};
    msg.msg_iov = &iov;
    msg.msg_iovlen = 1;
    msg.msg_control = control;
    msg.msg_controllen = sizeof(control);
    
    ssize_t n = recvmsg(sockfd, &msg, 0);
    
    if (n > 0) {
        // 解析控制訊息中的時間戳
        for (cmsghdr* cmsg = CMSG_FIRSTHDR(&msg); cmsg; cmsg = CMSG_NXTHDR(&msg, cmsg)) {
            if (cmsg->cmsg_level == SOL_SOCKET && 
                cmsg->cmsg_type == SO_TIMESTAMPING) {
                
                timespec* ts = (timespec*)CMSG_DATA(cmsg);
                
                printf("Hardware RX timestamp: %ld.%09ld\n", 
                       ts[2].tv_sec, ts[2].tv_nsec);
            }
        }
    }
}
```

### 應用層延遲統計

```cpp
#include <chrono>
#include <vector>
#include <algorithm>

class LatencyTracker {
    std::vector<uint64_t> latencies_;
    
public:
    void record(uint64_t latency_ns) {
        latencies_.push_back(latency_ns);
    }
    
    void print_statistics() {
        if (latencies_.empty()) return;
        
        std::sort(latencies_.begin(), latencies_.end());
        
        size_t count = latencies_.size();
        uint64_t min = latencies_.front();
        uint64_t max = latencies_.back();
        uint64_t median = latencies_[count / 2];
        uint64_t p99 = latencies_[count * 99 / 100];
        uint64_t p999 = latencies_[count * 999 / 1000];
        
        uint64_t sum = 0;
        for (auto lat : latencies_) {
            sum += lat;
        }
        uint64_t avg = sum / count;
        
        printf("Latency Statistics (ns):\n");
        printf("  Count:  %zu\n", count);
        printf("  Min:    %lu\n", min);
        printf("  Avg:    %lu\n", avg);
        printf("  Median: %lu\n", median);
        printf("  P99:    %lu\n", p99);
        printf("  P99.9:  %lu\n", p999);
        printf("  Max:    %lu\n", max);
    }
    
    void reset() {
        latencies_.clear();
    }
};

// 使用範例: 測量 RTT
void measure_rtt(int sockfd) {
    LatencyTracker tracker;
    char buffer[64] = "PING";
    
    for (int i = 0; i < 10000; ++i) {
        auto start = std::chrono::high_resolution_clock::now();
        
        send(sockfd, buffer, sizeof(buffer), 0);
        recv(sockfd, buffer, sizeof(buffer), 0);
        
        auto end = std::chrono::high_resolution_clock::now();
        auto latency = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();
        
        tracker.record(latency);
    }
    
    tracker.print_statistics();
}
```

## 監控工具

### sar - 系統活動報告

```bash
# 安裝 sysstat
sudo apt install sysstat

# 檢視網路統計 (每秒更新)
sar -n DEV 1

# 檢視 TCP 統計
sar -n TCP 1

# 檢視錯誤統計
sar -n EDEV 1

# 歷史資料 (需啟用 sysstat)
sar -n DEV -f /var/log/sysstat/sa01
```

### ss - Socket 統計

```bash
# 檢視 TCP 連線及緩衝區使用
ss -tim

# 檢視特定埠的連線
ss -tn state established '( dport = :8080 or sport = :8080 )'

# 檢視 socket 記憶體
ss -m

# 檢視擁塞控制資訊
ss -ti
```

### iftop - 即時流量監控

```bash
# 安裝
sudo apt install iftop

# 監控特定網卡
sudo iftop -i eth0

# 顯示埠
sudo iftop -P
```

### nload - 頻寬監控

```bash
sudo apt install nload
nload eth0
```

### perf - 效能分析

```bash
# 記錄網路事件
sudo perf record -e net:* -a -g -- sleep 10

# 分析報告
sudo perf report

# 檢視 TCP 重傳
sudo perf stat -e 'tcp:tcp_retransmit_skb' -a -- sleep 10

# CPU cache miss (與網路處理相關)
sudo perf stat -e cache-misses,cache-references ./my_app
```

### bpftrace - 動態追蹤

```bash
# 追蹤 TCP 連線
sudo bpftrace -e 'kprobe:tcp_connect { printf("Connect: %s\n", comm); }'

# 追蹤 TCP 重傳
sudo bpftrace -e 'kprobe:tcp_retransmit_skb { @retrans = count(); }'

# 追蹤 socket buffer 大小
sudo bpftrace -e 'kprobe:tcp_sendmsg { @size = hist(arg2); }'
```

## 效能瓶頸診斷

### 高 CPU 使用率

```bash
# 1. 檢視 CPU 使用
top -H -p $(pidof my_app)

# 2. 檢視系統調用
strace -c -p $(pidof my_app)

# 3. 檢視 CPU profiling
perf record -g -p $(pidof my_app) -- sleep 10
perf report

# 可能原因:
# - 過多系統調用 (使用批次 I/O)
# - Lock contention (使用 lock-free 結構)
# - Context switch (CPU pinning)
```

### UDP 丟包

```bash
# 1. 檢視丟包統計
netstat -su | grep -E "packet receive errors|receive buffer errors"

# 2. 檢視 ring buffer 丟包
ethtool -S eth0 | grep rx_dropped

# 3. 檢視 socket buffer overflow
ss -u -m

# 解決方案:
# - 增大 UDP 接收緩衝區
# - 增大 ring buffer 大小
# - 減少中斷合併延遲
# - 使用多佇列 RSS
```

### TCP 重傳

```bash
# 檢視重傳統計
netstat -s | grep retransmit

# 實時監控重傳
watch -n 1 'netstat -s | grep retransmit'

# 可能原因:
# - 網路擁塞 (調整 TCP 擁塞控制)
# - 封包亂序 (檢查網路設備)
# - 緩衝區不足 (增大 TCP buffer)
```

## HFT 最佳實踐檢查清單

### 硬體層級
- [ ] 網卡中斷綁定到特定 CPU (避免遷移)
- [ ] CPU 設定為 performance 模式 (固定最高頻率)
- [ ] 停用 C-States (避免延遲抖動)
- [ ] 應用程式綁定到特定 CPU
- [ ] NUMA-aware 記憶體分配
- [ ] 使用高效能網卡 (10GbE/25GbE/100GbE)
- [ ] 啟用 SR-IOV (虛擬化環境)

### 網卡層級
- [ ] 增大 ring buffer 大小
- [ ] 降低或停用中斷合併
- [ ] 啟用 RSS (多佇列)
- [ ] 配置 Flow Director
- [ ] 停用 TSO/GSO/GRO (極低延遲場景)
- [ ] 啟用硬體時間戳

### 協定棧層級
- [ ] 增大 TCP/UDP 緩衝區
- [ ] TCP 禁用 Nagle 演算法 (`TCP_NODELAY`)
- [ ] TCP 啟用 Quick ACK
- [ ] 調整 TCP 擁塞控制 (BBR/CUBIC)
- [ ] UDP 增大接收緩衝區
- [ ] 調整本地埠範圍

### 應用層級
- [ ] 使用 epoll/io_uring (避免 select/poll)
- [ ] Edge-Triggered 模式
- [ ] 批次處理 I/O (減少系統調用)
- [ ] 使用 Lock-Free 資料結構
- [ ] 零拷貝技術 (sendfile, MSG_ZEROCOPY)
- [ ] 考慮內核旁路 (DPDK, AF_XDP)

### 監控層級
- [ ] 記錄延遲統計 (P50/P99/P999)
- [ ] 監控 UDP 丟包率
- [ ] 監控 TCP 重傳率
- [ ] 監控 CPU 使用率
- [ ] 監控 context switch 次數
- [ ] 使用硬體時間戳測量延遲

---

## 參考資料 (References)

1. [Red Hat Performance Tuning Guide](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/)
2. [Linux Kernel Networking Documentation](https://www.kernel.org/doc/html/latest/networking/)
3. [Cloudflare - Optimizing TCP for high WAN throughput](https://blog.cloudflare.com/optimizing-tcp-for-high-throughput-and-low-latency/)
4. Gregg, Brendan. *Systems Performance: Enterprise and the Cloud* (2020)
5. [Intel Ethernet Flow Director](https://www.intel.com/content/www/us/en/architecture-and-technology/ethernet-flow-director-video.html)
6. [Tuning 10Gb network cards on Linux](https://fasterdata.es.net/host-tuning/linux/)
