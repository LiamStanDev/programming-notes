# 零拷貝與內核旁路技術 (Zero-Copy and Kernel Bypass)

## 概述

傳統網路 I/O 涉及多次記憶體拷貝與上下文切換,成為 HFT 系統的瓶頸。零拷貝 (Zero-Copy) 與內核旁路 (Kernel Bypass) 技術通過減少拷貝與避開內核協定棧,實現微秒級網路延遲。

## 傳統 I/O 的問題

### 資料拷貝路徑

```
讀取檔案並發送到網路 (傳統方式):

1. read(file_fd, buffer, size)
   Disk -> Kernel Buffer -> User Buffer (拷貝 1)
   
2. send(socket_fd, buffer, size)
   User Buffer -> Kernel Socket Buffer (拷貝 2)
   Kernel Socket Buffer -> NIC (DMA, 拷貝 3)

總計: 2 次 CPU 拷貝 + 1 次 DMA 拷貝 + 4 次上下文切換
```

### 上下文切換開銷

```cpp
// 每次系統調用都涉及上下文切換
char buffer[4096];

// User Mode -> Kernel Mode
ssize_t n = read(fd, buffer, sizeof(buffer));  // Context switch #1 & #2

// User Mode -> Kernel Mode
send(sockfd, buffer, n);                       // Context switch #3 & #4

// HFT 系統中,單次往返可能 > 1 μs
```

## Linux 零拷貝技術

### sendfile() - 檔案到 Socket

```cpp
#include <sys/sendfile.h>

// 零拷貝傳輸檔案
ssize_t zero_copy_file(int out_sockfd, int in_filefd, off_t offset, size_t count) {
    return sendfile(out_sockfd, in_filefd, &offset, count);
}

// 資料路徑:
// Disk -> Kernel Buffer -> NIC (僅 DMA 拷貝)
// 上下文切換: 2 次 (1 次系統調用)
```

**應用場景**: HTTP 伺服器發送靜態檔案

```cpp
// 高效能檔案伺服器
void serve_file(int client_sockfd, const char* filepath) {
    int filefd = open(filepath, O_RDONLY);
    if (filefd < 0) {
        throw std::runtime_error("open() failed");
    }
    
    struct stat st;
    fstat(filefd, &st);
    off_t offset = 0;
    
    // 一次系統調用傳輸整個檔案
    ssize_t sent = sendfile(client_sockfd, filefd, &offset, st.st_size);
    
    close(filefd);
}
```

### splice() - 管道間零拷貝

```cpp
#include <fcntl.h>

// Socket 到 Socket 零拷貝轉發 (Proxy)
ssize_t splice_proxy(int in_sockfd, int out_sockfd, size_t len) {
    int pipefd[2];
    pipe(pipefd);
    
    // in_sockfd -> pipe
    ssize_t bytes = splice(in_sockfd, nullptr, pipefd[1], nullptr, len, SPLICE_F_MOVE);
    
    if (bytes > 0) {
        // pipe -> out_sockfd
        splice(pipefd[0], nullptr, out_sockfd, nullptr, bytes, SPLICE_F_MOVE);
    }
    
    close(pipefd[0]);
    close(pipefd[1]);
    
    return bytes;
}
```

### MSG_ZEROCOPY (Kernel 4.14+)

```cpp
#include <sys/socket.h>
#include <linux/errqueue.h>

// 發送端零拷貝
class ZeroCopySender {
    int sockfd_;
    uint32_t notification_counter_ = 0;
    
public:
    explicit ZeroCopySender(int sockfd) : sockfd_(sockfd) {
        // 啟用 MSG_ZEROCOPY
        int optval = 1;
        setsockopt(sockfd_, SOL_SOCKET, SO_ZEROCOPY, &optval, sizeof(optval));
    }
    
    void send(const void* buffer, size_t len) {
        ssize_t sent = ::send(sockfd_, buffer, len, MSG_ZEROCOPY);
        
        if (sent < 0) {
            throw std::runtime_error("send(MSG_ZEROCOPY) failed");
        }
        
        notification_counter_++;
    }
    
    // 等待發送完成通知
    void wait_completion() {
        msghdr msg{};
        char control[100];
        msg.msg_control = control;
        msg.msg_controllen = sizeof(control);
        
        int ret = recvmsg(sockfd_, &msg, MSG_ERRQUEUE);
        
        if (ret > 0) {
            // 解析完成通知
            sock_extended_err* serr = nullptr;
            cmsghdr* cm = CMSG_FIRSTHDR(&msg);
            
            while (cm) {
                if (cm->cmsg_level == SOL_IP && cm->cmsg_type == IP_RECVERR) {
                    serr = (sock_extended_err*)CMSG_DATA(cm);
                    break;
                }
                cm = CMSG_NXTHDR(&msg, cm);
            }
            
            if (serr && serr->ee_errno == 0 && serr->ee_origin == SO_EE_ORIGIN_ZEROCOPY) {
                // 發送完成,可安全釋放緩衝區
                notification_counter_--;
            }
        }
    }
};

// 使用範例
void zerocopy_example() {
    int sockfd = create_tcp_client("127.0.0.1", 8080);
    ZeroCopySender sender(sockfd);
    
    std::vector<char> buffer(1024 * 1024);  // 1 MB
    
    sender.send(buffer.data(), buffer.size());
    
    // 等待發送完成 (異步)
    sender.wait_completion();
    
    // 現在可安全修改 buffer
}
```

**注意**: MSG_ZEROCOPY 僅對大緩衝區 (> 10 KB) 有效,小資料反而更慢

### mmap() + write()

```cpp
#include <sys/mman.h>
#include <sys/stat.h>

// 記憶體映射檔案發送
void mmap_send(int sockfd, const char* filepath) {
    int filefd = open(filepath, O_RDONLY);
    if (filefd < 0) {
        throw std::runtime_error("open() failed");
    }
    
    struct stat st;
    fstat(filefd, &st);
    
    // 將檔案映射到記憶體
    void* mapped = mmap(nullptr, st.st_size, PROT_READ, MAP_PRIVATE, filefd, 0);
    
    if (mapped == MAP_FAILED) {
        close(filefd);
        throw std::runtime_error("mmap() failed");
    }
    
    // 建議內核預讀
    madvise(mapped, st.st_size, MADV_SEQUENTIAL);
    
    // 發送 (避免拷貝到 user buffer)
    ssize_t sent = write(sockfd, mapped, st.st_size);
    
    munmap(mapped, st.st_size);
    close(filefd);
}
```

## 內核旁路: DPDK

### DPDK 簡介

Data Plane Development Kit (DPDK) 是 Intel 開發的高效能資料包處理框架:

```
傳統網路協定棧:
Application -> System Call -> Kernel TCP/IP Stack -> Driver -> NIC

DPDK (內核旁路):
Application (Poll Mode Driver) -> NIC (直接記憶體映射)

優勢:
- 無系統調用開銷
- 無上下文切換
- 零拷貝 (DMA 直達 user space)
- 批次處理 (batch processing)
```

### DPDK 核心概念

```cpp
// 1. EAL (Environment Abstraction Layer): 初始化
int ret = rte_eal_init(argc, argv);

// 2. Mempool: 預分配記憶體池
struct rte_mempool *mbuf_pool = rte_pktmbuf_pool_create(
    "MBUF_POOL",
    8192,              // 元素數量
    256,               // Cache size
    0,
    RTE_MBUF_DEFAULT_BUF_SIZE,
    rte_socket_id()
);

// 3. Port Configuration: 配置網卡
struct rte_eth_conf port_conf = {};
rte_eth_dev_configure(port_id, 1, 1, &port_conf);

// 4. RX/TX Queue Setup
rte_eth_rx_queue_setup(port_id, 0, 512, socket_id, nullptr, mbuf_pool);
rte_eth_tx_queue_setup(port_id, 0, 512, socket_id, nullptr);

// 5. Start Port
rte_eth_dev_start(port_id);
```

### DPDK 接收與發送

```cpp
#include <rte_eal.h>
#include <rte_ethdev.h>
#include <rte_mbuf.h>

class DPDKReceiver {
    uint16_t port_id_;
    struct rte_mempool *mbuf_pool_;
    
public:
    DPDKReceiver(uint16_t port_id, struct rte_mempool *pool) 
        : port_id_(port_id), mbuf_pool_(pool) {}
    
    void receive_loop() {
        struct rte_mbuf *rx_bufs[32];
        
        while (true) {
            // 批次接收 (無阻塞)
            uint16_t nb_rx = rte_eth_rx_burst(port_id_, 0, rx_bufs, 32);
            
            for (uint16_t i = 0; i < nb_rx; ++i) {
                // 處理封包
                process_packet(rx_bufs[i]);
                
                // 釋放 mbuf
                rte_pktmbuf_free(rx_bufs[i]);
            }
        }
    }
    
private:
    void process_packet(struct rte_mbuf *mbuf) {
        // 零拷貝存取封包資料
        uint8_t *pkt_data = rte_pktmbuf_mtod(mbuf, uint8_t*);
        uint16_t pkt_len = rte_pktmbuf_data_len(mbuf);
        
        // 解析 Ethernet Header
        struct rte_ether_hdr *eth_hdr = (struct rte_ether_hdr*)pkt_data;
        
        // 解析 IP Header
        if (eth_hdr->ether_type == rte_cpu_to_be_16(RTE_ETHER_TYPE_IPV4)) {
            struct rte_ipv4_hdr *ip_hdr = (struct rte_ipv4_hdr*)(eth_hdr + 1);
            
            // 解析 UDP Header
            if (ip_hdr->next_proto_id == IPPROTO_UDP) {
                struct rte_udp_hdr *udp_hdr = (struct rte_udp_hdr*)(ip_hdr + 1);
                
                // 應用層資料
                uint8_t *payload = (uint8_t*)(udp_hdr + 1);
                uint16_t payload_len = rte_be_to_cpu_16(udp_hdr->dgram_len) - sizeof(*udp_hdr);
                
                handle_market_data(payload, payload_len);
            }
        }
    }
};
```

### DPDK 發送封包

```cpp
void dpdk_send_packet(uint16_t port_id, struct rte_mempool *pool,
                      const void* data, size_t len) {
    // 分配 mbuf
    struct rte_mbuf *mbuf = rte_pktmbuf_alloc(pool);
    if (!mbuf) {
        throw std::runtime_error("rte_pktmbuf_alloc() failed");
    }
    
    // 拷貝資料到 mbuf (或直接構造封包)
    uint8_t *pkt_data = rte_pktmbuf_mtod(mbuf, uint8_t*);
    memcpy(pkt_data, data, len);
    
    mbuf->data_len = len;
    mbuf->pkt_len = len;
    
    // 發送 (批次)
    struct rte_mbuf *tx_bufs[1] = {mbuf};
    uint16_t nb_tx = rte_eth_tx_burst(port_id, 0, tx_bufs, 1);
    
    if (nb_tx < 1) {
        rte_pktmbuf_free(mbuf);
        throw std::runtime_error("rte_eth_tx_burst() failed");
    }
}
```

## HFT 應用: DPDK 市場資料接收

### 完整 DPDK 市場資料接收器

```cpp
#include <rte_eal.h>
#include <rte_ethdev.h>
#include <rte_cycles.h>

#define RX_RING_SIZE 1024
#define TX_RING_SIZE 1024
#define NUM_MBUFS 8191
#define MBUF_CACHE_SIZE 250
#define BURST_SIZE 32

struct MarketDataPacket {
    uint32_t sequence;
    uint32_t symbol_id;
    double price;
    int64_t volume;
    uint64_t timestamp;
} __attribute__((packed));

class DPDKMarketDataReceiver {
    uint16_t port_id_;
    struct rte_mempool *mbuf_pool_;
    
    // 統計
    uint64_t packets_received_ = 0;
    uint64_t bytes_received_ = 0;
    
public:
    DPDKMarketDataReceiver(int argc, char** argv) {
        // 初始化 EAL
        int ret = rte_eal_init(argc, argv);
        if (ret < 0) {
            throw std::runtime_error("rte_eal_init() failed");
        }
        
        // 建立 mempool
        mbuf_pool_ = rte_pktmbuf_pool_create("MBUF_POOL", NUM_MBUFS,
            MBUF_CACHE_SIZE, 0, RTE_MBUF_DEFAULT_BUF_SIZE, rte_socket_id());
        
        if (!mbuf_pool_) {
            throw std::runtime_error("rte_pktmbuf_pool_create() failed");
        }
        
        // 配置網卡
        port_id_ = 0;
        port_init();
    }
    
    void run() {
        printf("Starting market data receiver on port %u\n", port_id_);
        
        struct rte_mbuf *rx_bufs[BURST_SIZE];
        uint64_t start_tsc = rte_rdtsc();
        
        while (true) {
            // 批次接收
            uint16_t nb_rx = rte_eth_rx_burst(port_id_, 0, rx_bufs, BURST_SIZE);
            
            if (unlikely(nb_rx == 0)) {
                continue;
            }
            
            packets_received_ += nb_rx;
            
            // 處理封包
            for (uint16_t i = 0; i < nb_rx; ++i) {
                process_market_data_packet(rx_bufs[i]);
                rte_pktmbuf_free(rx_bufs[i]);
            }
            
            // 每秒統計
            uint64_t cur_tsc = rte_rdtsc();
            if (unlikely(cur_tsc - start_tsc >= rte_get_tsc_hz())) {
                print_stats();
                start_tsc = cur_tsc;
            }
        }
    }
    
private:
    void port_init() {
        struct rte_eth_conf port_conf = {};
        port_conf.rxmode.max_rx_pkt_len = RTE_ETHER_MAX_LEN;
        
        // 配置 port
        if (rte_eth_dev_configure(port_id_, 1, 1, &port_conf) < 0) {
            throw std::runtime_error("rte_eth_dev_configure() failed");
        }
        
        // 設定 RX queue
        if (rte_eth_rx_queue_setup(port_id_, 0, RX_RING_SIZE,
                rte_eth_dev_socket_id(port_id_), nullptr, mbuf_pool_) < 0) {
            throw std::runtime_error("rte_eth_rx_queue_setup() failed");
        }
        
        // 設定 TX queue
        if (rte_eth_tx_queue_setup(port_id_, 0, TX_RING_SIZE,
                rte_eth_dev_socket_id(port_id_), nullptr) < 0) {
            throw std::runtime_error("rte_eth_tx_queue_setup() failed");
        }
        
        // 啟動 port
        if (rte_eth_dev_start(port_id_) < 0) {
            throw std::runtime_error("rte_eth_dev_start() failed");
        }
        
        // 設定混雜模式 (接收所有封包)
        rte_eth_promiscuous_enable(port_id_);
    }
    
    void process_market_data_packet(struct rte_mbuf *mbuf) {
        uint8_t *pkt_data = rte_pktmbuf_mtod(mbuf, uint8_t*);
        uint16_t pkt_len = rte_pktmbuf_data_len(mbuf);
        
        bytes_received_ += pkt_len;
        
        // 跳過 Ethernet (14) + IP (20) + UDP (8) 標頭
        const size_t header_len = 14 + 20 + 8;
        if (pkt_len < header_len + sizeof(MarketDataPacket)) {
            return;
        }
        
        const auto* md = reinterpret_cast<const MarketDataPacket*>(pkt_data + header_len);
        
        // 處理市場資料 (零拷貝)
        handle_market_data(md);
    }
    
    void handle_market_data(const MarketDataPacket* md) {
        // 更新訂單簿、執行策略等
        // 注意: 在 DPDK 中,所有操作都在單執行緒中 (避免鎖)
    }
    
    void print_stats() {
        printf("Packets: %lu, Bytes: %lu, Rate: %.2f Mpps\n",
               packets_received_, bytes_received_,
               packets_received_ / 1e6);
        
        packets_received_ = 0;
        bytes_received_ = 0;
    }
};
```

## AF_XDP - Linux 原生內核旁路

### AF_XDP 簡介

AF_XDP (Address Family XDP) 是 Linux 原生的高效能 socket,結合 XDP (eXpress Data Path):

```
優勢:
- 內核原生支援 (無需第三方函式庫)
- 零拷貝 (shared memory)
- 低延遲 (< 1 μs)
- 支援標準 Linux 工具

劣勢:
- 效能略低於 DPDK
- Kernel >= 4.18
```

### AF_XDP 基礎使用

```cpp
#include <linux/if_xdp.h>
#include <bpf/xsk.h>

struct xsk_socket_info {
    struct xsk_ring_cons rx;
    struct xsk_ring_prod tx;
    struct xsk_umem *umem;
    struct xsk_socket *xsk;
    uint64_t umem_frame_addr;
    uint32_t outstanding_tx;
};

// 建立 AF_XDP socket
xsk_socket_info* create_xsk_socket(const char* ifname, int queue_id) {
    auto* xsk_info = new xsk_socket_info{};
    
    // 分配 UMEM (User Memory)
    size_t umem_size = XSK_RING_PROD__DEFAULT_NUM_DESCS * XSK_UMEM__DEFAULT_FRAME_SIZE;
    void *umem_area = mmap(nullptr, umem_size, PROT_READ | PROT_WRITE,
                          MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
    
    // 配置 UMEM
    xsk_umem_config umem_cfg = {
        .fill_size = XSK_RING_PROD__DEFAULT_NUM_DESCS,
        .comp_size = XSK_RING_CONS__DEFAULT_NUM_DESCS,
        .frame_size = XSK_UMEM__DEFAULT_FRAME_SIZE,
        .frame_headroom = 0,
        .flags = 0
    };
    
    xsk_ring_prod fill;
    xsk_ring_cons comp;
    
    xsk_umem__create(&xsk_info->umem, umem_area, umem_size, &fill, &comp, &umem_cfg);
    
    // 建立 socket
    xsk_socket_config xsk_cfg = {
        .rx_size = XSK_RING_CONS__DEFAULT_NUM_DESCS,
        .tx_size = XSK_RING_PROD__DEFAULT_NUM_DESCS,
        .libbpf_flags = 0,
        .xdp_flags = XDP_FLAGS_UPDATE_IF_NOEXIST,
        .bind_flags = XDP_ZEROCOPY  // 零拷貝模式
    };
    
    xsk_socket__create(&xsk_info->xsk, ifname, queue_id, xsk_info->umem,
                      &xsk_info->rx, &xsk_info->tx, &xsk_cfg);
    
    return xsk_info;
}
```

## 效能對比

```
Benchmark: 接收 10M 個 64-byte UDP 封包

1. 傳統 Socket (recvfrom):
   - 延遲: ~5-10 μs
   - CPU: 80%
   - 吞吐量: ~1 Mpps

2. epoll + 非阻塞:
   - 延遲: ~3-5 μs
   - CPU: 60%
   - 吞吐量: ~2 Mpps

3. io_uring:
   - 延遲: ~1-2 μs
   - CPU: 40%
   - 吞吐量: ~5 Mpps

4. AF_XDP:
   - 延遲: ~0.5-1 μs
   - CPU: 30% (單核)
   - 吞吐量: ~10 Mpps

5. DPDK:
   - 延遲: ~0.3-0.5 μs
   - CPU: 100% (busy-poll)
   - 吞吐量: ~14 Mpps
```

## 檢查清單

- [ ] 大檔案傳輸使用 `sendfile()`
- [ ] Socket 轉發使用 `splice()`
- [ ] 大緩衝區發送使用 `MSG_ZEROCOPY`
- [ ] HFT 系統考慮 DPDK 或 AF_XDP
- [ ] DPDK 使用批次接收 (burst mode)
- [ ] DPDK 綁定 CPU,避免 context switch
- [ ] 配置足夠的 Hugepages (DPDK)
- [ ] 監控封包丟失率 (UDP)
- [ ] Benchmark 驗證零拷貝效益
- [ ] 考慮內核版本與硬體支援

---

## 參考資料 (References)

1. [Zero Copy I/O - Linux Kernel Documentation](https://www.kernel.org/doc/html/latest/networking/msg_zerocopy.html)
2. [DPDK Documentation](https://doc.dpdk.org/)
3. [AF_XDP - Kernel Documentation](https://www.kernel.org/doc/html/latest/networking/af_xdp.html)
4. Intel. *Data Plane Development Kit (DPDK) Getting Started Guide* (2020)
5. [XDP Tutorial](https://github.com/xdp-project/xdp-tutorial)
6. Corbet, Jonathan. *Toward zero-copy networking* (LWN.net, 2018)
