# 05-ä½µç™¼èˆ‡ä¸¦è¡Œ

## ğŸ“– æ ¸å¿ƒæ¦‚å¿µ

Python ä½µç™¼èˆ‡ä¸¦è¡Œç·¨ç¨‹ï¼š
- **ç•°æ­¥ç·¨ç¨‹**: asyncio, async/await
- **å¤šç·šç¨‹**: threading, GIL
- **å¤šé€²ç¨‹**: multiprocessing
- **ä½µç™¼å·¥å…·**: concurrent.futures
- **åŒæ­¥åŸèª**: Lock, Semaphore, Event

```mermaid
graph TD
    A[ä½µç™¼èˆ‡ä¸¦è¡Œ] --> B[ç•°æ­¥ç·¨ç¨‹]
    A --> C[å¤šç·šç¨‹]
    A --> D[å¤šé€²ç¨‹]
    A --> E[ä½µç™¼å·¥å…·]
    
    B --> B1[asyncio]
    B --> B2[async/await]
    B --> B3[äº‹ä»¶å¾ªç’°]
    
    C --> C1[threading]
    C --> C2[GILé™åˆ¶]
    C --> C3[ç·šç¨‹åŒæ­¥]
    
    D --> D1[multiprocessing]
    D --> D2[é€²ç¨‹æ± ]
    D --> D3[é€²ç¨‹é–“é€šä¿¡]
    
    E --> E1[ThreadPoolExecutor]
    E --> E2[ProcessPoolExecutor]
```

## ğŸ”§ ç•°æ­¥ç·¨ç¨‹ (asyncio)

### åŸºç¤ async/await

```python
import asyncio

async def fetch_data(url: str, delay: int) -> dict:
    print(f"Fetching {url}...")
    await asyncio.sleep(delay)  # æ¨¡æ“¬ I/O æ“ä½œ
    return {"url": url, "data": "response"}

async def main():
    result = await fetch_data("https://api.example.com", 2)
    print(result)

# åŸ·è¡Œ
asyncio.run(main())
```

### ä¸¦ç™¼åŸ·è¡Œå¤šå€‹å”ç¨‹

```python
import asyncio
from typing import List

async def fetch_data(url: str, delay: int) -> str:
    await asyncio.sleep(delay)
    return f"Data from {url}"

async def main():
    urls = [
        ("https://api1.com", 2),
        ("https://api2.com", 1),
        ("https://api3.com", 3),
    ]
    
    # æ–¹å¼ä¸€ï¼šgather - ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
    tasks = [fetch_data(url, delay) for url, delay in urls]
    results = await asyncio.gather(*tasks)
    print(results)
    
    # æ–¹å¼äºŒï¼šTaskGroup (Python 3.11+)
    async with asyncio.TaskGroup() as tg:
        tasks = [tg.create_task(fetch_data(url, delay)) for url, delay in urls]
    results = [task.result() for task in tasks]
    
    # æ–¹å¼ä¸‰ï¼šas_completed - æŒ‰å®Œæˆé †åºè™•ç†
    tasks = [fetch_data(url, delay) for url, delay in urls]
    for coro in asyncio.as_completed(tasks):
        result = await coro
        print(f"Completed: {result}")

asyncio.run(main())
```

### è¶…æ™‚æ§åˆ¶

```python
import asyncio

async def slow_operation():
    await asyncio.sleep(5)
    return "Done"

async def main():
    try:
        result = await asyncio.wait_for(slow_operation(), timeout=2.0)
        print(result)
    except asyncio.TimeoutError:
        print("Operation timed out")

asyncio.run(main())
```

### ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
import asyncio

class AsyncDatabaseConnection:
    async def __aenter__(self):
        print("Opening connection...")
        await asyncio.sleep(0.5)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        print("Closing connection...")
        await asyncio.sleep(0.5)
    
    async def query(self, sql: str):
        await asyncio.sleep(0.1)
        return f"Results for: {sql}"

async def main():
    async with AsyncDatabaseConnection() as conn:
        result = await conn.query("SELECT * FROM users")
        print(result)

asyncio.run(main())
```

### ç•°æ­¥è¿­ä»£å™¨

```python
import asyncio

class AsyncRange:
    def __init__(self, start: int, stop: int):
        self.start = start
        self.stop = stop
        self.current = start
    
    def __aiter__(self):
        return self
    
    async def __anext__(self):
        if self.current >= self.stop:
            raise StopAsyncIteration
        await asyncio.sleep(0.1)
        value = self.current
        self.current += 1
        return value

async def main():
    async for num in AsyncRange(0, 5):
        print(num)

asyncio.run(main())
```

### å¯¦æˆ°ï¼šç•°æ­¥ HTTP å®¢æˆ¶ç«¯

```python
import asyncio
import aiohttp
from typing import List

async def fetch_url(session: aiohttp.ClientSession, url: str) -> dict:
    async with session.get(url) as response:
        return {
            "url": url,
            "status": response.status,
            "data": await response.text(),
        }

async def fetch_all(urls: List[str]) -> List[dict]:
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url(session, url) for url in urls]
        return await asyncio.gather(*tasks)

async def main():
    urls = [
        "https://api.github.com",
        "https://api.github.com/users/github",
        "https://api.github.com/repos/python/cpython",
    ]
    results = await fetch_all(urls)
    for result in results:
        print(f"{result['url']}: {result['status']}")

# asyncio.run(main())
```

## ğŸ”§ å¤šç·šç¨‹ (threading)

### åŸºç¤ç·šç¨‹ä½¿ç”¨

```python
import threading
import time

def worker(name: str, delay: int):
    print(f"Worker {name} starting")
    time.sleep(delay)
    print(f"Worker {name} done")

# å‰µå»ºç·šç¨‹
t1 = threading.Thread(target=worker, args=("A", 2))
t2 = threading.Thread(target=worker, args=("B", 1))

t1.start()
t2.start()

t1.join()  # ç­‰å¾…ç·šç¨‹å®Œæˆ
t2.join()

print("All workers done")
```

### ç·šç¨‹é¡

```python
import threading
import time

class WorkerThread(threading.Thread):
    def __init__(self, name: str, delay: int):
        super().__init__()
        self.name = name
        self.delay = delay
    
    def run(self):
        print(f"{self.name} starting")
        time.sleep(self.delay)
        print(f"{self.name} done")

worker1 = WorkerThread("Worker-1", 2)
worker2 = WorkerThread("Worker-2", 1)

worker1.start()
worker2.start()

worker1.join()
worker2.join()
```

### ç·šç¨‹åŒæ­¥ï¼šLock

```python
import threading

counter = 0
lock = threading.Lock()

def increment():
    global counter
    for _ in range(100000):
        with lock:  # ç²å–é–
            counter += 1

threads = [threading.Thread(target=increment) for _ in range(10)]

for t in threads:
    t.start()

for t in threads:
    t.join()

print(f"Counter: {counter}")  # 1000000
```

### ç·šç¨‹åŒæ­¥ï¼šEvent

```python
import threading
import time

event = threading.Event()

def waiter():
    print("Waiting for event...")
    event.wait()  # é˜»å¡ç›´åˆ°äº‹ä»¶è¢«è¨­ç½®
    print("Event received!")

def setter():
    time.sleep(2)
    print("Setting event...")
    event.set()

t1 = threading.Thread(target=waiter)
t2 = threading.Thread(target=setter)

t1.start()
t2.start()

t1.join()
t2.join()
```

### GIL é™åˆ¶

```python
import threading
import time

# âŒ CPU å¯†é›†å‹ä»»å‹™ä¸é©åˆå¤šç·šç¨‹ï¼ˆGIL é™åˆ¶ï¼‰
def cpu_bound_task(n: int):
    count = 0
    for i in range(n):
        count += i * i
    return count

# å–®ç·šç¨‹
start = time.time()
cpu_bound_task(10000000)
cpu_bound_task(10000000)
print(f"Single thread: {time.time() - start:.2f}s")

# å¤šç·šç¨‹ï¼ˆå¯èƒ½æ›´æ…¢ï¼ï¼‰
start = time.time()
t1 = threading.Thread(target=cpu_bound_task, args=(10000000,))
t2 = threading.Thread(target=cpu_bound_task, args=(10000000,))
t1.start()
t2.start()
t1.join()
t2.join()
print(f"Multi thread: {time.time() - start:.2f}s")
```

## ğŸ”§ å¤šé€²ç¨‹ (multiprocessing)

### åŸºç¤é€²ç¨‹ä½¿ç”¨

```python
import multiprocessing
import os

def worker(name: str):
    print(f"Worker {name} in process {os.getpid()}")

if __name__ == "__main__":
    p1 = multiprocessing.Process(target=worker, args=("A",))
    p2 = multiprocessing.Process(target=worker, args=("B",))
    
    p1.start()
    p2.start()
    
    p1.join()
    p2.join()
```

### CPU å¯†é›†å‹ä»»å‹™

```python
import multiprocessing
import time

def cpu_bound_task(n: int) -> int:
    count = 0
    for i in range(n):
        count += i * i
    return count

if __name__ == "__main__":
    # å–®é€²ç¨‹
    start = time.time()
    result1 = cpu_bound_task(10000000)
    result2 = cpu_bound_task(10000000)
    print(f"Single process: {time.time() - start:.2f}s")
    
    # å¤šé€²ç¨‹ï¼ˆé¡¯è‘—æå‡ï¼‰
    start = time.time()
    with multiprocessing.Pool(processes=2) as pool:
        results = pool.map(cpu_bound_task, [10000000, 10000000])
    print(f"Multi process: {time.time() - start:.2f}s")
```

### é€²ç¨‹é–“é€šä¿¡ï¼šQueue

```python
import multiprocessing

def producer(queue: multiprocessing.Queue):
    for i in range(5):
        queue.put(f"Item {i}")
    queue.put(None)  # çµæŸä¿¡è™Ÿ

def consumer(queue: multiprocessing.Queue):
    while True:
        item = queue.get()
        if item is None:
            break
        print(f"Consumed: {item}")

if __name__ == "__main__":
    queue = multiprocessing.Queue()
    
    p1 = multiprocessing.Process(target=producer, args=(queue,))
    p2 = multiprocessing.Process(target=consumer, args=(queue,))
    
    p1.start()
    p2.start()
    
    p1.join()
    p2.join()
```

### é€²ç¨‹é–“é€šä¿¡ï¼šPipe

```python
import multiprocessing

def sender(conn):
    conn.send(["Hello", "World"])
    conn.close()

def receiver(conn):
    msg = conn.recv()
    print(f"Received: {msg}")

if __name__ == "__main__":
    parent_conn, child_conn = multiprocessing.Pipe()
    
    p1 = multiprocessing.Process(target=sender, args=(child_conn,))
    p2 = multiprocessing.Process(target=receiver, args=(parent_conn,))
    
    p1.start()
    p2.start()
    
    p1.join()
    p2.join()
```

## ğŸ”§ concurrent.futures

### ThreadPoolExecutor

```python
from concurrent.futures import ThreadPoolExecutor
import time

def task(n: int) -> int:
    time.sleep(1)
    return n * n

# ä½¿ç”¨ç·šç¨‹æ± 
with ThreadPoolExecutor(max_workers=3) as executor:
    # æ–¹å¼ä¸€ï¼šsubmit
    futures = [executor.submit(task, i) for i in range(5)]
    results = [f.result() for f in futures]
    print(results)
    
    # æ–¹å¼äºŒï¼šmap
    results = list(executor.map(task, range(5)))
    print(results)
```

### ProcessPoolExecutor

```python
from concurrent.futures import ProcessPoolExecutor

def cpu_task(n: int) -> int:
    return sum(i * i for i in range(n))

if __name__ == "__main__":
    with ProcessPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(cpu_task, [1000000] * 4))
        print(results)
```

### as_completed - æŒ‰å®Œæˆé †åºè™•ç†

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import random

def task(n: int) -> int:
    delay = random.uniform(0.5, 2.0)
    time.sleep(delay)
    return n * n

with ThreadPoolExecutor(max_workers=3) as executor:
    futures = {executor.submit(task, i): i for i in range(5)}
    
    for future in as_completed(futures):
        original_value = futures[future]
        result = future.result()
        print(f"Task {original_value} completed with result {result}")
```

## ğŸ’¡ å¯¦æˆ°æ¡ˆä¾‹ï¼šä¸¦è¡Œç¶²è·¯çˆ¬èŸ²

```python
import asyncio
import aiohttp
from typing import List
from dataclasses import dataclass

@dataclass
class PageResult:
    url: str
    status: int
    content_length: int
    error: str | None = None

async def fetch_page(session: aiohttp.ClientSession, url: str) -> PageResult:
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
            content = await response.text()
            return PageResult(
                url=url,
                status=response.status,
                content_length=len(content),
            )
    except Exception as e:
        return PageResult(url=url, status=0, content_length=0, error=str(e))

async def crawl_websites(urls: List[str], max_concurrent: int = 10) -> List[PageResult]:
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def fetch_with_limit(session: aiohttp.ClientSession, url: str):
        async with semaphore:
            return await fetch_page(session, url)
    
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_with_limit(session, url) for url in urls]
        return await asyncio.gather(*tasks)

async def main():
    urls = [
        "https://example.com",
        "https://python.org",
        "https://github.com",
    ] * 10
    
    results = await crawl_websites(urls, max_concurrent=5)
    
    success = [r for r in results if r.error is None]
    failed = [r for r in results if r.error is not None]
    
    print(f"Success: {len(success)}, Failed: {len(failed)}")
    for result in failed:
        print(f"Failed: {result.url} - {result.error}")

# asyncio.run(main())
```

## âš ï¸ å¸¸è¦‹é™·é˜±

### 1. å¿˜è¨˜ await

```python
# âŒ éŒ¯èª¤
async def main():
    result = fetch_data()  # è¿”å›å”ç¨‹å°è±¡ï¼ŒæœªåŸ·è¡Œ
    print(result)

# âœ… æ­£ç¢º
async def main():
    result = await fetch_data()
    print(result)
```

### 2. åœ¨åŒæ­¥å‡½æ•¸ä¸­èª¿ç”¨ç•°æ­¥å‡½æ•¸

```python
# âŒ éŒ¯èª¤
def sync_function():
    result = await async_function()  # SyntaxError

# âœ… æ­£ç¢º
async def async_function_wrapper():
    result = await async_function()
    return result

def sync_function():
    result = asyncio.run(async_function_wrapper())
```

### 3. å…±äº«ç‹€æ…‹æœªåŠ é–

```python
# âŒ éŒ¯èª¤ï¼šç«¶æ…‹æ¢ä»¶
counter = 0

def increment():
    global counter
    for _ in range(100000):
        counter += 1  # éåŸå­æ“ä½œ

# âœ… æ­£ç¢º
lock = threading.Lock()

def increment():
    global counter
    for _ in range(100000):
        with lock:
            counter += 1
```

### 4. ProcessPoolExecutor æœªä½¿ç”¨ if __name__ == "__main__"

```python
# âŒ éŒ¯èª¤ï¼šæœƒç„¡é™éæ­¸
from concurrent.futures import ProcessPoolExecutor

executor = ProcessPoolExecutor()
# ...

# âœ… æ­£ç¢º
if __name__ == "__main__":
    executor = ProcessPoolExecutor()
    # ...
```

## ğŸ’¡ æœ€ä½³å¯¦è¸

1. **I/O å¯†é›†å‹ä»»å‹™ä½¿ç”¨ asyncio æˆ– threading**
   ```python
   # ç¶²è·¯è«‹æ±‚ã€æª”æ¡ˆ I/O
   async with aiohttp.ClientSession() as session:
       await session.get(url)
   ```

2. **CPU å¯†é›†å‹ä»»å‹™ä½¿ç”¨ multiprocessing**
   ```python
   # æ•¸æ“šè™•ç†ã€å½±åƒè™•ç†
   with ProcessPoolExecutor() as executor:
       results = executor.map(cpu_task, data)
   ```

3. **ä½¿ç”¨ concurrent.futures ç°¡åŒ–ä½µç™¼**
   ```python
   with ThreadPoolExecutor() as executor:
       futures = executor.map(task, items)
   ```

4. **è¨­ç½®è¶…æ™‚é¿å…ç„¡é™ç­‰å¾…**
   ```python
   await asyncio.wait_for(coro, timeout=5.0)
   ```

5. **ä½¿ç”¨ä¿¡è™Ÿé‡æ§åˆ¶ä½µç™¼æ•¸**
   ```python
   semaphore = asyncio.Semaphore(10)
   async with semaphore:
       await fetch_data()
   ```

6. **ç•°å¸¸è™•ç†**
   ```python
   try:
       result = await asyncio.gather(*tasks, return_exceptions=True)
       for r in result:
           if isinstance(r, Exception):
               print(f"Error: {r}")
   except Exception as e:
       print(f"Critical error: {e}")
   ```
